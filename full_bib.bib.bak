% Encoding: UTF-8

@InProceedings{1804.02433,
  author    = {Michael Rath and Jacob Rendall and Jin L. C. Guo and Jane Cleland-Huang and Patrick Maeder},
  booktitle = {Proceedings of the 40th International Conference on Software Engineering},
  title     = {Traceability in the Wild: Automatically Augmenting Incomplete Trace Links},
  year      = {2018},
  eprint    = {arXiv:1804.02433},
}

@InProceedings{292398,
  author    = {O. C. Z. Gotel and C. W. Finkelstein},
  booktitle = {Proceedings of IEEE International Conference on Requirements Engineering},
  title     = {An analysis of the requirements traceability problem},
  year      = {1994},
  month     = {Apr},
  pages     = {94--101},
  doi       = {10.1109/ICRE.1994.292398},
  keywords  = {systems analysis;post-requirements specification traceability;pre-requirements specification traceability;requirements engineering practice;requirements traceability problem analysis;requirements traceability tools;Educational institutions;Guidelines;Project management;Research and development},
}

@Article{4249808,
  author   = {J. Cleland-Huang and B. Berenbach and S. Clark and R. Settimi and E. Romanova},
  journal  = {Computer},
  title    = {Best Practices for Automated Traceability},
  year     = {2007},
  issn     = {0018-9162},
  month    = jun,
  number   = {6},
  pages    = {27--35},
  volume   = {40},
  doi      = {10.1109/MC.2007.195},
  keywords = {document handling;formal specification;information retrieval;automated traceability;information-retrieval techniques;legacy documents;requirements trace matrix;Best practices;Capability maturity model;Code standards;Maintenance engineering;Manuals;Software maintenance;Standards development;Standards organizations;System testing;Unified modeling language;automated traceability},
}
@inproceedings{Abebe2010,
  title={Natural language parsing of program element names for concept extraction},
  author={Abebe, Surafel Lemma and Tonella, Paolo},
  booktitle={IEEE 18th Int. Conf. on Prog. Comp. (ICPC) 2010},
  pages={156--159},
  year={2010},
  organization={IEEE}
}
@inproceedings{Adel2013,
abstract = {In this paper, we investigate the appli-cation of recurrent neural network lan-guage models (RNNLM) and factored language models (FLM) to the task of language modeling for Code-Switching speech. We present a way to integrate part-of-speech tags (POS) and language in-formation (LID) into these models which leads to significant improvements in terms of perplexity. Furthermore, a comparison between RNNLMs and FLMs and a de-tailed analysis of perplexities on the dif-ferent backoff levels are performed. Fi-nally, we show that recurrent neural net-works and factored language models can be combined using linear interpolation to achieve the best performance. The final combined language model provides 37.8{\%} relative improvement in terms of perplex-ity on the SEAME development set and a relative improvement of 32.7{\%} on the evaluation set compared to the traditional n-gram language model.},
author = {Adel, Heike and Vu, Ngoc Thang and Schultz, Tanja},
booktitle = {Proc. 51st Annu. Meet. Assoc. Comput. Linguist.},
isbn = {9781937284510},
pages = {206--211},
title = {{Combination of Recurrent Neural Networks and Factored Language Models for Code-Switching Language Modeling}},
year = {2013}
}
@article{alexandru2019redundancy,
  title={Redundancy-free analysis of multi-revision software artifacts},
  author={Alexandru, Carol V and Panichella, Sebastiano and Proksch, Sebastian and Gall, Harald C},
  journal={Empirical Software Engineering},
  volume={24},
  number={1},
  pages={332--380},
  year={2019},
  publisher={Springer}
}
@inproceedings{Alghamdi2016,
abstract = {We address the problem of Part of Speech tag-ging (POS) in the context of linguistic code switching (CS). CS is the phenomenon where a speaker switches between two languages or variants of the same language within or across utterances, known as intra-sentential or inter-sentential CS, respectively. Process-ing CS data is especially challenging in intra-sentential data given state of the art monolin-gual NLP technology since such technology is geared toward the processing of one language at a time. In this paper we explore multiple strategies of applying state of the art POS tag-gers to CS data. We investigate the landscape in two CS language pairs, Spanish-English and Modern Standard Arabic-Arabic dialects. We compare the use of two POS taggers vs. a unified tagger trained on CS data. Our results show that applying a machine learning frame-work using two state fof the art POS taggers achieves better performance compared to all other approaches that we investigate.},
author = {Alghamdi, Fahad and Molina, Giovanni and Diab, Mona and Solorio, Thamar and Hawwari, Abdelati and Soto, Victor and Hirschberg, Julia},
booktitle = {Proc. Second Work. Comput. Approaches to Code Switch. EMNLP},
title = {{Part of Speech Tagging for Code Switched Data}},
year = {2016}
}
@article{Allamanis2018,
abstract = {Learning tasks on source code (i.e., formal languages) have been considered recently, but most work has tried to transfer natural language methods and does not capitalize on the unique opportunities offered by code's known sematics. For example, long-range dependencies induced by using the same variable or function in distant locations are often not considered. We propose to use graphs to represent both the syntactic and semantic structure of code and use graph-based deep learning methods to learn to reason over program structures. In this work, we present how to construct graphs from source code and how to scale Gated Graph Neural Networks training to such large graphs. We evaluate our method on two tasks: VARNAMING, in which a network attempts to predict the name of a variable given its usage, and VARMISUSE, in which the network learns to reason about selecting the correct variable that should be used at a given program location. Our comparison to methods that use less structured program representations shows the advantages of modeling known structure, and suggests that our models learn to infer meaningful names and to solve the VARMISUSE task in many cases. Additionally, our testing showed that VARMISUSE identifies a number of bugs in mature open-source projects.},
archivePrefix = {arXiv},
arxivId = {1711.00740},
author = {Allamanis, Miltiadis and Brockschmidt, Marc and Khademi, Mahmoud},
eprint = {1711.00740},
journal = {6th Int. Conf. Learn. Represent. ICLR 2018 - Conf. Track Proc.},
mendeley-groups = {Untangle},
pages = {1--17},
title = {{Learning to represent programs with graphs}},
year = {2018}
}
@inproceedings{Asuncion:2009:CCL:1556908.1557008,
	author = {Asuncion, Hazeline U. and Taylor, Richard N.},
	title = {Capturing Custom Link Semantics Among Heterogeneous Artifacts and Tools},
	booktitle = {Proceedings of the 2009 ICSE Workshop on Traceability in Emerging Forms of Software Engineering},
	series = {TEFSE '09},
	year = {2009},
	isbn = {978-1-4244-3741-2},
	pages = {1--5},
	numpages = {5},
	doi = {10.1109/TEFSE.2009.5069574},
	acmid = {1557008},
	publisher = {IEEE Computer Society},
	address = {Washington, DC, USA},
}
@article{AuthorTopic,
abstract = {We propose a new unsupervised learning technique for ex- tracting information from large text collections. We model documents as if they were generated by a two-stage stochas- tic process. Each author is represented by a probability distribution over topics, and each topic is represented as a probability distribution over words for that topic. The words in a multi-author paper are assumed to be the result of a mixture of each authors' topic mixture. The topic-word and author-topic distributions are learned from data in an unsupervised manner using a Markov chain Monte Carlo al- gorithm. We apply the methodology to a large corpus of 160,000 abstracts and 85,000 authors from the well-known CiteSeer digital library, and learn a model with 300 topics. We discuss in detail the interpretation of the results dis- covered by the system including specific topic and author models, ranking of authors by topic and topics by author, significant trends in the computer science literature between 1990 and 2002, parsing of abstracts by topics and authors and detection of unusual papers by specific authors. An on- line query interface to the model is also discussed that allows interactive exploration of author-topic models for corpora such as CiteSeer.},
author = {Steyvers, Mark and Smyth, Padhraic and Rosen-Zvi, Michal and Griffiths, Thomas},
doi = {10.1.1.10.2031},
isbn = {1581138889},
journal = {Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining},
keywords = {gibbs sampling,text modeling,unsupervised learning},
number = {1990},
pages = {315},
pmid = {4086762619115516774},
title = {{Probabilistic author-topic models for information discovery}},
year = {2004}
}
@misc{BacklogRefinement,
  title = {{Agile Alliance: Backlog refinement}},
  author={{Agile Alliance}},
  howpublished = {\url{https://www.agilealliance.org/glossary/backlog-grooming/}},
  note = {Accessed: 2019-11-26},
  year = {2019}
}
@article{Baldi2008,
	abstract = {After more than 10 years, Aspect-Oriented Programming (AOP) is still a controversial idea. While the concept of aspects appeals to everyone's intuitions, concrete AOP solutions often fail to convince researchers and practitioners alike. This discrepancy results in part from a lack of an adequate theory of aspects, which in turn leads to the development of AOP solutions that are useful in limited situations. We propose a new theory of aspects that can be summarized as follows: concerns are latent topics that can be automatically extracted using statistical topic modeling techniques adapted to software. Software scattering and tangling can be measured precisely by the entropies of the underlying topic-over-files and files-over-topics distributions. Aspects are latent topics with high scattering entropy. The theory is validated empirically on both the large scale, with a study of 4,632 Java projects, and the small scale, with a study of 5 individual projects. From these analyses, we identify two dozen topics that emerge as general-purpose aspects across multiple projects, as well as project-specific topics/concerns. The approach is also shown to produce results that are compatible with previous methods for identifying aspects, and also extends them. Our work provides not only a concrete approach for identifying aspects at several scales in an unsupervised manner but, more importantly, a formulation of AOP grounded in information theory. The understanding of aspects under this new perspective makes additional progress toward the design of models and tools that facilitate software development.},
	author = {Baldi, Pierre F. and Lopes, Cristina V. and Linstead, Erik J. and Bajracharya, Sushil K.},
	doi = {10.1145/1449955.1449807},
	isbn = {9781605582153},
	issn = {03621340},
	journal = {ACM SIGPLAN Notices},
	keywords = {aspect-oriented programming,scattering,tan-},
	number = {10},
	pages = {543},
	title = {{A theory of aspects as latent topics}},
	volume = {43},
	year = {2008}
}
@inproceedings{Baltes2018,
  title={Sotorrent: Reconstructing and analyzing the evolution of {Stack} {Overflow} posts},
  author={Baltes, Sebastian and Dumani, Lorik and Treude, Christoph and Diehl, Stephan},
  booktitle={Proc. 15th Int. Conf. Min. Soft. Rep.},
  pages={319--330},
  year={2018},
  organization={ACM}
}
@misc{bangorTalk,
  title={Bangor Talk Miami Corpus},
  howpublished = "\url{http://www.bangortalk.org.uk/speakers.php?c= miami}",
  year = {2011}, 
}
}
@inproceedings{Banko:2004:PST:1220355.1220435,
 author = {Banko, Michele and Moore, Robert C.},
 title = {Part of Speech Tagging in Context},
 booktitle = {Proc. 20th Int. Conf. Comp. Ling.},
 series = {COLING '04},
 year = {2004},
 location = {Geneva, Switzerland},
 articleno = {556},
 url = {https://doi.org/10.3115/1220355.1220435},
 doi = {10.3115/1220355.1220435},
 acmid = {1220435},
 publisher = {Association for Computational Linguistics},
 address = {Stroudsburg, PA, USA},
}
@article{Barnett2015,
abstract = {Code Reviews, an important and popular mechanism for quality assurance, are often performed on a change set, a set of modified files that are meant to be committed to a source repository as an atomic action. Understanding a code review is more difficult when the change set consists of multiple, independent, code differences. We introduce CLUSTERCHANGES, an automatic technique for decomposing change sets and evaluate its effectiveness through both a quantitative analysis and a qualitative user study.},
author = {Barnett, Mike and Bird, Christian and Brunet, Jo{\~{a}}o and Lahiri, Shuvendu K.},
doi = {10.1109/ICSE.2015.35},
isbn = {9781479919345},
issn = {02705257},
journal = {Proc. - Int. Conf. Softw. Eng.},
number = {August 2014},
pages = {134--144},
title = {{Helping developers help themselves: Automatic decomposition of code review changesets}},
volume = {1},
year = {2015}
}
@article{Biggerstaff1993,
abstract = {A person understands a program because they are able to relate the structures of the program and its environment to their human oriented conceptual knowledge about the world. The problem of discovering individual human oriented concepts and assigning them to their implementation oriented counterparts for a given a program is the concept assignment problem. We will argue that the solution to this problem requires methods that have a strong plausible reasoning component. We will illustrate these ideas through example scenarios using an existing design recovery system called DESIRE. Finally, we will evaluate DESIRE based on its usage on real-world problems over the years},
author = {Biggerstaff, Ted J. and Mitbander, Bharat G. and Webster, Dallas},
doi = {10.1109/ICSE.1993.346017},
isbn = {0-8186-3700-5},
issn = {0005-9366},
journal = {Proc. 1993 15th Int. Conf. Softw. Eng.},
keywords = {Reverse engineering,concept,connectionist,domain,first expression of computational,intent is designed for,knowledge base,plausible reasoning,recognition,slicing},
mendeley-groups = {Untangle},
number = {6},
pages = {482--498},
pmid = {346017},
title = {{The concept assignment problem in program understanding}},
url = {http://academic.research.microsoft.com/Publication/540795/the-concept-assignment-problem-in-program-understanding{\%}5Cnhttp://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=346017{\%}5Cnhttp://www.ncbi.nlm.nih.gov/pubmed/346017{\%}5Cnhttp://www.scopus.com/inwar},
volume = {91},
year = {1993}
}
@article{Biggerstaff1994,
abstract = {Note: OCR errors may be found in this Reference List extracted from the full text article. ACM has opted to expose the complete List rather than only correct and linked references.},
author = {Biggerstaff, Ted J. and Mitbander, Bharat G. and Webster, Dallas E.},
doi = {10.1145/175290.175300},
isbn = {0001-0782},
issn = {00010782},
journal = {Commun. ACM},
number = {5},
pages = {72--82},
title = {{Program understanding and the concept assignment problem}},
volume = {37},
year = {1994}
}
@inproceedings{Binkley2011,
abstract = {Recent software development tools have exploited the mining of natural language information found within software and its supporting documentation. To make the most of this information, researchers have drawn upon the work of the natural language processing community for tools and techniques. One such tool provides part-of-speech information, which finds application in improving the searching of software repositories and extracting domain information found in identifiers. Unfortunately, the natural language found is software differs from that found in standard prose. This difference potentially limits the effectiveness of off-the-shelf tools. An empirical investigation finds that with minimal guidance an existing tagger was correct 88{\%} of the time when tagging the words found in source code identifiers. The investigation then uses the improved part-of-speech information to tag a large corpus of over 145,000 structure-field names. From patterns in the tags several rules emerge that seek to understand past usage and to improve future naming.},
address = {New York, New York, USA},
author = {Binkley, Dave and Hearn, Matthew and Lawrie, Dawn},
booktitle = {Proceeding 8th Work. Conf. Min. Softw. Repos. - MSR '11},
doi = {10.1145/1985441.1985471},
isbn = {9781450305747},
issn = {02705257},
keywords = {identifier analysis,natural language processing,program comprehension},
mendeley-groups = {Code pseudo-PoS tagging},
pages = {203},
publisher = {ACM Press},
title = {{Improving identifier informativeness using part of speech information}},
url = {http://portal.acm.org/citation.cfm?doid=1985441.1985471},
year = {2011}
}
@article{Bird2009,
	abstract = {Software engineering researchers have long been interested in where and why bugs occur in code, and in predicting where they might turn up next. Historical bug-occurence data has been key to this research. Bug tracking systems, and code version histories, record when, how and by whom bugs were fixed; from these sources, datasets that relate file changes to bug fixes can be extracted. These historical datasets can be used to test hypotheses concerning processes of bug introduction, and also to build statistical bug prediction models. Unfortunately, processes and humans are imperfect, and only a fraction of bug fixes are actually labelled in source code version histories, and thus become available for study in the extracted datasets. The question naturally arises, are the bug fixes recorded in these historical datasets a fair representation of the full population of bug fixes? In this paper, we investigate historical data from several software projects, and find strong evidence of systematic bias. We then investigate the potential effects of "unfair, imbalanced" datasets on the performance of prediction techniques. We draw the lesson that bias is a critical problem that threatens both the effectiveness of processes that rely on biased datasets to build prediction models and the generalizability of hypotheses tested on biased data.},
	author = {Bird, Christian and Bachmann, Adrian and Aune, Eirik and Duffy, John and Bernstein, Abraham and Filkov, Vladimir and Devanbu, Premkumar},
	doi = {10.1145/1595696.1595716},
	isbn = {978-1-60558-001-2},
	issn = {01681656},
	journal = {Proc. ESEC/FSE},
	pages = {121--130},
	pmid = {18984014},
	title = {{Fair and Balanced?: Bias in Bug-Fix Datasets}},
	year = {2009}
}
@book{BirdKleinLoper09,
  author = {Steven Bird and Ewan Klein and Edward Loper},
  title = {{Natural Language Processing with Python}},
  publisher = {O'Reilly Media},
  year = 2009
}
@Article{Borg2014,
author="Borg, Markus
and Runeson, Per
and Ard{\"o}, Anders",
title="Recovering from a decade: a systematic mapping  of information retrieval approaches  to software traceability",
journal="Empirical Software Engineering",
year="2014",
month="Dec",
day="01",
volume="19",
number="6",
pages="1565--1616",
abstract="Engineers in large-scale software development have to manage large amounts of information, spread across many artifacts. Several researchers have proposed expressing retrieval of trace links among artifacts, i.e. trace recovery, as an Information Retrieval (IR) problem. The objective of this study is to produce a map of work on IR-based trace recovery, with a particular focus on previous evaluations and strength of evidence. We conducted a systematic mapping of IR-based trace recovery. Of the 79 publications classified, a majority applied algebraic IR models. While a set of studies on students indicate that IR-based trace recovery tools support certain work tasks, most previous studies do not go beyond reporting precision and recall of candidate trace links from evaluations using datasets containing less than 500 artifacts. Our review identified a need of industrial case studies. Furthermore, we conclude that the overall quality of reporting should be improved regarding both context and tool details, measures reported, and use of IR terminology. Finally, based on our empirical findings, we present suggestions on how to advance research on IR-based  trace recovery.",
issn="1573-7616",
doi="10.1007/s10664-013-9255-y",
url="https://doi.org/10.1007/s10664-013-9255-y"
}
@book{Breiman+84,
	address = {Belmont, CA},
	author = {Breiman, L. and Friedman, J. and Ohlsen, R. and Stone, C.},
	citeulike-article-id = {931140},
	posted-at = {2006-11-06 08:07:54},
	priority = {0},
	publisher = {Wadsworth International Group},
	title = {Classification and regression trees},
	year = {1984}
}
@article{Brockschmidt2019,
abstract = {Generative models for source code are an interesting structured prediction problem, requiring to reason about both hard syntactic and semantic constraints as well as about natural, likely programs. We present a novel model for this problem that uses a graph to represent the intermediate state of the generated output. Our model generates code by interleaving grammar-driven expansion steps with graph augmentation and neural message passing steps. An experimental evaluation shows that our new model can generate semantically meaningful expressions, outperforming a range of strong baselines.},
archivePrefix = {arXiv},
arxivId = {1805.08490},
author = {Brockschmidt, Marc and Allamanis, Miltiadis and Gaunt, Alexander and Polozov, Oleksandr},
eprint = {1805.08490},
journal = {7th Int. Conf. Learn. Represent. ICLR 2019},
mendeley-groups = {Untangle},
pages = {1--24},
title = {{Generative code modeling with graphs}},
year = {2019}
}
@article{BugLocBasedOnHistory,
abstract = {Context Several issues or defects in released software during the maintenance phase are reported to the development team. It is costly and time-consuming for developers to precisely localize bugs. Bug reports and the code change history are frequently used and provide information for identifying fault locations during the software maintenance phase. Objective It is difficult to standardize the style of bug reports written in natural languages to improve the accuracy of bug localization. The objective of this paper is to propose an effective information retrieval-based bug localization method to find suspicious files and methods for resolving bugs. Method In this paper, we propose a novel information retrieval-based bug localization approach, termed Bug Localization using Integrated Analysis (BLIA). Our proposed BLIA integrates analyzed data by utilizing texts, stack traces and comments in bug reports, structured information of source files, and the source code change history. We improved the granularity of bug localization from the file level to the method level by extending previous bug repository data. Results We evaluated the effectiveness of our approach based on experiments using three open-source projects, namely AspectJ, SWT, and ZXing. In terms of the mean average precision, on average our approach improves the metric of BugLocator, BLUiR, BRTracer, AmaLgam and the preliminary version of BLIA by 54{\%}, 42{\%}, 30{\%}, 25{\%} and 15{\%}, respectively, at the file level of bug localization. Conclusion Compared with prior tools, the results showed that BLIA outperforms these other methods. We analyzed the influence of each score of BLIA from various combinations based on the analyzed information. Our proposed enhancement significantly improved the accuracy. To improve the granularity level of bug localization, a new approach at the method level is proposed and its potential is evaluated.},
author = {Youm, Klaus Changsun and Ahn, June and Lee, Eunseok},
doi = {10.1016/j.infsof.2016.11.002},
isbn = {9781467396448},
issn = {09505849},
journal = {Information and Software Technology},
keywords = {Bug localization,Bug reports,Code change history,Information retrieval,Method analysis,Stack traces},
pages = {177--192},
publisher = {Elsevier B.V.},
title = {{Improved bug localization based on code change histories and bug reports}},
volume = {82},
year = {2017}
}
@INPROCEEDINGS{BugScout, 
	author={A. T. Nguyen and T. T. Nguyen and J. Al-Kofahi and H. V. Nguyen and T. N. Nguyen}, 
	booktitle={2011 26th IEEE/ACM International Conference on Automated Software Engineering (ASE 2011)}, 
	title={A topic-based approach for narrowing the search space of buggy files from a bug report}, 
	year={2011}, 
	pages={263-272}, 
	keywords={program debugging;software engineering;text analysis;BugScout;bug report;buggy code;buggy files;search space;software development;source code;textual contents;topic based approach;Prediction algorithms;Software algorithms;Software systems;Synchronization;Training;Vectors;Defect Localization;Topic Modeling}, 
	doi={10.1109/ASE.2011.6100062}, 
	ISSN={1938-4300}, 
	month={Nov},}
@INPROCEEDINGS{BugTriage, 
	author={G. Yang and T. Zhang and B. Lee}, 
	booktitle={2014 IEEE 38th Annual Computer Software and Applications Conference}, 
	title={Towards Semi-automatic Bug Triage and Severity Prediction Based on Topic Model and Multi-feature of Bug Reports}, 
	year={2014}, 
	pages={97-106}, 
	keywords={program debugging;public domain software;software maintenance;Eclipse;Mozilla;Net beans;bug fixing;bug repository;bug resolution;bug severity prediction;historical bug reports;open source projects;semiautomatic bug triage;software maintenance;topic extraction;topic model;Accuracy;Computer bugs;Feature extraction;Predictive models;Social network services;Software;Vectors;bug triage;corrective software maintenance;multi-feature;severity prediction;topic model}, 
	doi={10.1109/COMPSAC.2014.16}, 
	month={July},}
@article{Capobianco2013,
  title={Improving IR-based traceability recovery via noun-based indexing of software artifacts},
  author={Capobianco, Giovanni and Lucia, Andrea De and Oliveto, Rocco and Panichella, Annibale and Panichella, Sebastiano},
  journal={Journal of Software: Evolution and Process},
  volume={25},
  number={7},
  pages={743--762},
  year={2013},
  publisher={Wiley Online Library}
}
@article{Carvalho2015,
abstract = {Program comprehension techniques often explore program identifiers, to infer knowledge about programs. The relevance of source code identifiers as one relevant source of information about programs is already established in the literature, as well as their direct impact on future comprehension tasks. Most programming languages enforce some constrains on identifiers strings (e.g., white spaces or commas are not allowed). Also, programmers often use word combinations and abbreviations, to devise strings that represent single, or multiple, domain concepts in order to increase programming linguistic efficiency (convey more semantics writing less). These strings do not always use explicit marks to distinguish the terms used (e.g., CamelCase or underscores), so techniques often referred as hard splitting are not enough. This paper introduces Lingua::IdSplitter a dictionary based algorithm for splitting and expanding strings that compose multi-term identifiers. It explores the use of general programming and abbreviations dictionaries, but also a custom dictionary automatically generated from software natural language content, prone to include application domain terms and specific abbreviations. This approach was applied to two software packages, written in C, achieving a f-measure of around 90{\%} for correctly splitting and expanding identifiers. A comparison with current state-of-the-art approaches is also presented.},
author = {Carvalho, Nuno Ramos and Almeida, Jos{\'{e}} Jo{\~{a}}o and Henriques, Pedro Rangel and Varanda, Maria Jo{\~{a}}o},
doi = {10.1016/j.jss.2014.10.013},
issn = {01641212},
journal = {J. Syst. Softw.},
keywords = {Identifier splitting,Natural language processing,Program comprehension},
pages = {117--128},
title = {{From source code identifiers to natural language terms}},
volume = {100},
year = {2015}
}
@article{ChangeScribe,
abstract = {During software maintenances tasks, commit messages are an important source of information, knowledge, and documentation that developers rely upon. However, the number and nature of daily activities and interruptions can influence the quality of resulting commit messages. This formal demonstration paper presents ChangeScribe, a tool for automatically generating commit messages. ChangeScribe is available at http://www.cs.wm.edu/semeru/changescribe (Eclipse plugin, instructions, demos and the source code).},
author = {Linares-Vasquez, Mario and Cortes-Coy, Luis Fernando and Aponte, Jairo and Poshyvanyk, Denys},
doi = {10.1109/ICSE.2015.229},
isbn = {9781479919345},
issn = {02705257},
journal = {Proceedings - International Conference on Software Engineering},
keywords = {Code changes,Commit message,Summarization},
pages = {709--712},
title = {{ChangeScribe: A Tool for Automatically Generating Commit Messages}},
volume = {2},
year = {2015}
}
@article{Chen2017,
abstract = {We study methods for supervised community detection on graphs. This estimation problem is typically formulated in terms of the spectrum of certain operators, as well as with posterior inference under certain probabilistic graphical models. Focusing on random graph families such as the Stochastic Block Model, recent research has unified both approaches, and identified both statistical and computational signal-to-noise detection thresholds. We identify the resulting class of algorithms with a family of Graph Neural Networks and show that they can reach those detection thresholds in a purely data-driven manner, without access to the underlying generative models and with no parameter assumptions. For that purpose, we propose to augment GNNs with the non-Backtracking operator, defined on the line graph of edge adjacencies. We also perform the first analysis of optimization landscape on a simplified GNN family, revealing an interesting transition from rugged to simple as the graph size increases. Finally, the resulting model is also tested on real datasets, performing significantly better than rigid parametric models.},
archivePrefix = {arXiv},
arxivId = {1705.08415},
author = {Chen, Zhengdao and Li, Xiang and Bruna, Joan},
eprint = {1705.08415},
title = {{Supervised Community Detection with Hierarchical Graph Neural Networks}},
url = {http://arxiv.org/abs/1705.08415},
year = {2017}
}
@article{Cleland-Huang2014,
	abstract = {Software traceability is a sought-after, yet often elusive quality in software-intensive systems. Required in safety-critical systems by many certifying bodies, such as the USA Federal Aviation Authority, software traceability is an essential element of the software development process. In practice, traceability is often conducted in an ad-hoc, after-the-fact manner and, therefore, its benefits are not always fully realized. Over the past decade, researchers have focused on specific areas of the traceability problem, developing more sophisticated tooling, promoting strategic planning, applying information retrieval techniques capable of semi-automating the trace creation and maintenance process, developing new trace query languages and visualization techniques that use trace links, and applying traceability in specific domains such as Model Driven Development, product line systems, and agile project environments. In this paper, we build upon a prior body of work to highlight the state-of-the-art in software traceability, and to present compelling areas of research that need to be addressed.},
	author = {Cleland-Huang, Jane and Gotel, Olly and Hayes, Jane Huffman and M{\"{a}}der, Patrick and Zisman, Andrea},
	doi = {10.1145/2593882.2593891},
	file = {:D$\backslash$:/Downloads/Cleland-Huang et al., 2014, Software Traceability Trends and Future Directions.pdf:pdf},
	isbn = {978-1-4503-2865-4},
	journal = {FOSE 2014: Proceedings of the on Future of Software Engineering (36th ICSE 2014)},
	keywords = {road map,software traceability},
	pages = {55--69},
	title = {{Software Traceability: Trends and Future Directions}},
	year = {2014}
}
@incollection{cleland2012traceability,
  title={Traceability in agile projects},
  author={Cleland-Huang, Jane},
  booktitle={Software and Systems Traceability},
  pages={265--275},
  year={2012},
  publisher={Springer}
}
@misc{CodeCommentCorpus,
  title = {{Code Comment Corpus}},
  howpublished = "\url{https://github.com/PPPI/POSIT/blob/master/data/corpora/lucid.zip}",
  year = {2019}, 
  note = "[Online; accessed 24-Jan-2020]"
}
@article{Cohen1960,
author = {Jacob Cohen},
title ={A Coefficient of Agreement for Nominal Scales},
journal = {Educational and Psychological Measurement},
volume = {20},
number = {1},
pages = {37-46},
year = {1960},
doi = {10.1177/001316446002000104},

URL = {https://doi.org/10.1177/001316446002000104},
eprint = {https://doi.org/10.1177/001316446002000104}
}
@inproceedings{CombiningStructureWithLSI,
	author = {Maletic, Jonathan I. and Marcus, Andrian},
	title = {Supporting Program Comprehension Using Semantic and Structural Information},
	booktitle = {Proceedings of the 23rd International Conference on Software Engineering},
	series = {ICSE '01},
	year = {2001},
	isbn = {0-7695-1050-7},
	location = {Toronto, Ontario, Canada},
	pages = {103--112},
	numpages = {10},
	acmid = {381484},
	publisher = {IEEE Computer Society},
	address = {Washington, DC, USA},
}
@article{CommentCoheranceSemanticSimilarity,
	abstract = {Bug fixing is an essential activity in the software maintenance, because most of the software systems have unavoidable defects. When new bugs are submitted, triagers have to find and assign appropriate developers to fix the bugs. However, if the bugs are at first assigned to inappropriate developers, they may later have to be reassigned to other developers. That increases the time and cost for fixing bugs. Therefore, finding appropriate developers becomes a key to bug resolution. When triagers assign a new bug report, it is necessary to decide how quickly the bug report should be addressed. Thus, the bug severity is an important factor in bug fixing. In this paper, we propose a novel method for the bug triage and bug severity prediction. First, we extract topic(s) from historical bug reports in the bug repository and find bug reports related to each topic. When a new bug report arrives, we decide the topic(s) to which the report belongs. Then we utilize multi-feature to identify corresponding reports that have the same multi-feature (e.g., Component, product, priority and severity) with the new bug report. Thus, given a new bug report, we are able to recommend the most appropriate developer to fix each bug and predict its severity. To evaluate our approach, we not only measured the effectiveness of our study by using about 30,000 golden bug reports extracted from three open source projects (Eclipse, Mozilla, and Net beans), but also compared some related studies. The results show that our approach is likely to effectively recommend the appropriate developer to fix the given bug and predict its severity.},
	author = {Yang, Geunseok and Zhang, Tao and Lee, Byungjeong},
	doi = {10.1109/COMPSAC.2014.16},
	isbn = {978-1-4799-3575-8},
	issn = {07303157},
	journal = {Proceedings - International Computer Software and Applications Conference},
	keywords = {bug triage,corrective software maintenance,multi-feature,severity prediction,topic model},
	pages = {97--106},
	title = {{Towards semi-automatic bug triage and severity prediction based on topic model and multi-feature of bug reports}},
	year = {2014}
}
@article{ConceptLocalisation,
author = {Abebe, Surafel Lemma and Alicante, Anita and Corazza, Anna and Tonella, Paolo},
doi = {10.1016/j.jss.2013.07.009},
issn = {01641212},
journal = {Journal of Systems and Software},
month = {nov},
number = {11},
pages = {2919--2938},
title = {{Supporting concept location through identifier parsing and ontology extraction}},
volume = {86},
year = {2013}
}
@inproceedings{Constant:2011:MPT:2021121.2021134,
 author = {Constant, Matthieu and Sigogne, Anthony},
 title = {MWU-aware part-of-speech Tagging with a CRF Model and Lexical Resources},
 booktitle = {Proc. Workshop Multiword Expr.},
 series = {MWE '11},
 year = {2011},
 isbn = {978-1-932432-97-8},
 location = {Portland, Oregon},
 pages = {49--56},
 numpages = {8},
 url = {http://dl.acm.org/citation.cfm?id=2021121.2021134},
 acmid = {2021134},
 publisher = {Association for Computational Linguistics},
 address = {Stroudsburg, PA, USA},
}
@inproceedings{constant:hal-00790624,
  TITLE = {{Evaluating the Impact of External Lexical Resources into a CRF-based Multiword Segmenter and part-of-speech Tagger}},
  AUTHOR = {Constant, Mathieu and Tellier, Isabelle},
  URL = {https://hal-upec-upem.archives-ouvertes.fr/hal-00790624},
  BOOKTITLE = {{8th Int. Conf. Lang. Res. Eval. (LREC'12)}},
  ADDRESS = {Turkey},
  PAGES = {646-650},
  YEAR = {2012},
  MONTH = May,
  KEYWORDS = {Multiword Expressions ; Part Of Speech tagging ; Statistical and machine learning methods ; Conditional Random Fields ; Morphosyntactic lexicons ; lexical segmentation},
  PDF = {https://hal-upec-upem.archives-ouvertes.fr/hal-00790624/file/constant-tellier-lrec2012.pdf},
  HAL_ID = {hal-00790624},
  HAL_VERSION = {v1},
}
@article{CVB-LDA,
abstract = {Latent Dirichlet allocation (LDA) is a Bayesian network that has recently gained much popularity in applications ranging from document modeling to computer vision. Due to the large scale nature of these applications, current inference procedures like variational Bayes and Gibbs sampling have been found lacking. In this paper we propose the collapsed variational Bayesian inference algorithm for LDA, and show that it is computationally efficient, easy to implement and significantly more accurate than standard variational Bayesian inference for LDA.},
author = {Teh, Yee Whye and Newman, David and Welling, Max and Neaman, D},
isbn = {0-262-19568-2},
issn = {1049-5258},
journal = {Advances in Neural Information Processing Systems 19 (NIPS 2006)},
keywords = {LDA: collapsed variational Bayes,Taylor expansion,approximation},
pages = {1353--1360},
title = {{A Collapsed Variational Bayesian Inference Algorithm for Latent Dirichlet Allocation}},
year = {2007}
}
@INPROCEEDINGS{DeepLearningForTraces, 
	author={J. Guo and J. Cheng and J. Cleland-Huang}, 
	booktitle={2017 IEEE/ACM 39th International Conference on Software Engineering (ICSE)}, 
	title={Semantically Enhanced Software Traceability Using Deep Learning Techniques}, 
	year={2017}, 
	pages={3-14}, 
	keywords={Machine learning;Natural language processing;Recurrent neural networks;Semantics;Software;Standards;Training;Deep Learning;Recurrent Neural Network;Semantic Representation;Traceability}, 
	doi={10.1109/ICSE.2017.9}, 
	month={May},}
@inproceedings{Dias2015,
abstract = {After working for some time, developers commit their code changes to a version control system. When doing so, they often bundle unrelated changes (e.g., bug fix and refactoring) in a single commit, thus creating a so-called tangled commit. Sharing tangled commits is problematic because it makes review, reversion, and integration of these commits harder and historical analyses of the project less reliable. Researchers have worked at untangling existing commits, i.e., finding which part of a commit relates to which task. In this paper, we contribute to this line of work in two ways: (1) A publicly available dataset of untangled code changes, created with the help of two developers who accurately split their code changes into self contained tasks over a period of four months; (2) a novel approach, EpiceaUntangler, to help developers share untangled commits (aka. atomic commits) by using fine-grained code change information. EpiceaUntangler is based and tested on the publicly available dataset, and further evaluated by deploying it to 7 developers, who used it for 2 weeks. We recorded a median success rate of 91{\%} and average one of 75{\%}, in automatically creating clusters of untangled fine-grained code changes.},
archivePrefix = {arXiv},
arxivId = {1502.06757},
author = {Dias, Martin and Bacchelli, Alberto and Gousios, Georgios and Cassou, Damien and Ducasse, Stephane},
booktitle = {2015 IEEE 22nd Int. Conf. Softw. Anal. Evol. Reengineering, SANER 2015 - Proc.},
doi = {10.1109/SANER.2015.7081844},
eprint = {1502.06757},
file = {:D$\backslash$:/OneDrive/Documents/Mendeley Desktop/Dias et al/2015 IEEE 22nd International Conference on Software Analysis, Evolution, and Reengineering, SANER 2015 - Proceedings/Dias et al.{\_}2015{\_}Untangling fine-grained code changes.pdf:pdf},
isbn = {9781479984695},
issn = {1534-5351},
mendeley-groups = {Untangle},
month = {mar},
pages = {341--350},
publisher = {IEEE},
title = {{Untangling fine-grained code changes}},
url = {http://ieeexplore.ieee.org/document/7081844/},
year = {2015}
}
@inproceedings{dietrich2019man,
  title={Man vs machine: a study into language identification of stack overflow code snippets},
  author={Dietrich, Jens and Luczak-Roesch, Markus and Dalefield, Elroy},
  booktitle={Proc. 16th Int. Conf. Min. Soft. Repo.},
  pages={205--209},
  year={2019},
  organization={IEEE Press}
}
@INPROCEEDINGS{DifferentVectorRepresentations, 
	author={G. Capobianco and A. De Lucia and R. Oliveto and A. Panichella and S. Panichella}, 
	booktitle={2009 16th Working Conference on Reverse Engineering}, 
	title={Traceability Recovery Using Numerical Analysis}, 
	year={2009}, 
	pages={195-204}, 
	keywords={information retrieval;software engineering;source coding;Jensen-Shannon method;artifact language;code;information retrieval;latent semantic indexing;numerical analysis;software documentation;traceability recovery;vector space model;vector-based IR models;Documentation;Indexing;Information retrieval;Interpolation;Large scale integration;Numerical analysis;Reverse engineering;Software engineering;Software maintenance;Spline;B-splines;Empirical Studies;Information Retrieval;Jensen-Shannon method;Latent Semantic Indexing;Numerical Analysis;Traceability Recovery;Vector Space Model}, 
	doi={10.1109/WCRE.2009.14}, 
	ISSN={1095-1350}, 
	month={Oct},}
@article{EarlyFailedImplementPrediction,
abstract = {Online feature request management systems are popular tools for gathering stakeholders' change requests during system evolution. Deciding which feature requests require attention and how much upfront analysis to perform on them is an important problem in this context: too little upfront analysis may result in inadequate functionalities being developed, costly changes, and wasted development effort; too much upfront analysis is a waste of time and resources. Early predictions about which feature requests are most likely to fail due to insufficient or inadequate upfront analysis could facilitate such decisions. Our objective is to study whether it is possible to make such predictions automatically from the characteristics of the online discussions on feature requests. This paper presents a study of feature request failures in seven large projects, an automated tool-implemented framework for constructing failure prediction models, and a comparison of the performance of the different prediction techniques for these projects. The comparison relies on a cost-benefit model for assessing the value of additional upfront analysis. In this model, the value of additional upfront analysis depends on its probability of success in preventing failures and on the relative cost of the failures it prevents compared to its own cost. We show that for reasonable estimations of these two parameters, automated prediction models provide more value than a set of baselines for many failure types and projects. This suggests automated failure prediction during requirements elicitation to be a promising approach for guiding requirements engineering efforts in online settings.},
author = {Fitzgerald, Camilo and Letier, Emmanuel and Finkelstein, Anthony},
doi = {10.1007/s00766-012-0150-7},
isbn = {9781457709234},
issn = {09473602},
journal = {Requirements Engineering},
keywords = {Cost-benefit of requirements engineering,Early failure prediction,Feature requests management systems,Global software development,Open source},
number = {2},
pages = {117--132},
title = {{Early failure prediction in feature request management systems: An extended study}},
volume = {17},
year = {2012}
}
@inproceedings{Enslen2009,
abstract = {Automated software engineering tools (e.g., program search, concern location, code reuse, quality assessment, etc.) increasingly rely on natural language information from comments and identifiers in code. The first step in analyzing words from identifiers requires splitting identifiers into their constituent words. Unlike natural languages, where space and punctuation are used to delineate words, identifiers cannot contain spaces. One common way to split identifiers is to follow programming language naming conventions. For example, Java programmers often use camel case, where words are delineated by uppercase letters or non-alphabetic characters. However, programmers also create identifiers by concatenating sequences of words together with no discernible delineation, which poses challenges to automatic identifier splitting. In this paper, we present an algorithm to automatically split identifiers into sequences of words by mining word frequencies in source code. With these word frequencies, our identifier splitter uses a scoring technique to automatically select the most appropriate partitioning for an identifier. In an evaluation of over 8000 identifiers from open source Java programs, our Samurai approach outperforms the existing state of the art techniques.},
author = {Enslen, Eric and Hill, Emily and Pollock, Lori and Vijay-Shanker, K.},
booktitle = {Proc. 2009 6th IEEE Int. Work. Conf. Min. Softw. Repos. MSR 2009},
doi = {10.1109/MSR.2009.5069482},
isbn = {9781424434930},
issn = {2160-1852},
pages = {71--80},
title = {{Mining source code to automatically split identifiers for software analysis}},
year = {2009}
}
@inproceedings{Eshkevari2010,
abstract = {Identifiers are an important source of information during program understanding and maintenance. Programmers often use identifiers to build their mental models of the software artifacts. We have performed a preliminary study to examine the relation between the terms in identifiers, their spread in entities, and fault proneness. We introduced term entropy and context-coverage to measure how scattered terms are across program entities and how unrelated are the methods and attributes containing these terms. Our results showed that methods and attributes containing terms with high entropy and context-coverage are more fault-prone. We plan to build on this study by extracting linguistic information form methods and classes. Using this information, we plan to establish traceability link from domain concepts to source code, and to propose linguistic based refactoring.},
author = {Eshkevari, Laleh Mousavi},
booktitle = {Proc. - Work. Conf. Reverse Eng. WCRE},
doi = {10.1109/WCRE.2010.44},
isbn = {9780769541235},
issn = {10951350},
pages = {297--300},
title = {{Linguistic driven refactoring of source code identifiers}},
year = {2010}
}
@INPROCEEDINGS{ExplainingDefects, 
	author={T. H. Chen and S. W. Thomas and M. Nagappan and A. E. Hassan}, 
	booktitle={2012 9th IEEE Working Conference on Mining Software Repositories (MSR)}, 
	title={Explaining software defects using topic models}, 
	year={2012}, 
	pages={189-198}, 
	keywords={software fault tolerance;software metrics;software quality;statistical analysis;Eclipse;I/O tasks;Mozilla Firefox;Mylyn;business logic;code lines;code quality;defect-proneness;historical metrics;software defects;software development;software project;source code entities;statistical topic modeling technique;structural metrics;Correlation;Fires;History;Java;Measurement;Software systems;code quality;software concerns;topic modeling}, 
	doi={10.1109/MSR.2012.6224280}, 
	ISSN={2160-1852}, 
	month={June},}
@Article{Falessi2017,
author="Falessi, Davide
and Di Penta, Massimiliano
and Canfora, Gerardo
and Cantone, Giovanni",
title="Estimating the number of remaining links in traceability recovery",
journal="Empirical Software Engineering",
year="2017",
month="Jun",
day="01",
volume="22",
number="3",
pages="996--1027",
abstract="Although very important in software engineering, establishing traceability links between software artifacts is extremely tedious, error-prone, and it requires significant effort. Even when approaches for automated traceability recovery exist, these provide the requirements analyst with a, usually very long, ranked list of candidate links that needs to be manually inspected. In this paper we introduce an approach called Estimation of the Number of Remaining Links (ENRL) which aims at estimating, via Machine Learning (ML) classifiers, the number of remaining positive links in a ranked list of candidate traceability links produced by a Natural Language Processing techniques-based recovery approach. We have evaluated the accuracy of the ENRL approach by considering several ML classifiers and NLP techniques on three datasets from industry and academia, and concerning traceability links among different kinds of software artifacts including requirements, use cases, design documents, source code, and test cases. Results from our study indicate that: (i) specific estimation models are able to provide accurate estimates of the number of remaining positive links; (ii) the estimation accuracy depends on the choice of the NLP technique, and (iii) univariate estimation models outperform multivariate ones.",
issn="1573-7616",
doi="10.1007/s10664-016-9460-6",
url="https://doi.org/10.1007/s10664-016-9460-6"
}
@inproceedings{Falleri2010,
abstract = {A large part of the time allocated to software maintenance is dedicated to the program comprehension. Many approaches that uses the program structure or the external documentation have been created to assist program comprehension. However, the identifiers of the program are an important source of information that is still not widely used for this purpose. In this article, we propose an approach, based upon Natural Language Processing techniques, that automatically extracts and organizes concepts from software identifiers in a WordNet-like structure that we call lexical views. These lexical views give useful insight on an overall software architecture and can be used to improve results of many software engineering tasks. The proposal is evaluated against a corpus of 24 open source programs. {\textcopyright} 2010 IEEE.},
author = {Falleri, J. R. and Huchard, M. and Lafourcade, M. and Nebut, C. and Prince, V. and Dao, M.},
booktitle = {IEEE Int. Conf. Progr. Compr.},
doi = {10.1109/ICPC.2010.12},
isbn = {9780769541136},
issn = {1092-8138},
month = {jun},
pages = {4--13},
pmid = {23839494},
publisher = {IEEE},
title = {{Automatic extraction of a WordNet-like identifier network from software}},
url = {http://ieeexplore.ieee.org/document/5521783/},
year = {2010}
} 
@inproceedings{Ferrante1984,
abstract = {In this paper we present an intermediate program representation, called the program dependence graph {\{}(PDG){\}}, that makes explicit both the data and control dependences for each operation in a program. Data dependences have been used to represent only the relevant data flow relationships of a program. Control dependences are introduced to analogously represent only the essential control flow relationships of a program. Control dependences are derived from the usual control flow graph. Many traditional optimizations operate more efficiently on the {\{}PDG.{\}} Since dependences in the {\{}PDG{\}} connect computationally related parts of the program, a single walk of these dependences is sufficient to perform many optimizations. The {\{}PDG{\}} allows transformations such as vectorization, that previously required special treatment of control dependence, to be performed in a manner that is uniform for both control and data dependences. Program transformations that require interaction of the two dependence types can also be easily handled with our representation. As an example, an incremental approach to modifying data dependences resulting from branch deletion or loop unrolling is introduced. The {\{}PDG{\}} supports incremental optimization, permitting transformations to be triggered by one another and applied only to affected dependences.},
author = {Ferrante, Jeanne and Ottenstein, Karl J. and Warren, Joe D},
booktitle = {Lect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics)},
doi = {10.1007/3-540-12925-1_33},
isbn = {9783540129257},
issn = {16113349},
keywords = {D34 [Programming Languages]: Processors-compilers,Languages,Performance Additional Key Words and Phrases: Data,branch deletion,code motion,debugging,dependence analysis,incremental data flow analysis,intermediate program represen-tation,internal program form,loop fusion,loop unrolling,node splitting,optimization General Terms: Algorithms,parallelism,slicing,vectorization},
mendeley-groups = {Untangle},
pages = {125--132},
title = {{The program dependence graph and its use in optimization}},
url = {https://www.cs.utexas.edu/{~}pingali/CS395T/2009fa/papers/ferrante87.pdf},
volume = {167 LNCS},
year = {1984}
}


@article{FixingCommits,
abstract = {Background: software engineering research (SE) lacks theory and methodologies for addressing human aspects in software development. Development tasks are undertaken through cognitive processing activities. Affects (emotions, moods, feelings) have a linkage to cognitive processing activities and the productivity of individuals. SE research needs to incorporate affect measurements to valorize human factors and to enhance management styles. Objective: analyze the affects dimensions of valence, arousal, and dominance of software developers and their real-time correlation with their self-assessed productivity (sPR). Method: repeated measurements design with 8 participants (4 students, 4 professionals), conveniently sampled and studied individually over 90 minutes of programming. The analysis was performed by fitting a linear mixed- effects (LME) model. Results: valence and dominance are positively correlated with the sPR. The model was able to express about 38{\%} of deviance from the sPR. Many lessons were learned when employing psychological measurements in SE and for fitting LME. Conclusion: this article demonstrates the value of applying psychological tests in SE and echoes a call to valorize the human, individualized aspects of software developers. It reports a body of knowledge about affects, their classification, their measurement, and the best practices to perform psychological measurements in SE with LME models.},
archivePrefix = {arXiv},
arxivId = {1408.1293},
author = {Tufano, Michele and Bavota, Gabriele and Poshyvanyk, Denys and {Di Penta}, Massimiliano and Oliveto, Rocco and {De Lucia}, Andrea},
doi = {10.1002/smr.1797},
eprint = {1408.1293},
isbn = {9781450330565},
issn = {20477473},
journal = {Journal of Software: Evolution and Process},
keywords = {Conceptual schema,Design pattern detection,Object-relational mapping,Reverse engineering},
month = {aug},
number = {12},
pages = {1172--1192},
pmid = {67195556},
title = {{An empirical study on developer-related factors characterizing fix-inducing commits}},
volume = {26},
year = {2016}
}

@article{fonseca2015evaluating,
  title={Evaluating word embeddings and a revised corpus for part-of-speech tagging in Portuguese},
  author={Fonseca, Erick R and Rosa, Jo{\~a}o Lu{\'\i}s G and Alu{\'\i}sio, Sandra Maria},
  journal={Journal of the Brazilian Computer Society},
  volume={21},
  number={1},
  pages={2},
  year={2015},
  publisher={SpringerOpen}
}

@article{FRLink,
abstract = {Context: Though linking issues and commits plays an important role in software verification and maintenance, such link information is not always explicitly provided during software development or maintenance activities. Current practices in recovering such links highly depend on tedious manual examination. To automatically recover missing links, several approaches have been proposed to compare issue reports with log messages and source code files in commits. However, none of such approaches looked at the role of non-source code complementary documents in commits; nor did they consider the distinct roles each piece of the source code played in the same commit. Objective: We propose to revisit the definition of relevant files contributing to missing link recovery. More specifically, our work extends existing approaches from two perspectives: (1) Inclusion extension: incorporating complementary documents (i.e., non-source documents) to learn from more relevant data; (2) Exclusion extension: analyzing and filtering out irrelevant source code files to reduce data noise. Method: We propose a File Relevance-based approach (FRLink), to implement the above two considerations. FRLink utilizes non-source documents in commits, since they typically clarify code changes details, with similar textual information from corresponding issues. Moreover, FRLink differentiates the roles of different source code files in a single commit and discards files containing no similar code terms as those in issues based on similarity analysis. Results: FRLink is evaluated on 6 projects and compared with RCLinker, which is the latest state-of-the-art approach in missing link recovery. The result shows that FRLink outperforms RCLinker in F-Measure by 40.75{\%} when achieving the highest recalls. Conclusion: FRLink can significantly improve the performance of missing link recovery compared with existing approaches. This indicates that in missing link recovery studies, sophisticated data selection and processing techniques necessitate more discussions due to the increasing variety and volume of information associated with issues and commits.},
author = {Sun, Yan and Wang, Qing and Yang, Ye},
doi = {10.1016/j.infsof.2016.11.010},
issn = {09505849},
journal = {Information and Software Technology},
pages = {33--47},
title = {{FRLink: Improving the recovery of missing issue-commit links by revisiting file relevance}},
volume = {84},
year = {2017}
}

@inproceedings{gensim,
	title = {{Software Framework for Topic Modelling with Large Corpora}},
	author = {Radim {\v R}eh{\r u}{\v r}ek and Petr Sojka},
	booktitle = {{Proceedings of the LREC 2010 Workshop on New
	Challenges for NLP Frameworks}},
	pages = {45--50},
	year = 2010,
	month = May,
	day = 22,
	publisher = {ELRA},
	address = {Valletta, Malta},
	note={http://is.muni.cz/publication/884893/en},
	language={English}
}

@misc{ghlinkdoc,
	title={{GitHub: Autolinked references and URLs}},
	author={{GitHub}},
	howpublished={\url{https://help.github.com/articles/autolinked-references-and-urls/}},
	note = {Accessed: 2017-08-20},
	year = {2017}
}

@inproceedings{GHTorrent,
author = {Gousios, Georgios and Spinellis, D.},
booktitle = {2012 9th IEEE Working Conference on Mining Software Repositories (MSR)},
doi = {10.1109/MSR.2012.6224294},
isbn = {978-1-4673-1761-0},
month = {jun},
pages = {12--21},
publisher = {IEEE},
title = {{GHTorrent: Github's data from a firehose}},
year = {2012}
}
@article{Goldwater2007,
author = {Goldwater, Sharon and Griffiths, Tom},
journal = {Proc. 45th Annu. Meet. Assoc. Comput. Linguist.},
pages = {744--751},
title = {{A fully Bayesian approach to unsupervised part-of-speech tagging}},
url = {http://aclweb.org/anthology/P07-1094},
year = {2007}
}

@article{Gonen2018,
abstract = {We focus on the problem of language modeling for code-switched language, in the context of automatic speech recognition (ASR). Language modeling for code-switched language is challenging for (at least) three reasons: (1) lack of available large-scale code-switched data for training; (2) lack of a replicable evaluation setup that is ASR directed yet isolates language modeling performance from the other intricacies of the ASR system; and (3) the reliance on generative modeling. We tackle these three issues: we propose an ASR-motivated evaluation setup which is decoupled from an ASR system and the choice of vocabulary, and provide an evaluation dataset for English-Spanish code-switching. This setup lends itself to a discriminative training approach, which we demonstrate to work better than generative language modeling. Finally, we present an effective training protocol that integrates small amounts of code-switched data with large amounts of monolingual data, for both the generative and discriminative cases.},
archivePrefix = {arXiv},
arxivId = {1810.11895},
author = {Gonen, Hila and Goldberg, Yoav},
eprint = {1810.11895},
title = {{Language Modeling for Code-Switching: Evaluation, Integration of Monolingual Data, and Discriminative Training}},
url = {http://arxiv.org/abs/1810.11895},
year = {2018}
}
@misc{GoogleGHTorrent,
	title = {{Google Cloud Public Table of GitHub Projects}},
	howpublished = {\url{https://bigquery.cloud.google.com/dataset/ghtorrent-bq:ght}},
	author = {Gousios, Georgios and Spinellis, D.},
	note = {{Contents from 2.9M public, open source licensed repositories on GitHub; Accessed: 2017-08-10}},
	year = {2017}
	}
@inproceedings{Haiduc2010a,
 author = {Haiduc, Sonia and Aponte, Jairo and Marcus, Andrian},
 title = {Supporting Program Comprehension with Source Code Summarization},
 booktitle = {Proc. 32Nd ACM/IEEE International Conference on Software Engineering - Volume 2},
 series = {ICSE '10},
 year = {2010},
 isbn = {978-1-60558-719-6},
 location = {Cape Town, South Africa},
 pages = {223--226},
 numpages = {4},
 url = {http://doi.acm.org/10.1145/1810295.1810335},
 doi = {10.1145/1810295.1810335},
 acmid = {1810335},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {program comprehension, summary, text summarization},
}
@INPROCEEDINGS{Haiduc2010b, 
author={S. Haiduc and J. Aponte and L. Moreno and A. Marcus}, 
booktitle={2010 17th Work. Conf. on Rev. Eng.}, 
title={On the Use of Automated Text Summarization Techniques for Summarizing Source Code}, 
year={2010}, 
volume={}, 
number={}, 
pages={35-44}, 
abstract={During maintenance developers cannot read the entire code of large systems. They need a way to get a quick understanding of source code entities (such as, classes, methods, packages, etc.), so they can efficiently identify and then focus on the ones related to their task at hand. Sometimes reading just a method header or a class name does not tell enough about its purpose and meaning, while reading the entire implementation takes too long. We study a solution which mitigates the two approaches, i.e., short and accurate textual descriptions that illustrate the software entities without having to read the details of the implementation. We create such descriptions using techniques from automatic text summarization. The paper presents a study that investigates the suitability of various such techniques for generating source code summaries. The results indicate that a combination of text summarization techniques is most appropriate for source code summarization and that developers generally agree with the summaries produced.}, 
keywords={software maintenance;automated text summarization technique;textual description;software entity;source code summarization;Lead;Large scale integration;Semantics;Software systems;Computer science;Correlation;text summarization;program comprehension}, 
doi={10.1109/WCRE.2010.13}, 
ISSN={2375-5369}, 
month={Oct},
}
@inproceedings{Hellendoorn2018,
abstract = {Dynamically typed languages such as JavaScript and Python are increasingly popular, yet static typing has not been totally eclipsed: Python now supports type annotations and languages like TypeScript offer a middle-ground for JavaScript: a strict superset of JavaScript, to which it transpiles, coupled with a type system that permits partially typed programs. However, static typing has a cost: adding annotations, reading the added syntax, and wrestling with the type system to fix type errors. Type inference can ease the transition to more statically typed code and unlock the benefits of richer compile-time information, but is limited in languages like JavaScript as it cannot soundly handle duck-typing or runtime evaluation via eval. We propose DeepTyper, a deep learning model that understands which types naturally occur in certain contexts and relations and can provide type suggestions, which can often be verified by the type checker, even if it could not infer the type initially. DeepTyper, leverages an automatically aligned corpus of tokens and types to accurately predict thousands of variable and function type annotations. Furthermore, we demonstrate that context is key in accurately assigning these types and introduce a technique to reduce overfitting on local cues while highlighting the need for further improvements. Finally, we show that our model can interact with a compiler to provide more than 4,000 additional type annotations with over 95{\%} precision that could not be inferred without the aid of DeepTyper. CCS CONCEPTS • Software and its engineering → Software notations and tools; Automated static analysis; • Theory of computation → Type structures; KEYWORDS},
author = {Hellendoorn, Vincent J and Bird, Christian and Barr, Earl T and Allamanis, Miltiadis},
booktitle = {Proc. 2018 26th ACM Jt. Meet. Eur. Softw. Eng. Conf. Symp. Found. Softw. Eng. - ESEC/FSE 2018},
doi = {10.1145/3236024.3236051},
mendeley-groups = {Code pseudo-PoS tagging},
pages = {152--162},
publisher = {ACM},
title = {{Deep learning type inference}},
url = {https://doi.org/10.1145/3236024.3236051 http://dl.acm.org/citation.cfm?doid=3236024.3236051},
volume = {18},
year = {2018}
}
@article{Herzig2013,
abstract = {When interacting with version control systems, developers often commit unrelated or loosely related code changes in a single transaction. When analyzing the version history, such tangled changes will make all changes to all modules appear related, possibly compromising the resulting analyses through noise and bias. In an investigation of five open-source Java projects, we found up to 15{\%} of all bug fixes to consist of multiple tangled changes. Using a multi-predictor approach to untangle changes, we show that on average at least 16.6{\%} of all source files are incorrectly associated with bug reports. We recommend better change organization to limit the impact of tangled changes.},
author = {Herzig, Kim and Zeller, Andreas},
doi = {10.1109/MSR.2013.6624018},
isbn = {9781467329361},
issn = {21601852},
journal = {IEEE Int. Work. Conf. Min. Softw. Repos.},
keywords = {Bias,Data quality,Mining software repositories,Noise,Tangled code changes},
mendeley-groups = {Untangle},
pages = {121--130},
title = {{The impact of tangled code changes}},
year = {2013}
}
@article{Herzig2016,
abstract = {When interacting with version control systems, developers often commit unrelated or loosely related code changes in a single transaction. When analyzing the version history, such tangled changes will make all changes to all modules appear related, possibly compromising the resulting analyses through noise and bias. In an investigation of five open-source Java projects, we found up to 15{\%} of all bug fixes to consist of multiple tangled changes. Using a multi-predictor approach to untangle changes, we show that on average at least 16.6{\%} of all source files are incorrectly associated with bug reports. We recommend better change organization to limit the impact of tangled changes.},
author = {Herzig, Kim and Just, Sascha and Zeller, Andreas},
doi = {10.1007/s10664-015-9376-6},
isbn = {9781467329361},
issn = {15737616},
journal = {Empir. Softw. Eng.},
keywords = {Data noise,Defect prediction,Untangling},
number = {2},
pages = {303--336},
title = {{The impact of tangled code changes on defect prediction models}},
volume = {21},
year = {2016}
}
@book{hogg_tanis_zimmerman_2020, place={Hoboken, NJ}, title={Probability and statistical inference}, publisher={Pearson}, author={Hogg, Robert V. and Tanis, Elliot A. and Zimmerman, Dale L.}, year={2020}}
@article{HOLLAND1983,
title = "Stochastic blockmodels: First steps",
journal = "Social Networks",
volume = "5",
number = "2",
pages = "109 - 137",
year = "1983",
issn = "0378-8733",
doi = "https://doi.org/10.1016/0378-8733(83)90021-7",
url = "http://www.sciencedirect.com/science/article/pii/0378873383900217",
author = "Paul W. Holland and Kathryn Blackmond Laskey and Samuel Leinhardt"
}
@inproceedings{Host2007,
abstract = {Method names make or break abstractions: good ones communicate the intention of the method, whereas bad ones cause confusion and frustration. The task of naming is subject to the whims and idiosyncracies of the individual since programmers have little to guide them except their personal experience. By analysing method implementations taken from a corpus of Java applications, we establish the meaning of verbs in method names based on actual use. The result is an automatically generated, domain-neutral lexicon of verbs, similar to a natural language dictionary, that represents the common usages of many programmers},
author = {H{\o}st, Einar W. and {\O}stvold, Bjarte M.},
booktitle = {SCAM 2007 - Proc. 7th IEEE Int. Work. Conf. Source Code Anal. Manip.},
doi = {10.1109/SCAM.2007.18},
isbn = {0769528805},
month = {sep},
pages = {193--202},
publisher = {IEEE},
title = {{The programmer's lexicon, volume I: The verbs}},
url = {http://ieeexplore.ieee.org/document/4362913/},
year = {2007}
}
@article{HowManyRF,
author = {Mayumi Oshiro, Thais and Santoro Perez, Pedro and Baranauskas, José},
year = {2012},
month = {07},
pages = {},
title = {How Many Trees in a Random Forest?},
volume = {7376},
journal = {Lecture notes in computer science}
}
@article{Huang2015,
abstract = {In this paper, we propose a variety of Long Short-Term Memory (LSTM) based models for sequence tagging. These models include LSTM networks, bidirectional LSTM (BI-LSTM) networks, LSTM with a Conditional Random Field (CRF) layer (LSTM-CRF) and bidirectional LSTM with a CRF layer (BI-LSTM-CRF). Our work is the first to apply a bidirectional LSTM CRF (denoted as BI-LSTM-CRF) model to NLP benchmark sequence tagging data sets. We show that the BI-LSTM-CRF model can efficiently use both past and future input features thanks to a bidirectional LSTM component. It can also use sentence level tag information thanks to a CRF layer. The BI-LSTM-CRF model can produce state of the art (or close to) accuracy on POS, chunking and NER data sets. In addition, it is robust and has less dependence on word embedding as compared to previous observations.},
archivePrefix = {arXiv},
arxivId = {1508.01991},
author = {Huang, Zhiheng and Xu, Wei and Yu, Kai},
doi = {10.1061/(ASCE)CO.1943-7862.0000274.},
eprint = {1508.01991},
isbn = {9781510827585},
issn = {0270-6474},
pmid = {3944616},
title = {{Bidirectional LSTM-CRF Models for Sequence Tagging}},
url = {http://arxiv.org/abs/1508.01991},
year = {2015}
}
@article{InternetScaleMining,
abstract = {Large repositories of source code available over the Internet, or within large organizations, create new challenges and opportunities for data mining and statistical machine learning. Here we first develop Sourcerer, an infrastructure for the automated crawling, parsing, fingerprinting, and database storage of open source software on an Internet-scale. In one experiment, we gather 4,632 Java projects from SourceForge and Apache totaling over 38 million lines of code from 9,250 developers. Simple statistical analyses of the data first reveal robust power-law behavior for package, method call, and lexical containment distributions. We then develop and apply unsupervised, probabilistic, topic and author-topic (AT) models to automatically discover the topics embedded in the code and extract topic-word, document-topic, and AT distributions. In addition to serving as a convenient summary for program function and developer activities, these and other related distributions provide a statistical and information-theoretic basis for quantifying and analyzing source file similarity, developer similarity and competence, topic scattering, and document tangling, with direct applications to software engineering an software development staffing. Finally, by combining software textual content with structural information captured by our CodeRank approach, we are able to significantly improve software retrieval performance, increasing the area under the curve (AUC) retrieval metric to 0.92– roughly 10–30{\%} better than previous approaches based on text alone. A prototype of the system is available at: http://sourcerer.ics.uci.edu .},
author = {Linstead, Erik and Bajracharya, Sushil and Ngo, Trung and Rigor, Paul and Lopes, Cristina and Baldi, Pierre},
doi = {10.1007/s10618-008-0118-x},
isbn = {1384-5810},
issn = {13845810},
journal = {Data Mining and Knowledge Discovery},
keywords = {Author-topic probabilistic modeling,Code retrieval,Code search,Mining software,Program understanding,Software analysis},
number = {2},
pages = {300--336},
title = {{Sourcerer: Mining and searching internet-scale software repositories}},
volume = {18},
year = {2009}
}
@inproceedings{Jamatia2015,
  title={part-of-speech tagging for code-mixed english-hindi twitter and facebook chat messages},
  author={Jamatia, Anupam and Gamb{\"a}ck, Bj{\"o}rn and Das, Amitava},
  booktitle={Proc. Int. Conf. Rec. Adv. in Nat. Lang. Proc.},
  pages={239--248},
  year={2015}
}
@misc{jiralinkdoc,
	title={{JIRA: Link JIRA issues to Confluence pages automatically}},
	author={{JIRA}},
	howpublished={\url{https://www.atlassian.com/blog/confluence/link-jira-issues-to-confluence-pages-automatically}},
	note = {Accessed: 2017-08-20},
	year = {2017}}
@inproceedings{Johnson:2013:WDS:2486788.2486877,
 author = {Johnson, Brittany and Song, Yoonki and Murphy-Hill, Emerson and Bowdidge, Robert},
 title = {Why Don\&\#039;T Software Developers Use Static Analysis Tools to Find Bugs?},
 booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
 series = {ICSE '13},
 year = {2013},
 isbn = {978-1-4673-3076-3},
 location = {San Francisco, CA, USA},
 pages = {672--681},
 numpages = {10},
 url = {http://dl.acm.org/citation.cfm?id=2486788.2486877},
 acmid = {2486877},
 publisher = {IEEE Press},
 address = {Piscataway, NJ, USA},
}
@inproceedings{Kalliamvakou2014,
abstract = {We are now witnessing the rapid growth of decentralized source code management (DSCM) systems, in which every developer has her own repository. DSCMs facilitate a style of collaboration in which work output can flow sideways (and privately) between collaborators, rather than always up and down (and publicly) via a central repository. Decentralization comes with both the promise of new data and the peril of its misinterpretation. We focus on git, a very popular DSCM used in high-profile projects. Decentralization, and other features of git, such as automatically recorded contributor attribution, lead to richer content histories, giving rise to new questions such as ldquoHow do contributions flow between developers to the official project repository?rdquo However, there are pitfalls. Commits may be reordered, deleted, or edited as they move between repositories. The semantics of terms common to SCMs and DSCMs sometimes differ markedly, potentially creating confusion. For example, a commit is immediately visible to all developers in centralized SCMs, but not in DSCMs. Our goal is to help researchers interested in DSCMs avoid these and other perils when mining and analyzing git data.},
address = {New York, New York, USA},
author = {Kalliamvakou, Eirini and Gousios, Georgios and Blincoe, Kelly and Singer, Leif and German, Daniel M. and Damian, Daniela},
booktitle = {Proc. 11th Work. Conf. Min. Softw. Repos. - MSR 2014},
doi = {10.1145/2597073.2597074},
isbn = {9781450328630},
keywords = {Mining software repositories,bias,code reviews,git,github},
pages = {92--101},
publisher = {ACM Press},
title = {{The promises and perils of mining GitHub}},
url = {http://dl.acm.org/citation.cfm?doid=2597073.2597074},
year = {2014}
}
@inproceedings{Kalliamvakou2015,
abstract = {Researchers are currently drawn to study projects hosted on GitHub due to its popularity, ease of obtaining data, and its distinctive built-in social features. GitHub has been found to create a transparent development environment, which together with a pull request-based workflow, provides a lightweight mech- anism for committing, reviewing and managing code changes. These features impact how GitHub is used and the benefits it provides to teams' development and collaboration. While most of the evidence we have is from GitHub's use in open source software (OSS) projects, GitHub is also used in an increasing number of commercial projects. It is unknown how GitHub supports these projects given that GitHub's workflow model does not intuitively fit the commercial development way of working. In this paper, we report findings from an online survey and interviews with GitHub users on how GitHub is used for collaboration in commercial projects. We found that many commercial projects adopted practices that are more typical of OSS projects including reduced communication, more independent work, and self-organization. We discuss how GitHub's transparency and popular workflow can promote open collaboration, allowing organizations to increase code reuse and promote knowledge sharing across their teams.},
author = {Kalliamvakou, Eirini and Damian, Daniela and Blincoe, Kelly and Singer, Leif and German, Daniel M},
booktitle = {Proc. - Int. Conf. Softw. Eng.},
doi = {10.1109/ICSE.2015.74},
isbn = {9781479919345},
issn = {02705257},
keywords = {GitHub,collaboration,commercial projects,coordination,open source,practices,pull requests,workflow},
mendeley-groups = {Aide-m{\'{e}}moire},
pages = {574--585},
title = {{Open source-style collaborative development practices in commercial projects using GitHub}},
volume = {1},
year = {2015}
}
@article{Kawrykow2011,
abstract = {Numerous techniques involve mining change data captured in software archives to assist engineering efforts, for example to identify components that tend to evolve together. We observed that important changes to software artifacts are sometimes accompanied by numerous non-essential modifications, such as local variable refactorings, or textual differences induced as part of a rename refactoring. We developed a tool-supported technique for detecting non-essential code differences in the revision histories of software systems. We used our technique to investigate code changes in over 24,000 change sets gathered from the change histories of seven long-lived open-source systems. We found that up to 15.5{\&}{\#}x025; of a system's method updates were due solely to non-essential differences. We also report on numerous observations on the distribution of non-essential differences in change history and their potential impact on change-based analyses.},
author = {Kawrykow, David and Robillard, Martin P.},
doi = {10.1145/1985793.1985842},
isbn = {9781450304450},
issn = {0270-5257},
journal = {Proceeding 33rd Int. Conf. Softw. Eng. - ICSE '11},
keywords = {differenc-,mining software repositories,software change analysis},
pages = {351},
title = {{Non-essential changes in version histories}},
url = {http://portal.acm.org/citation.cfm?doid=1985793.1985842},
year = {2011}
}
@article{Kersten2005,
	abstract = {Even when working on a well-modularized software system, programmers tend to spend more time navigating the code than working with it. This phenomenon arises because it is impossible to modularize the code for all tasks that occur over the lifetime of a system. We describe the use of a degree-of-interest (DOI) model to capture the task context of program elements scattered across a code base. The Mylar tool that we built encodes the DOI of program elements by monitoring the programmer's activity, and displays the encoded DOI model in views of Java and AspectJ programs. We also present the results of a preliminary diary study in which professional programmers used Mylar for their daily work on enterprise-scale Java systems.},
	author = {Kersten, Mik and Murphy, Gail C},
	doi = {10.1145/1052898.1052912},
	file = {:D$\backslash$:/Downloads/Kersten, Murphy, 2005, Mylar.pdf:pdf},
	isbn = {1595930434},
	issn = {0740-7459},
	journal = {Proceedings of the 4th international conference on Aspect-oriented software development - AOSD '05},
	keywords = {a cohesive,for instance,in the code,may need to examine,programmers must,repeatedly visit multiple places,system,the code corresponding to,they,to a large software,to make a change},
	pages = {159--168},
	title = {{Mylar}},
	year = {2005}
} 
@inproceedings{kim2006program,
  title={Program element matching for multi-version program analyses},
  author={Kim, Miryung and Notkin, David},
  booktitle={Proceedings of the 2006 international workshop on Mining software repositories},
  pages={58--64},
  year={2006}
}
@article{Kingma14,
  author = {Kingma, Diederik P. and Ba, Jimmy},
  ee = {http://arxiv.org/abs/1412.6980},
  journal = {CoRR},
  keywords = {dblp},
  timestamp = {2018-08-14T14:24:27.000+0200},
  title = {Adam: A Method for Stochastic Optimization.},
  url = {http://dblp.uni-trier.de/db/journals/corr/corr1412.html\#KingmaB14},
  volume = {abs/1412.6980},
  year = 2014
} 
@inproceedings{Kirinuki2014,
abstract = {Although there is a principle that states a commit should only include changes for a single task, it is not always respected by developers. This means that code repositories often include commits that contain tangled changes. The presence of such tangled changes hinders analyzing code repositories because most mining software repository (MSR) approaches are designed with the assumption that every commit includes only changes for a single task. In this paper, we propose a technique to inform developers that they are in the process of committing tangled changes. The proposed technique utilizes the changes included in the past commits to judge whether a given commit includes tangled changes. If it determines that the proposed commit may include tangled changes, it offers suggestions on how the tangled changes can be split into a set of untangled changes.},
address = {New York, New York, USA},
author = {Kirinuki, Hiroyuki and Higo, Yoshiki and Hotta, Keisuke and Kusumoto, Shinji},
booktitle = {Proc. 22nd Int. Conf. Progr. Compr. (ICPC 2014)},
doi = {10.1145/2597008.2597798},
isbn = {9781450328791},
keywords = {mining software repositories,tan-,understanding commits},
mendeley-groups = {Untangle},
pages = {262--265},
publisher = {ACM Press},
title = {{Hey! are you committing tangled changes?}},
url = {http://dl.acm.org/citation.cfm?doid=2597008.2597798},
year = {2014}
}
@article{Kirinuki2017,
abstract = {{\textcopyright} 2016 IEEE. It is generally said that we should not perform code changes formultiple tasks in a single commit. Such code changes are called tangledones. Committing tangled changes is harmful to developers. Forexample, it is costly to merge a part of tangled changes with othercommits. Moreover, the presence of such tangled changes hindersanalyzing code repositories. That is because most of the miningsoftware repository approaches are designed under the assumption thatevery commit includes only changes for a single task. In this paper, wepropose a technique which informs developers that they are about tocommit tangled changes. The technique also suggests how to split agiven commit into multiple commits by using past code changes. Theproposed technique allows developers to determine whether they acceptthe suggestion or commit as it stands. By providing such support todevelopers, they can avoid committing tangled changes.},
author = {Kirinuki, Hiroyuki and Higo, Yoshiki and Hotta, Keisuke and Kusumoto, Shinji},
doi = {10.1109/APSEC.2016.028},
isbn = {9781509055753},
issn = {15301362},
journal = {Proc. - Asia-Pacific Softw. Eng. Conf. APSEC},
keywords = {Change pattern,Repository mining,Source code analysis},
pages = {129--136},
title = {{Splitting commits via past code changes}},
year = {2017}
} 
@article{Knyazev2007,
abstract = {We describe our software package Block Locally Optimal Preconditioned Eigenvalue Xolvers (BLOPEX) publicly released recently. BLOPEX is available as a stand-alone serial library, as an external package to PETSc (``Portable, Extensible Toolkit for Scientific Computation'', a general purpose suite of tools for the scalable solution of partial differential equations and related problems developed by Argonne National Laboratory), and is also built into {\{}$\backslash$it hypre{\}} (``High Performance Preconditioners'', scalable linear solvers package developed by Lawrence Livermore National Laboratory). The present BLOPEX release includes only one solver--the Locally Optimal Block Preconditioned Conjugate Gradient (LOBPCG) method for symmetric eigenvalue problems. {\{}$\backslash$it hypre{\}} provides users with advanced high-quality parallel preconditioners for linear systems, in particular, with domain decomposition and multigrid preconditioners. With BLOPEX, the same preconditioners can now be efficiently used for symmetric eigenvalue problems. PETSc facilitates the integration of independently developed application modules with strict attention to component interoperability, and makes BLOPEX extremely easy to compile and use with preconditioners that are available via PETSc. We present the LOBPCG algorithm in BLOPEX for {\{}$\backslash$it hypre{\}} and PETSc. We demonstrate numerically the scalability of BLOPEX by testing it on a number of distributed and shared memory parallel systems, including a Beowulf system, SUN Fire 880, an AMD dual-core Opteron workstation, and IBM BlueGene/L supercomputer, using PETSc domain decomposition and {\{}$\backslash$it hypre{\}} multigrid preconditioning. We test BLOPEX on a model problem, the standard 7-point finite-difference approximation of the 3-D Laplacian, with the problem size in the range {\$}10{\^{}}5-10{\^{}}8{\$}.},
archivePrefix = {arXiv},
arxivId = {0705.2626},
author = {Knyazev, A V and Argentati, M E and Lashuk, I. and Ovtchinnikov, E E},
doi = {10.1137/060661624},
eprint = {0705.2626},
isbn = {1064827500},
issn = {1064-8275},
journal = {SIAM J. Sci. Comput.},
keywords = {BLOPEX,Beowulf.,BlueGene,Conjugate gradient,LOBPCG,Laplacian,PETSc,domain decomposition,eigenvalue,eigenvector,hypre,iterative method,multigrid,parallel computing,preconditioner,preconditioning},
mendeley-groups = {Untangle},
number = {5},
pages = {2224--2239},
title = {{Block Locally Optimal Preconditioned Eigenvalue Xolvers (BLOPEX) in Hypre and PETSc}},
url = {http://math.cudenver.edu/ http://epubs.siam.org/doi/10.1137/060661624},
volume = {29},
year = {2007}
}

@article {Krzakala2013,
	author = {Krzakala, Florent and Moore, Cristopher and Mossel, Elchanan and Neeman, Joe and Sly, Allan and Zdeborov{\'a}, Lenka and Zhang, Pan},
	title = {Spectral redemption in clustering sparse networks},
	volume = {110},
	number = {52},
	pages = {20935--20940},
	year = {2013},
	doi = {10.1073/pnas.1312486110},
	publisher = {National Academy of Sciences},
	abstract = {Spectral algorithms are widely applied to data clustering problems, including finding communities or partitions in graphs and networks. We propose a way of encoding sparse data using a {\textquotedblleft}nonbacktracking{\textquotedblright} matrix, and show that the corresponding spectral algorithm performs optimally for some popular generative models, including the stochastic block model. This is in contrast with classical spectral algorithms, based on the adjacency matrix, random walk matrix, and graph Laplacian, which perform poorly in the sparse case, failing significantly above a recently discovered phase transition for the detectability of communities. Further support for the method is provided by experiments on real networks as well as by theoretical arguments and analogies from probability theory, statistical physics, and the theory of random matrices.Spectral algorithms are classic approaches to clustering and community detection in networks. However, for sparse networks the standard versions of these algorithms are suboptimal, in some cases completely failing to detect communities even when other algorithms such as belief propagation can do so. Here, we present a class of spectral algorithms based on a nonbacktracking walk on the directed edges of the graph. The spectrum of this operator is much better-behaved than that of the adjacency matrix or other commonly used matrices, maintaining a strong separation between the bulk eigenvalues and the eigenvalues relevant to community structure even in the sparse case. We show that our algorithm is optimal for graphs generated by the stochastic block model, detecting communities all of the way down to the theoretical limit. We also show the spectrum of the nonbacktracking operator for some real-world networks, illustrating its advantages over traditional spectral clustering.},
	issn = {0027-8424},
	URL = {http://www.pnas.org/content/110/52/20935},
	eprint = {http://www.pnas.org/content/110/52/20935.full.pdf},
	journal = {Proceedings of the National Academy of Sciences}
}

@article{Kuhn1955,
author = {Kuhn, H. W.},
title = {The Hungarian method for the assignment problem},
journal = {Naval Research Logistics Quarterly},
volume = {2},
number = {1--2},
pages = {83-97},
doi = {10.1002/nav.3800020109},
abstract = {Abstract Assuming that numerical scores are available for the performance of each of n persons on each of n jobs, the “assignment problem” is the quest for an assignment of persons to jobs so that the sum of the n scores so obtained is as large as possible. It is shown that ideas latent in the work of two Hungarian mathematicians may be exploited to yield a new method of solving this problem.}
}

@inproceedings{lakshminarayanan2014mondrian,
  title={Mondrian forests: Efficient online random forests},
  author={Lakshminarayanan, Balaji and Roy, Daniel M and Teh, Yee Whye},
  booktitle={Advances in neural information processing systems},
  pages={3140--3148},
  year={2014}
}

@inproceedings{Lapata2001,
abstract = {In this paper we investigate polysemous adjectives whose meaning varies depending on the nouns they modify (e.g., fast). We acquire the meanings of these adjectives from a large corpus and propose a probabilistic model which provides a ranking on the set of possible interpretations. We identify lexical semantic information automatically by exploiting the consistent correspondences between surface syntactic cues and lexical meaning. We evaluate our results against paraphrase judgments elicited experimentally from humans and show that the model's ranking of meanings correlates reliably with human intuitions: meanings that are found highly probable by the model are also rated as plausible by the subjects.},
author = {Lapata, Maria},
booktitle = {Proc. Second Meet. North Am. Chapter Assoc. Comput. Linguist. Lang. Technol.},
doi = {http://dx.doi.org/10.3115/1073336.1073345},
pages = {1--8},
title = {{A corpus-based account of regular polysemy: the case of context-sensitive adjectives}},
year = {2001}
}


@inproceedings{Lawrie2011,
abstract = {Maintaining modern software requires significant tool support. Effective tools exploit a variety of information and techniques to aid a software maintainer. One area of recent interest in tool development exploits the natural language information found in source code. Such Information Retrieval (IR) based tools compliment traditional static analysis tools and have tackled problems, such as feature location, that otherwise require considerable human effort. To reap the full benefit of IR-based techniques, the language used across all software artifacts (e.g., requirements, design, change requests, tests, and source code) must be consistent. Unfortunately, there is a significant proportion of invented vocabulary in source code. Vocabulary normalization aligns the vocabulary found in the source code with that found in other software artifacts. Most existing work related to normalization has focused on splitting an identifier into its constituent parts. The next step is to expand each part into a (dictionary) word that matches the vocabulary used in other software artifacts. Building on a successful approach to splitting identifiers, an implementation of an expansion algorithm is presented. Experiments on two systems find that up to 66{\%} of identifiers are correctly expanded, which is within about 20{\%} of the current system's best-case performance. Not only is this performance comparable to previous techniques, but the result is achieved in the absence of special purpose rules and not limited to restricted syntactic contexts. Results from these experiments also show the impact that varying levels of documentation (including both internal documentation such as the requirements and design, and external, or user-level, documentation) have on the algorithm's performance.},
author = {Lawrie, Dawn and Binkley, Dave},
booktitle = {IEEE Int. Conf. Softw. Maintenance, ICSM},
doi = {10.1109/ICSM.2011.6080778},
isbn = {9781457706646},
issn = {1063-6773},
keywords = {natural language processing,program comprehension,source code analysis tools},
pages = {113--122},
title = {{Expanding identifiers to normalize source code vocabulary}},
year = {2011}
}

@article{lda,
abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
archivePrefix = {arXiv},
arxivId = {1111.6189},
author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
doi = {10.1111/j.1365-2966.2012.21196.x},
eprint = {1111.6189},
isbn = {9781577352815},
issn = {00358711},
journal = {Journal of Machine Learning Research},
month = {sep},
pmid = {21362469},
title = {{Latent Dirichlet Allocation}},
year = {2003}
}

@inproceedings{LDAAsClustering,
	author = {Gorla, Alessandra and Tavecchia, Ilaria and Gross, Florian and Zeller, Andreas},
	title = {Checking App Behavior Against App Descriptions},
	booktitle = {Proceedings of the 36th International Conference on Software Engineering},
	series = {ICSE 2014},
	year = {2014},
	isbn = {978-1-4503-2756-5},
	location = {Hyderabad, India},
	pages = {1025--1035},
	numpages = {11},
	doi = {10.1145/2568225.2568276},
	acmid = {2568276},
	publisher = {ACM},
	address = {New York, NY, USA},
	keywords = {Android, clustering, description analysis, malware detection},
}

@INPROCEEDINGS{LDASourceRetrival, 
	author={S. K. Lukins and N. A. Kraft and L. H. Etzkorn}, 
	booktitle={2008 15th Working Conference on Reverse Engineering}, 
	title={Source Code Retrieval for Bug Localization Using Latent Dirichlet Allocation}, 
	year={2008}, 
	pages={155-164}, 
	keywords={indexing;information retrieval;software engineering;LDA-based static technique;automatic bug localization;automating bug localization;information retrieval;latent Dirichlet allocation;latent semantic indexing;source code retrieval;Aging;Computer bugs;Costs;Indexing;Information retrieval;Large scale integration;Linear discriminant analysis;Reverse engineering;Software maintenance;Software systems;LDA;LSI;bug localization;information retrieval;latent Dirichlet allocation;latent semantic indexing;program comprehension}, 
	doi={10.1109/WCRE.2008.33}, 
	ISSN={1095-1350}, 
	month={Oct},}
@article{LDATime,
	author = {Thomas, Stephen W. and Adams, Bram and Hassan, Ahmed E. and Blostein, Dorothea},
	title = {Studying Software Evolution Using Topic Models},
	journal = {Sci. Comput. Program.},
	issue_date = {February, 2014},
	volume = {80},
	month = feb,
	year = {2014},
	issn = {0167-6423},
	pages = {457--479},
	numpages = {23},
	doi = {10.1016/j.scico.2012.08.003},
	acmid = {2564429},
	publisher = {Elsevier North-Holland, Inc.},
	address = {Amsterdam, The Netherlands, The Netherlands},
	keywords = {Latent Dirichlet allocation, Mining software repositories, Software evolution, Topic model},
} 
@INPROCEEDINGS{LDATime2, 
	author={A. Hindle and M. W. Godfrey and R. C. Holt}, 
	booktitle={2009 IEEE International Conference on Software Maintenance}, 
	title={What's hot and what's not: Windowed developer topic analysis}, 
	year={2009}, 
	pages={339-348}, 
	keywords={program visualisation;programming language semantics;software engineering;word processing;commit-log comments;latent dirichlet allocation tool;latent semantic indexing tool;software developers;software project development;windowed topic analysis;Control systems;Database systems;History;Indexing;Large scale integration;Linear discriminant analysis;Pattern analysis;Performance analysis;Software development management;Visualization}, 
	doi={10.1109/ICSM.2009.5306310}, 
	ISSN={1063-6773}, 
	month={Sept},} 
@article{LDATopicEvolution,
	abstract = {As development on a software project progresses, developers shift their focus between different topics and tasks many times. Managers and newcomer developers often seek ways of understanding what tasks have recently been worked on and how much effort has gone into each; for example, a manager might wonder what unexpected tasks occupied their team's attention during a period when they were supposed to have been implementing new features. Tools such as Latent Dirichlet Allocation (LDA) and Latent Semantic Indexing (LSI) can be used to extract a set of independent topics from a corpus of commit-log comments. Previous work in the area has created a single set of topics by analyzing comments from the entire lifetime of the project. In this paper, we propose windowing the topic analysis to give a more nuanced view of the system's evolution. By using a defined time-window of, for example, one month, we can track which topics come and go over time, and which ones recur. We propose visualizations of this model that allows us to explore the evolving stream of topics of development occurring over time. We demonstrate that windowed topic analysis offers advantages over topic analysis applied to a project's lifetime because many topics are quite local.},
	author = {Hindle, Abram and Godfrey, Michael W. and Holt, Richard C.},
	doi = {10.1109/ICSM.2009.5306310},
	isbn = {9781424448289},
	issn = {1063-6773},
	journal = {IEEE International Conference on Software Maintenance, ICSM},
	pages = {339--348},
	title = {{What's hot and what's not: Windowed developer topic analysis}},
	year = {2009}
}
@article{Le2014,
abstract = {Software development is inherently incremental; however, it is challenging to correctly introduce changes on top of existing code. Recent studies show that 15{\%}-24{\%} of the bug fixes are incorrect, and the most important yet hard-to-acquire information for programming changes is whether this change breaks any code elsewhere. This paper presents a framework, called Hydrogen, for patch verification. Hydrogen aims to automatically determine whether a patch correctly fixes a bug, a new bug is introduced in the change, a bug can impact multiple software releases, and the patch is applicable for all the impacted releases. Hydrogen consists of a novel program representation, namely multiversion interprocedural control flow graph (MVICFG), that integrates and compares control flow of multiple versions of programs, and a demand-driven, path-sensitive symbolic analysis that traverses the MVICFG for detecting bugs related to software changes and versions. In this paper, we present the definition, construction and applications of MVICFGs. Our experimental results show that Hydrogen correctly builds desired MVICFGs and is scalable to real-life programs such as libpng, tightvnc and putty. We experimentally demonstrate that MVICFGs can enable efficient patch verification. Using the results generated by Hydrogen, we have found a few documentation errors related to patches for a set of open-source programs.},
author = {Le, Wei and Pattison, Shannon D.},
doi = {10.1145/2568225.2568304},
isbn = {9781450327565},
issn = {02705257},
journal = {Proc. 36th Int. Conf. Softw. Eng. - ICSE 2014},
keywords = {multiversion,patch verification,software changes},
pages = {1047--1058},
title = {{Patch verification via multiversion interprocedural control flow graphs}},
url = {http://dl.acm.org/citation.cfm?doid=2568225.2568304},
year = {2014}
}
@article{Li2016,
abstract = {Graph-structured data appears frequently in domains including chemistry, natural language semantics, social networks, and knowledge bases. In this work, we study feature learning techniques for graph-structured inputs. Our starting point is previous work on Graph Neural Networks (Scarselli et al., 2009), which we modify to use gated recurrent units and modern optimization techniques and then extend to output sequences. The result is a flexible and broadly useful class of neural network models that has favorable inductive biases relative to purely sequence-based models (e.g., LSTMs) when the problem is graph-structured. We demonstrate the capabilities on some simple AI (bAbI) and graph algorithm learning tasks. We then show it achieves state-of-the-art performance on a problem from program verification, in which subgraphs need to be described as abstract data structures.},
archivePrefix = {arXiv},
arxivId = {1511.05493},
author = {Li, Yujia and Zemel, Richard and Brockschmidt, Marc and Tarlow, Daniel},
eprint = {1511.05493},
journal = {4th Int. Conf. Learn. Represent. ICLR 2016 - Conf. Track Proc.},
mendeley-groups = {Untangle},
number = {1},
pages = {1--20},
title = {{Gated graph sequence neural networks}},
year = {2016}
}
@article{li2019graph,
  title={Graph matching networks for learning the similarity of graph structured objects},
  author={Li, Yujia and Gu, Chenjie and Dullien, Thomas and Vinyals, Oriol and Kohli, Pushmeet},
  journal={arXiv preprint arXiv:1904.12787},
  year={2019}
}
@article{Linstead2007,
	abstract = {We develop and apply statistical topic models to software as a means of extracting concepts from source code. The effec- tiveness of the technique is demonstrated on 1,555 projects from SourceForge and Apache consisting of 113,000 files and 19 million lines of code. In addition to providing an auto- mated, unsupervised, solution to the problem of summariz- ing program functionality, the approach provides a proba- bilistic framework with which to analyze and visualize source file similarity. Finally, we introduce an information-theoretic approach for computing tangling and scattering of extracted concepts, and present preliminary results.},
	author = {Linstead, Erik and Rigor, Paul and Bajracharya, Sushil and Lopes, Cristina and Baldi, Pierre},
	doi = {10.1145/1321631.1321709},
	isbn = {9781595938824},
	journal = {Proc. ASE},
	keywords = {mining software,program understanding,topic models},
	number = {April 2016},
	pages = {461},
	title = {{Mining concepts from code with probabilistic topic models}},
	year = {2007}
}
@article{liu2019automatic,
  title={Automatic Generation of Pull Request Descriptions},
  author={Liu, Zhongxin and Xia, Xin and Treude, Christoph and Lo, David and Li, Shanping},
  journal={arXiv preprint arXiv:1909.06987},
  year={2019}
}
@INPROCEEDINGS{LSIAsClustering, 
	author={J. I. Maletic and N. Valluri}, 
	booktitle={14th IEEE International Conference on Automated Software Engineering}, 
	title={Automatic software clustering via Latent Semantic Analysis}, 
	year={1999}, 
	pages={251-254}, 
	keywords={automatic programming;computational linguistics;natural languages;software reusability;software tools;statistical analysis;LEDA library;LSA;Latent Semantic Analysis;MINIX operating system;automatic software clustering;corpus based statistical method;documentation;internal documentation;natural language;normal application domain;program source code;semantic meaning;software components;software reuse;Application software;Computer science;Documentation;Identity-based encryption;Matrix decomposition;Natural languages;Operating systems;Read only memory;Sparse matrices;Statistical analysis}, 
	doi={10.1109/ASE.1999.802296}, 
	month={Oct},
}
@article{LSIAsClustering2,
	title = "Semantic clustering: Identifying topics in source code",
	journal = "Information and Software Technology",
	volume = "49",
	number = "3",
	pages = "230 - 243",
	year = "2007",
	note = "12th Working Conference on Reverse Engineering",
	issn = "0950-5849",
	doi = "http://dx.doi.org/10.1016/j.infsof.2006.10.017",
	author = "Adrian Kuhn and Stéphane Ducasse and Tudor Gîrba",
	keywords = "Reverse engineering",
	keywords = "Clustering",
	keywords = "Latent Semantic Indexing",
	keywords = "Visualization"
}
@INPROCEEDINGS{LSIAsClustering3, 
	author={A. Kuhn and S. Ducasse and T. Girba}, 
	booktitle={12th Working Conference on Reverse Engineering (WCRE'05)}, 
	title={Enriching reverse engineering with semantic clustering}, 
	year={2005}, 
	pages={10 pp.-}, 
	keywords={formal specification;indexing;information retrieval;program diagnostics;programming language semantics;reverse engineering;structured programming;artifacts clustering;information retrieval;latent semantic indexing;reverse engineering;semantic clustering;software system;source code semantic;system structure;Computational modeling;Computer simulation;Indexing;Information analysis;Information retrieval;Large scale integration;Reverse engineering;Software systems;Vocabulary;Web server;clustering;concept location;reverse engineering;semantic analysis}, 
	doi={10.1109/WCRE.2005.16}, 
	ISSN={1095-1350}, 
	month={Nov},}
@INPROCEEDINGS{LSIForTestPrioritization, 
	author={M. M. Islam and A. Marchetto and A. Susi and G. Scanniello}, 
	booktitle={2012 16th European Conference on Software Maintenance and Reengineering}, 
	title={A Multi-Objective Technique to Prioritize Test Cases Based on Latent Semantic Indexing}, 
	year={2012}, 
	pages={21-30}, 
	keywords={Java;program testing;software fault tolerance;Java applications;fault discovery;latent semantic indexing;multiobjective test prioritization technique;random prioritization technique;single objective function;single-objective prioritization techniques;source code;system requirements;test case prioritization;traceability links;Business;Indexing;Large scale integration;Semantics;Software;Testing;Weight measurement;Regression Testing;Requirements;Test Case Prioritization;Testing;Traceability}, 
	doi={10.1109/CSMR.2012.13}, 
	ISSN={1534-5351}, 
	month={March},}
@article{Maalej2010,
	abstract = {Work descriptions are informal notes taken by developers to summarize work achieved in a particular session. Existing studies indicate that maintaining them is a distracting task, which costs a developer more than 30 min. a day. The goal of this research is to analyze the purposes of work descriptions, and find out if automated tools can assist developers in efficiently creating them. For this, we mine a large dataset of heterogeneous work descriptions from open source and commercial projects. We analyze the semantics of these documents and identify common information entities and granularity levels. Information on performed actions, concerned artifacts, references and new work, shows the work management purpose of work descriptions. Information on problems, rationale and experience shows their knowledge sharing purpose. We discuss how work description information, in particular information used for work management, can be generated by observing developers' interactions. Our findings have many implications for next generation software engineering tools.},
	author = {Maalej, Walid and Happel, Hans-J{\"{o}}rg},
	doi = {10.1109/MSR.2010.5463344},
	isbn = {978-1-4244-6802-7},
	issn = {02705257},
	journal = {7th IEEE Working Conference on Mining Software Repositories (MSR 2010)},
	keywords = {Collaborative work,Costs,Information analysis,Planing,Predictive models,Software engineering,Vocabulary,automated tools,development work,heterogeneous work descriptions,knowledge management,knowledge sharing,software development management,software engineering tools,work management purpose},
	pages = {191--200},
	title = {{Can Development Work Describe Itself?}},
	year = {2010}
}
@inproceedings{Madani2011,
abstract = {The existing software engineering literature has empirically shown that a proper choice of identifiers influences software understandability and maintainability. Researchers have noticed that identifiers are one of the most important source of information about program entities and that the semantic of identifiers guide the cognitive process. Recognizing the words forming identifiers is not an easy task when naming conventions (e.g., Camel Case) are not used or strictly followed and{\&}{\#}x02013;or when these words have been abbreviated or otherwise transformed. This paper proposes a technique inspired from speech recognition, i.e., dynamic time warping, to split identifiers into component words. The proposed technique has been applied to identifiers extracted from two different applications: JHotDraw and Lynx. Results compared to manually-built oracles and with Camel Case algorithm are encouraging. In fact, they show that the technique successfully recognizes words composing identifiers (even when abbreviated) in about 90{\&}{\#}x025; of cases and that it performs better than Camel Case. Furthermore, it was able to spot mistakes in the manually-built oracle.},
author = {Madani, Nioosha and Guerrouj, Latifa and {Di Penta}, Massimiliano and Gu{\'{e}}h{\'{e}}neuc, Yann Ga{\"{e}}l and Antoniol, Giuliano},
booktitle = {Proc. Eur. Conf. Softw. Maint. Reengineering, CSMR},
doi = {10.1109/CSMR.2010.31},
isbn = {9780769543215},
issn = {15345351},
keywords = {Program comprehension,Source code identifiers},
pages = {68--77},
title = {{Recognizing words from source code identifiers using speech recognition techniques}},
year = {2011}
}
@book{Manning:2008:IIR:1394399,
 author = {Manning, Christopher D. and Raghavan, Prabhakar and Sch\"{u}tze, Hinrich},
 title = {Introduction to Information Retrieval},
 year = {2008},
 chapter = {8},
 isbn = {0521865719, 9780521865715},
 publisher = {Cambridge University Press},
 address = {New York, NY, USA},
} 
@article{marcus1993building,
  title={Building a large annotated corpus of English: The Penn Treebank},
  author={Marcus, Mitchell and Santorini, Beatrice and Marcinkiewicz, Mary Ann},
  year={1993}
}
@article{Maskeri2008,
	abstract = {One of the difficulties in maintaining a large software system is the absence of documented business domain topics and correlation between these domain topics and source code. Without such a correlation, people without any prior appli- cation knowledge would find it hard to comprehend the func- tionality of the system. Latent Dirichlet Allocation (LDA), a statistical model, has emerged as a popular technique for discovering topics in large text document corpus. But its ap- plicability in extracting business domain topics from source code has not been explored so far. This paper investigates LDA in the context of comprehending large software systems and proposes a human assisted approach based on LDA for extracting domain topics from source code. This method has been applied on a number of open source and propri- etary systems. Preliminary results indicate that LDA is able to identify some of the domain topics and is a satisfactory starting point for further manual refinement of topics},
	author = {Maskeri, Girish and Sarkar, Santonu and Heafield, Kenneth},
	doi = {10.1145/1342211.1342234},
	isbn = {9781595939173},
	journal = {Proceedings of the 1st conference on India software engineering conference - ISEC '08},
	keywords = {LDA,maintenance,program comprehension},
	pages = {113},
	title = {{Mining business topics in source code using latent dirichlet allocation}},
	year = {2008}
}
@article{Mi2016,
	abstract = {Background: Bug fixing is a long-term and time-consuming activity. A software bug experiences a typical life cycle from newly reported to finally closed by developers, but it could be reopened afterwards for further actions due to reasons such as unclear description given by the bug reporter and developer negligence. Bug reopening is neither desirable nor could be completely avoided in practice, and it is more likely to bring unnecessary workloads to already-busy developers. Aims: To the best of our knowledge, there has been a little previous work on software bug reopening. In order to fur-ther study in this area, we perform an empirical analysis to provide a comprehensive understanding of this special area. Method: Based on four open source projects from Eclipse product family, they are CDT, JDT, PDE and Platform, we first quantitatively analyze reopened bugs from perspec-tives of proportion, impacts and time distribution. After initial exploration on their characteristics, we then quali-tatively summarize root causes for bug reopening, this is carried out by investigating developer discussions recorded in Eclipse Bugzilla. Results: Results show that 6{\%}-10{\%} of total bugs will lead to reopening eventually. Over 93{\%} of reopened bugs place serious influence on the normal opera-tion of the system being developed. Several key reasons for bug reopening have been identified in our empirical study. Conclusions: Although reopened bugs have significant im-pacts on both end users and developers, it is quite possible to reduce bug reopening rate through the adoption of ap-propriate methods, such as promoting effective and efficient communication among bug reporters and developers, which is supported by empirical evidence in this study.},
	author = {Mi, Qing and Keung, Jacky},
	doi = {10.1145/2915970.2915986},
	isbn = {9781450336918},
	journal = {Proceedings of the 20th International Conference on Evaluation and Assessment in Software Engineering - EASE '16},
	keywords = {bug reports,bug tracking system,empirical software engineering,open source,projects,reopened bugs},
	pages = {1--10},
	title = {{An empirical analysis of reopened bugs based on open source projects}},
	year = {2016}
}
@inproceedings{mikolov2013distributed,
  title={Distributed representations of words and phrases and their compositionality},
  author={Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  booktitle={Advances in neural information processing systems},
  pages={3111--3119},
  year={2013}
}
@article{mikolov2013efficient,
  title={Efficient estimation of word representations in vector space},
  author={Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  journal={arXiv preprint arXiv:1301.3781},
  year={2013}
}
@inproceedings{Mills:2017:ATL:3106237.3121280,
 author = {Mills, Chris},
 title = {Automating Traceability Link Recovery Through Classification},
 booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
 series = {ESEC/FSE 2017},
 year = {2017},
 isbn = {978-1-4503-5105-8},
 location = {Paderborn, Germany},
 pages = {1068--1070},
 numpages = {3},
 url = {http://doi.acm.org/10.1145/3106237.3121280},
 doi = {10.1145/3106237.3121280},
 acmid = {3121280},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Classification, Machine Learning, Traceability},
}
@article{MissingLinks,
abstract = {Empirical studies of software defects rely on links between bug databases and program code repositories. This linkage is typically based on bug-fixes identified in developer-entered commit logs. Unfortunately, developers do not always report which commits perform bug-fixes. Prior work suggests that such links can be a biased sample of the entire population of fixed bugs. The validity of statistical hypotheses-testing based on linked data could well be affected by bias. Given the wide use of linked defect data, it is vital to gauge the nature and extent of the bias, and try to develop testable theories andmodels of the bias. To do this, we must establish ground truth: manually analyze a complete version history corpus, and nail down those commits that fix defects, and those that do not. This is a difficult task, requiring an ex- pert to compare versions, analyze changes, find related bugs in the bug database, reverse-engineer missing links, and finally record their work for use later. This effort must be repeated for hundreds of commits to obtain a useful sam- ple of reported and unreported bug-fix commits. We make several contributions. First, we present Linkster, a tool to facilitate link reverse-engineering. Second, we evaluate this tool, engaging a core developer of the Apache HTTP web server project to exhaustively annotate 493 commits that occurred during a six week period. Finally, we analyze this comprehensive data set, showing that there are serious and consequential problems in the data.},
author = {Bachmann, Adrian and Bird, Christian and Rahman, Foyzur and Devanbu, Premkumar and Bernstein, Abraham},
doi = {10.1145/1882291.1882308},
isbn = {9781605587912},
journal = {Proceedings of the eighteenth ACM SIGSOFT international symposium on Foundations of software engineering - FSE '10},
keywords = {apache,are widely used in,bias,case study,commit,especially bug reports and,logs,manual annotation,software engineering research,software process data,the,tool},
pages = {97},
title = {{The missing links}},
year = {2010}
}
@article{MLink,
abstract = {The links between the bug reports in an issue-tracking system and the corresponding fixing changes in a version repository are not often recorded by developers. Such linking information is crucial for research in mining software repositories in measuring software defects and maintenance efforts. However, the state-of-the-art bug-to-fix link recovery approaches still rely much on textual matching between bug reports and commit/change logs and cannot handle well the cases where their contents are not textually similar.$\backslash$n$\backslash$nThis paper introduces MLink, a multi-layered approach that takes into account not only textual features but also source code features of the changed code corresponding to the commit logs. It is also capable of learning the association relations between the terms in bug reports and the names of entities/components in the changed source code of the commits from the established bug-to-fix links, and uses them for link recovery between the reports and commits that do not share much similar texts. Our empirical evaluation on real-world projects shows that MLink can improve the state-of-the-art bug-to-fix link recovery methods by 11--18{\%}, 13--17{\%}, and 8--17{\%} in F-score, recall, and precision, respectively.},
address = {New York, New York, USA},
author = {Nguyen, Anh Tuan and Nguyen, Tung Thanh and Nguyen, Hoan Anh and Nguyen, Tien N.},
doi = {10.1145/2393596.2393671},
isbn = {9781450316149},
journal = {Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering - FSE '12},
keywords = {bug-to-fix links,bugs,fixes,mining software repository},
pages = {1},
publisher = {ACM Press},
title = {{Multi-layered approach for recovering links between bug reports and fixes}},
year = {2012}
}
@article{Mockus2002,
abstract = {According to its proponents, open source style software development has the capacity to compete successfully, and perhaps in many cases displace, traditional commercial development methods. In order to begin investigating such claims, we examine data from two major open source projects, the Apache web server and the Mozilla browser. By using email archives of source code change history and problem reports we quantify aspects of developer participation, core team size, code ownership, productivity, defect density, and problem resolution intervals for these OSS projects. We develop several hypotheses by comparing the Apache project with several commercial projects. We then test and refine several of these hypotheses, based on an analysis of Mozilla data. We conclude with thoughts about the prospects for high-performance commercial/open source process hybrids.},
author = {Mockus, Audris and Fielding, Roy T and Herbsleb, James D},
doi = {10.1145/567793.567795},
isbn = {1049-331X},
issn = {1049331X},
journal = {ACM Trans. Softw. Eng. Methodol.},
keywords = {OSS},
number = {3},
pages = {309--346},
title = {{Two Case Studies of Open Source Software Development: Apache and Mozilla}},
url = {http://portal.acm.org/citation.cfm?doid=567793.567795},
volume = {11},
year = {2002}
}
@inproceedings{Moonen2002,
abstract = {Impact analysis is needed for the planning and estimation of software maintenance projects. Traditional impact analysis techniques tend to be too expensive for this phase, so there is need for more lightweight approaches. We present a technique for the generation of lightweight impact analyzers from island grammars. We demonstrate this technique using a real-world case study in which we describe how island grammars can be used to find account numbers in the software portfolio of a large bank. We show how we have implemented this analysis and achieved lightweightness using a reusable generative framework for impact analyzers.},
author = {Moonen, L.},
booktitle = {Proc. - IEEE Work. Progr. Compr.},
doi = {10.1109/WPC.2002.1021343},
isbn = {0769514952},
issn = {10928138},
keywords = {Costs,Databases,Documentation,Navigation,Performance analysis,Portfolios,Robustness,Software maintenance,Software performance,Software systems},
pages = {219--228},
publisher = {IEEE Comput. Soc},
title = {{Lightweight impact analysis using island grammars}},
url = {http://ieeexplore.ieee.org/document/1021343/},
volume = {2002-Janua},
year = {2002}
}
@article{Murphy-Hill2012,
abstract = {Refactoring is widely practiced by developers, and considerable research and development effort has been invested in refactoring tools. However, little has been reported about the adoption of refactoring tools, and many assumptions about refactoring practice have little empirical support. In this paper, we examine refactoring tool usage and evaluate some of the assumptions made by other researchers. To measure tool usage, we randomly sampled code changes from four Eclipse and eight Mylyn developers and ascertained, for each refactoring, if it was performed manually or with tool support. We found that refactoring tools are seldom used: 11 percent by Eclipse developers and 9 percent by Mylyn developers. To understand refactoring practice at large, we drew from a variety of data sets spanning more than 39,000 developers, 240,000 tool-assisted refactorings, 2,500 developer hours, and 12,000 version control commits. Using these data, we cast doubt on several previously stated assumptions about how programmers refactor, while validating others. Finally, we interviewed the Eclipse and Mylyn developers to help us understand why they did not use refactoring tools and to gather ideas for future research.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Murphy-Hill, Emerson and Parnin, Chris and Black, Andrew P.},
doi = {10.1109/TSE.2011.41},
eprint = {arXiv:1011.1669v3},
isbn = {9781424434527},
issn = {00985589},
journal = {IEEE Trans. Softw. Eng.},
keywords = {Refactoring,floss refactoring,refactoring tools,root-canal refactoring},
number = {1},
pages = {5--18},
pmid = {25246403},
title = {{How we refactor, and how we know it}},
volume = {38},
year = {2012}
}
@article{nerur2005challenges,
  title={Challenges of migrating to agile methodologies},
  author={Nerur, Sridhar and Mahapatra, RadhaKanta and Mangalaraj, George},
  journal={Communications of the ACM},
  volume={48},
  number={5},
  pages={72--78},
  year={2005},
  publisher={Citeseer}
}
@INPROCEEDINGS{Neumuller06, 
	author={C. Neumuller and P. Grunbacher}, 
	booktitle={21st IEEE/ACM International Conference on Automated Software Engineering (ASE'06)}, 
	title={Automating Software Traceability in Very Small Companies: A Case Study and Lessons Learne}, 
	year={2006}, 
	pages={145-156}, 
	keywords={program diagnostics;software architecture;software tools;APIS traceability environment;IT industry;software company;software traceability;Automation;Business;Computer industry;Databases;Engineering management;ISO standards;Inhibitors;Laboratories;Software engineering;Standards development}, 
	doi={10.1109/ASE.2006.25}, 
	ISSN={1938-4300}, 
	month={Sept},}
@article{Newman2017,
abstract = {A set of lexical categories, analogous to part-of-speech categories for English prose, is defined for source-code identifiers. The lexical category for an identifier is determined from its declaration in the source code, syntactic meaning in the programming language, and static program analysis. Current techniques for assigning lexical categories to identifiers use natural-language part-of-speech taggers. However, these NLP approaches assign lexical tags based on how terms are used in English prose. The approach taken here differs in that it uses only source code to determine the lexical category. The approach assigns a lexical category to each identifier and stores this information along with each declaration. srcML is used as the infrastructure to implement the approach and so the lexical information is stored directly in the srcML markup as an additional XML element for each identifier. These lexical-category annotations can then be later used by tools that automatically generate such things as code summarization or documentation. The approach is applied to 50 open source projects and the soundness of the defined lexical categories evaluated. The evaluation shows that at every level of minimum support tested, categorization is consistent at least 79{\%} of the time with an overall consistency (across all supports) of at least 88{\%}. The categories reveal a correlation between how an identifier is named and how it is declared. This provides a syntax-oriented view (as opposed to English part-of-speech view) of developer intent of identifiers.},
author = {Newman, Christian D and Alsuhaibani, Reem S and Collard, Michael L and Maletic, Jonathan I},
isbn = {9781509055012},
journal = {SANER'17},
keywords = {Natural Language Processing,identifier analysis,part-of-speech tagging,program comprehension},
title = {{Lexical Categories for Source Code Identifiers}},
year = {2017}
}

@techreport{Ng2007,
abstract = {Despite many empirical successes of spectral clustering methods-algorithms that cluster points using eigenvectors of matrices derived from the data-there are several unresolved issues. First, there are a wide variety of algorithms that use the eigenvectors in slightly different ways. Second, many of these algorithms have no proof that they will actually compute a reasonable clustering. In this paper, we present a simple spectral clustering algorithm that can be implemented using a few lines of Matlab. Using tools from matrix perturbation theory, we analyze the algorithm, and give conditions under which it can be expected to do well. We also show surprisingly good experimental results on a number of challenging clustering problems.},
author = {Ng, Andrew Y and Jordan, Michael I},
mendeley-groups = {Untangle},
title = {{On Spectral Clustering: Analysis and an algorithm}},
url = {https://papers.nips.cc/paper/2092-on-spectral-clustering-analysis-and-an-algorithm.pdf}
}

@article{nielsen1993response,
  title={Response times: the three important limits},
  author={Nielsen, Jakob},
  journal={Usability Engineering},
  year={1993},
  publisher={Academic Press}
}

@inproceedings{NonSourceInMLR,
address = {New York, New York, USA},
author = {Sun, Yan and Wang, Qing and Li, Mingshu},
booktitle = {Proceedings of the 10th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement - ESEM '16},
doi = {10.1145/2961111.2962605},
isbn = {9781450344272},
keywords = {mining soft-,missing link recovery,non-source documents,software maintenance,ware repositories},
pages = {1--10},
publisher = {ACM Press},
title = {{Understanding the Contribution of Non-source Documents in Improving Missing Link Recovery}},
year = {2016}
}

@misc{Octoverse2016,
  title = {{GitHub Octoverse 2016}},
  author={{GitHub}},
  howpublished = {\url{https://octoverse.github.com/}},
  note = {Accessed: 2017-08-07},
  year = {2016}
}

@misc{ohes,
	title={{OpenHub statistics for elasticsearch}},
	author={{OpenHub}},
	howpublished={\url{https://www.openhub.net/p/elasticsearch/analyses/latest/languages_summary}},
	note = {Accessed: 30.10.2017}
}

@INPROCEEDINGS{Oliveto2010, 
	author={R. Oliveto and M. Gethers and D. Poshyvanyk and A. De Lucia}, 
	booktitle={2010 IEEE 18th International Conference on Program Comprehension}, 
	title={On the Equivalence of Information Retrieval Methods for Automated Traceability Link Recovery}, 
	year={2010}, 
	pages={68-71}, 
	keywords={information retrieval;principal component analysis;Jensen-Shannon method;automated traceability link recovery;information retrieval methods;latent Dirichlet allocation;principal component analysis;statistical analysis;vector space model;Computer science;Documentation;Indexing;Informatics;Information retrieval;Large scale integration;Linear discriminant analysis;Mathematics;Principal component analysis;Software maintenance;Empirical Studies;Information Retrieval;Traceability Recovery}, 
	doi={10.1109/ICPC.2010.20}, 
	ISSN={1092-8138}, 
	month={June},}

@misc{online_corpora,
	title = {{Datasets as pickled python objects}},
	author = {P\^arțachi, Profir-Petru and Barr, Earl T. and White, David R.},
	howpublished = {\url{https://figshare.com/s/83c448eb518b3d04651f}},
	note = {Accessed: 2020-02-25},
	year = {2017}}

@article{OnlineLDA,
title = {Online Learning for Latent Dirichlet Allocation},
author = {Matthew Hoffman and Francis R. Bach and David M. Blei},
booktitle = {Advances in Neural Information Processing Systems 23},
editor = {J. D. Lafferty and C. K. I. Williams and J. Shawe-Taylor and R. S. Zemel and A. Culotta},
pages = {856--864},
year = {2010},
publisher = {Curran Associates, Inc.},
}

@article{petrov2011universal,
  title={A universal part-of-speech tagset},
  author={Petrov, Slav and Das, Dipanjan and McDonald, Ryan},
  journal={arXiv preprint arXiv:1104.2086},
  year={2011}
}

@ARTICLE{Pinheiro96, 
	author={F. A. C. Pinheiro and J. A. Goguen}, 
	journal={IEEE Software}, 
	title={An object-oriented tool for tracing requirements}, 
	year={1996}, 
	volume={13}, 
	number={2}, 
	pages={52-64}, 
	keywords={formal specification;object-oriented programming;systems analysis;continuous evolution;large scale development;object oriented tool;requirements tracing;tracing requirements;tracing tool;CD recording;Disk recording;Joining processes;Programming;Social factors;Software systems;Tellurium;Virtual manufacturing}, 
	doi={10.1109/52.506462}, 
	ISSN={0740-7459}, 
	month={March},}

@inproceedings{Pirapuraj2017,
abstract = {Massive amount of source codes are available free and open. Reusing those open source codes in projects can reduce the project duration and cost. Even though several Code Search Engines (CSE) are available, finding the most relevant code can be challenging. In this paper we propose a framework that can be used to overcome the above said challenge. The proposed solution starts with a Software Architecture (Class Diagram) in XML format and extracts information from the XML file, and then, it fetches relevant projects using three types of crawlers from GitHub, SourceForge, and GoogleCode. Then it finds the most relevant projects among the vast amount of downloaded projects. This research considers only Java projects. All java files in every project will be represented in Abstract Syntax Tree (AST) to extract identifiers (class names, method names, and attributes name) and comments. Action words (verbs) are extracted from comments using Part of Speech technique (POS). Those identifiers and XML file information need to be analyzed for matching. If identifiers are matched, marks will be given to those identifiers, likewise marks will be added together and then if the total mark is greater than 50{\%}, the.java file will be considered as a relevant code. Otherwise, WordNet will be used to get synonym of those identifiers and repeat the matching process using those synonyms. For connected word identifiers, camel case splitter and N-gram technique are used to separate those words. The Stanford Spellchecker is used to identify abbreviated words. The results indicate successful identification of relevant source codes.},
author = {Pirapuraj, P. and Perera, Indika},
booktitle = {3rd Int. Moratuwa Eng. Res. Conf. MERCon 2017},
doi = {10.1109/MERCon.2017.7980465},
isbn = {9781509064915},
keywords = {Class Diagram,Code reuse,N-gram technique,PoS Tagging,Software Architecture,Source code identification,WordNet},
pages = {105--110},
title = {{Analyzing source code identifiers for code reuse using NLP techniques and WordNet}},
year = {2017}
}

@article{pLSI,
abstract = {Probabilistic Latent Semantic Indexing is a novel approac v h to automated documen t indexing whic h is based on a sta? tistical laten t class model for factor analysis of coun t data? Fitted from a training corpus of text documen ts b y a gen? eralization of the Expectation Maximization algorithm? the utilized model is able to deal with domain?speci?c synon ym y as w ell as with polysemous w ords? In con trast to standard Laten t Seman tic Indexing ?LSI? b y Singular V alue Decom? position? the probabilistic v arian t has a solid statistical foun? dation and de?nes a proper generativ e data model? Retriev al experimen ts on a n um ber of test collections indicate sub? stan tial performance gains o er direct term matc v hing meth? odsasw ell as o er LSI? In particular? the com v bination of models with di?eren t dimensionalities has pro en to be ad? v v an tageous? ?},
archivePrefix = {arXiv},
arxivId = {2073829},
author = {Hofmann, Thomas},
doi = {10.1021/ac801303x},
eprint = {2073829},
isbn = {1581130961},
issn = {00032700},
journal = {Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval},
pages = {50--57},
pmid = {18989936},
title = {{Probabilistic latent semantic indexing}},
year = {1999}
}

@INPROCEEDINGS{Pohl96, 
	author={K. Pohl}, 
	booktitle={Proceedings of the Second International Conference on Requirements Engineering}, 
	title={PRO-ART: enabling requirements pre-traceability}, 
	year={1996}, 
	pages={76-84}, 
	keywords={software quality;software tools;systems analysis;PRO-ART 2.0;PRO-ART requirements engineering environment;automated trace capture;high-quality software systems development;requirements pre-traceability;requirements traceability;scalability problems;selective trace retrieval;three-dimensional requirements engineering framework;tool interoperability approach;trace information structuring;trace-repository;Buildings;Computer errors;Contracts;Design engineering;Environmental management;Information retrieval;Logic design;Maintenance engineering;Scalability;Software systems}, 
	doi={10.1109/ICRE.1996.491432}, 
	month={Apr},}

@inproceedings{ponzanelli2014holistic,
  title={Holistic recommender systems for software engineering},
  author={Ponzanelli, Luca},
  booktitle={Companion Proc. 36th Int. Conf. Soft. Eng.},
  pages={686--689},
  year={2014}
}

@inproceedings{ponzanelli2014improving,
  title={Improving low quality stack overflow post detection},
  author={Ponzanelli, Luca and Mocci, Andrea and Bacchelli, Alberto and Lanza, Michele and Fullerton, David},
  booktitle={2014 IEEE Int. Conf. Soft. Maint. Evol.},
  pages={541--544},
  year={2014},
  organization={IEEE}
}

@article{Ponzanelli2015a,
abstract = {Stack Overflow is the de facto Question and Answer (Q{\&}A) website for developers, and it has been used in many approaches by software engineering researchers to mine useful data. However, the contents of a Stack Overflow discussion are inherently heterogeneous, mixing natural language, source code, stack traces and configuration files in XML or JSON format. We constructed a full island grammar capable of modeling the set of 700,000 Stack Overflow discussions talking about Java, building a heterogeneous abstract syntax tree (H-AST) of each post (question, answer or comment) in a discussion. The resulting dataset models every Stack Overflow discussion, providing a full H-AST for each type of structured fragment (i.e., JSON, XML, Java, Stack traces), and complementing this information with a set of basic meta-information like term frequency to enable natural language analyses. Our dataset allows the end-user to perform combined analyses of the Stack Overflow by visiting the H-AST of a discussion.},
author = {Ponzanelli, Luca and Mocci, Andrea and Lanza, Michele},
doi = {10.1109/MSR.2015.67},
isbn = {9780769555942},
issn = {21601860},
journal = {IEEE Int. Work. Conf. Min. Softw. Repos.},
keywords = {H-ast,Island parsing,Unstructured data},
pages = {474--477},
publisher = {IEEE},
title = {{StORMeD: Stack overflow ready made data}},
volume = {2015-Augus},
year = {2015}
}

@article{Ponzanelli2015b,
abstract = {Summarization is hailed as a promising approach to reduce the amount of information that must be taken in by the person who wants to understand development artifacts, such as pieces of code, bug reports, emails, etc. However, existing approaches treat artifacts as pure textual entities, disregarding the heterogeneous and partially structured nature of most artifacts, which contain intertwined pieces of distinct type, such as source code, diffs, stack traces, human language, etc. We present a novel approach to augment existing summarization techniques (such as LexRank) to deal with the heterogeneous and multidimensional nature of complex artifacts. Our preliminary results on heterogeneous artifacts suggest our approach outperforms the current text-based approaches.},
author = {Ponzanelli, Luca and Mocci, Andrea and Lanza, Michele},
doi = {10.1109/MSR.2015.49},
isbn = {9780769555942},
issn = {21601860},
journal = {IEEE Int. Work. Conf. Min. Softw. Repos.},
keywords = {Holistic,Stack overfliow,Summarization},
mendeley-groups = {Code pseudo-PoS tagging},
pages = {401--405},
title = {{Summarizing complex development artifacts by mining heterogeneous data}},
volume = {2015-Augus},
year = {2015}
}

@article{Poplack1980,
author = {Poplack, Shana},
year = {1980},
month = {01},
pages = {581-618},
title = {Sometimes I’ll start a sentence in Spanish Y TERMINO EN ESPAÑOL: toward a typology of code-switching 1},
volume = {18},
journal = {Linguistics},
doi = {10.1515/ling.1980.18.7-8.581}
}

@misc{Porter1980,
	abstract = {The automatic removal of suffixes from words in English is of particular interest in the field of information retrieval. An algorithm for suffix stripping is described, which has been implemented as a short, fast program in BCPL....},
	archivePrefix = {arXiv},
	arxivId = {http://dx.doi.org/10.1108/BIJ-10-2012-0068},
	author = {Porter, M.F.},
	booktitle = {Program: electronic library and information systems},
	doi = {10.1108/eb046814},
	eprint = {/dx.doi.org/10.1108/BIJ-10-2012-0068},
	isbn = {1558604545},
	issn = {0033-0337},
	number = {3},
	pages = {130--137},
	pmid = {16143652},
	primaryClass = {http:},
	title = {{An algorithm for suffix stripping}},
	volume = {14},
	year = {1980}
}

@inproceedings{Pratapa2018,
abstract = {We compare three existing bilingual word embedding approaches, and a novel approach of training skip-grams on synthetic code-mixed text generated through linguistic models of code-mixing, on two tasks-sentiment analysis and POS tagging for code-mixed text. Our results show that while CVM and CCA based embeddings perform as well as the proposed embedding technique on semantic and syntactic tasks respectively, the proposed approach provides the best performance for both tasks overall. Thus, this study demonstrates that existing bilingual embedding techniques are not ideal for code-mixed text processing and there is a need for learning multilingual word embedding from the code-mixed text.},
author = {Pratapa, Adithya and Choudhury, Monojit and Sitaram, Sunayana},
booktitle = {Proc. 2018 Conf. Empir. Methods Nat. Lang. Process.},
pages = {3067--3072},
title = {{Word Embeddings for Code-Mixed Language Processing}},
url = {http://aclweb.org/anthology/D18-1344},
year = {2018}
}
@article{prechelt2014bflinks,
  title={Bflinks: Reliable Bugfix Links via Bidirectional References and Tuned Heuristics},
  author={Prechelt, Lutz and Pepper, Alexander},
  journal={International scholarly research notices},
  volume={2014},
  year={2014},
  publisher={Hindawi}
}

@misc{ProjectRepo,
	title = {{\Ourtoollong: Accurate Issue Links at Pull Request submission}},
	author = {P\^arțachi, Profir-Petru and Barr, Earl T. and White, David R.},
	howpublished = {\url{https://github.com/PPPI/a-m/}},
	note = {Accessed: 2017-08-14},
	year = {2017}
}

@article{Pu2011,
abstract = {This research was motivated by our interest in understanding the criteria for measuring the success of a recommender system from users' point view. Even though existing work has suggested a wide range of criteria, the consistency and validity of the combined criteria have not been tested. In this paper, we describe a unifying evaluation framework, called ResQue (Recommender systems' Quality of user experience), which aimed at measuring the qualities of the recommended items, the system's usability, usefulness, interface and interaction qualities, users' satisfaction with the systems, and the influence of these qualities on users' behavioral intentions, including their intention to purchase the products recommended to them and return to the system. We also show the results of applying psychometric methods to validate the combined criteria using data collected from a large user survey. The outcomes of the validation are able to 1) support the consistency, validity and reliability of the selected criteria; and 2) explain the quality of user experience and the key determinants motivating users to adopt the recommender technology. The final model consists of thirty two questions and fifteen constructs, defining the essential qualities of an effective and satisfying recommender system, as well as providing practitioners and scholars with a cost-effective way to evaluate the success of a recommender system and identify important areas in which to invest development resources.},
author = {Pu, Pearl and Chen, Li},
isbn = {9781450306836},
keywords = {ACM Classification Keywords H12 [User/Machine Syst,Design,H52 [User Interfaces]: evaluation/methodology,Human Factors Keywords Recommender systems,e-Commerce recommender,post-study questionnaire,quality of user experience,user-centered design General Terms Measurement},
mendeley-groups = {Aide-m{\'{e}}moire},
pages = {157--164},
journal={Proceedings of the fifth ACM conference on Recommender systems},
title = {{A User - Centric Evaluation Framework for Recommender Systems}},
year = {2011}
}

@INPROCEEDINGS{PULink, 
author={Y. Sun and C. Chen and Q. Wang and B. Boehm}, 
booktitle={2017 32nd IEEE/ACM International Conference on Automated Software Engineering (ASE)}, 
title={Improving missing issue-commit link recovery using positive and unlabeled data}, 
year={2017}, 
volume={}, 
number={}, 
pages={147-152}, 
keywords={learning (artificial intelligence);pattern classification;software maintenance;automatic link recovery approaches;improving missing issue-commit link recovery;link recovery model;missing links;positive data;traditional classifier;unlabeled link data;Feature extraction;Indexes;Metadata;Software maintenance;Training}, 
doi={10.1109/ASE.2017.8115627}, 
ISSN={}, 
month={Oct},}

@phdthesis{R2Fix,
abstract = {Many bugs, even those that are known and documented in bug reports, remain in mature software for a long time due to the lack of the development resources to fix them. We propose a general approach, R2Fix, to automatically generate bug-fixing patches from free-form bug reports. R2Fix combines past fix patterns, machine learning techniques, and semantic patch generation techniques to fix bugs automatically. We evaluate R2Fix on three projects, i.e., the Linux kernel, Mozilla, and Apache, for three important types of bugs: buffer overflows, null pointer bugs, and memory leaks. R2Fix generates 57 patches correctly, 5 of which are new patches for bugs that have not been fixed by developers yet. We reported all 5 new patches to the developers, 4 have already been accepted and committed to the code repositories. The 57 correct patches generated by R2Fix could have shortened and saved up to an average of 63 days of bug diagnosis and patch generation time. 2013 IEEE.},
author = {Liu, Chen and Yang, Jinqiu and Tan, Lin and Hafiz, Munawar},
booktitle = {A thesis presented to the University of Waterloo in fulfillment of the thesis requirement for the degree of Master of Applied Science in Electrical and Computer Engineering},
doi = {10.1109/ICST.2013.24},
isbn = {978-0-7695-4968-2},
issn = {2159-4848},
keywords = {automated bug fixing,automated program repair,bug report classification,fix pattern study},
school = {University of Waterloo},
title = {{R2Fix: Automatically generating bug fixes from bug reports}},
year = {2013}
}

@inproceedings{RCLinker,
abstract = {Links between issue reports and their corresponding commits in$\backslash$nversion control systems are often missing. However, these links are$\backslash$nimportant for measuring the quality of various parts of a software$\backslash$nsystem, predicting defects, and many other tasks. A number of existing$\backslash$napproaches have been designed to solve this problem by automatically$\backslash$nlinking bug reports to source code commits via comparison of textual$\backslash$ninformation in commit messages with textual contents in the bug reports.$\backslash$nYet, the effectiveness of these techniques is oftentimes sub optimal$\backslash$nwhen commit messages are empty or only contain minimum information, this$\backslash$nparticular problem makes the process of recovering trace ability links$\backslash$nbetween commits and bug reports particularly challenging. In this work,$\backslash$nwe aim at improving the effectiveness of existing bug linking techniques$\backslash$nby utilizing rich contextual information. We rely on a recently proposed$\backslash$ntool, namely Change Scribe, which generates commit messages containing$\backslash$nrich contextual information by using a number of code summarization$\backslash$ntechniques. Our approach then extracts features from these automatically$\backslash$ngenerated commit messages and bug reports and inputs them into a$\backslash$nclassification technique that creates a discriminative model used to$\backslash$npredict if a link exists between a commit message and a bug report. We$\backslash$ncompared our approach, coined as RCLinker (Rich Context Linker), to$\backslash$nMLink, which is an existing state-of-the-art bug linking approach. Our$\backslash$nexperiment results on bug reports from 6 software projects show that$\backslash$nRCLinker can outperform MLink in terms of F-measure by 138.66{\%}.},
author = {Le, Tien Duy B and Linares-V{\'{a}}squez, Mario and Lo, David and Poshyvanyk, Denys},
booktitle = {IEEE International Conference on Program Comprehension},
doi = {10.1109/ICPC.2015.13},
isbn = {9781467381598},
keywords = {ChangeScribe,Classification,Feature Extraction,Recovering Missing Links},
month = {may},
pages = {36--47},
publisher = {IEEE},
title = {{RCLinker: Automated Linking of Issue Reports and Commits Leveraging Rich Contextual Information}},
volume = {2015-Augus},
year = {2015}
}

@inproceedings{refinym2018dash,
abstract = {Source code is bimodal: it combines a formal, algorithmic channel and a natural language channel of identiiers and comments. In this work, we model the bimodality of code with name lows, an assignment low graph augmented to track identiier names. Conceptual types are logically distinct types that do not always coincide with program types. Passwords and URLs are example conceptual types that can share the program type string. Our tool, RefiNym, is an unsupervised method that mines a lattice of conceptual types from name lows and reiies them into distinct nominal types. For string, RefiNym inds and splits conceptual types originally merged into a single type, reducing the number of same-type variables per scope from 8.7 to 2.2 while eliminating 21.9{\%} of scopes that have more than one same-type variable in scope. This makes the code more self-documenting and frees the type system to prevent a developer from inadvertently assigning data across conceptual types. CCS CONCEPTS {\textperiodcentered} Software and its engineering → Data types and structures;},
author = {Dash, Santanu Kumar and Allamanis, Miltiadis and Barr, Earl T},
booktitle = {Proc. 2018 26th ACM Jt. Meet. Eur. Softw. Eng. Conf. Symp. Found. Softw. Eng. - ESEC/FSE 2018},
doi = {10.1145/3236024.3236042},
isbn = {9781450355735},
keywords = {Information-theoretic Clustering,Type Refinement},
pages = {107--117},
title = {{RefiNym: using names to refine types}},
year = {2018}
}

@article{relink,
author = {Wu, Rongxin and Zhang, Hongyu and Kim, Sunghun and Cheung, Shing-Chi},
 title = {ReLink: Recovering Links Between Bugs and Changes},
 journal = {Proceedings of the 19th ACM SIGSOFT Symposium and the 13th European Conference on Foundations of Software Engineering},
 series = {ESEC/FSE '11},
 year = {2011},
 isbn = {978-1-4503-0443-6},
 location = {Szeged, Hungary},
 pages = {15--25},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/2025113.2025120},
 doi = {10.1145/2025113.2025120},
 acmid = {2025120},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bugs, changes, data quality, mining software repository, missing links},
}

@inproceedings{RetForBugLoc,
abstract = {From the standpoint of retrieval from large software libraries for the purpose of bug localization, we compare five generic text models and certain composite variations thereof. The generic models are: the Unigram Model (UM), the Vector Space Model (VSM), the Latent Semantic Analysis Model (LSA), the Latent Dirichlet Allocation Model (LDA), and the Cluster Based Document Model (CBDM). The task is to locate the files that are relevant to a bug reported in the form of a textual description by a software developer. We use for our study iBUGS, a benchmarked bug localization dataset with 75 KLOC and a large number of bugs (291). A major conclusion of our comparative study is that simple text models such as UM and VSM are more effective at correctly retrieving the relevant files from a library as compared to the more sophisticated models such as LDA. The retrieval effectiveness for the various models was measured using the following two metrics: (1) Mean Average Precision; and (2) Rank-based metrics. Using the SCORE metric, we also compare the retrieval effectiveness of the models in our study with some other bug localization tools.},
author = {Rao, Shivani and Kak, Avinash},
booktitle = {Proceeding of the 8th working conference on Mining software repositories - MSR '11},
doi = {10.1145/1985441.1985451},
isbn = {9781450305747},
issn = {02705257},
keywords = {bug localization,information,latent dirichlet allocation,latent semantic analysis,retrieval,software engineering},
pages = {43},
title = {{Retrieval from software libraries for bug localization}},
year = {2011}
}

@article{Rigby2014,
abstract = {Peer review is seen as an important quality-assurance mechanism in both industrial development and the open-source software (OSS) community. The techniques for performing inspections have been well studied in industry; in OSS development, software peer reviews are not as well understood. To develop an empirical understanding of OSS peer review, we examine the review policies of 25 OSS projects and study the archival records of six large, mature, successful OSS projects. We extract a series of measures based on those used in traditional inspection experiments. We measure the frequency of review, the size of the contribution under review, the level of participation during review, the experience and expertise of the individuals involved in the review, the review interval, and the number of issues discussed during review. We create statistical models of the review efficiency, review interval, and effectiveness, the issues discussed during review, to determine which measures have the largest impact on review efficacy. We find that OSS peer reviews are conducted asynchronously by empowered experts who focus on changes that are in their area of expertise. Reviewers provide timely, regular feedback on small changes. The descrip-tive statistics clearly show that OSS review is drastically different from traditional inspection.},
author = {Rigby, Peter C. and German, Daniel M. and Cowen, Laura and Storey, Margaret-Anne},
doi = {10.1145/2594458},
issn = {1049331X},
journal = {ACM Trans. Softw. Eng. Methodol.},
number = {4},
pages = {1--33},
title = {{Peer Review on Open-Source Software Projects}},
url = {http://dl.acm.org/citation.cfm?doid=2668018.2594458},
volume = {23},
year = {2014}
}

@inproceedings{Robillard:2003:FTL:776816.776969,
	author = {Robillard, Martin P. and Murphy, Gail C.},
	title = {FEAT: A Tool for Locating, Describing, and Analyzing Concerns in Source Code},
	booktitle = {Proceedings of the 25th International Conference on Software Engineering},
	series = {ICSE '03},
	year = {2003},
	isbn = {0-7695-1877-X},
	location = {Portland, Oregon},
	pages = {822--823},
	numpages = {2},
	acmid = {776969},
	publisher = {IEEE Computer Society},
	address = {Washington, DC, USA},
}

@article{Roover2017,
author = {Roover, De and Muylaert, Ward},
title = {{Untangling Source Code Changes Using Program Slicing}},
year = {2017}
}

@misc{Roslyn,
  author = {{Microsoft}},
	title = {Microsoft Rosyln},
	howpublished = {\url{https://github.com/dotnet/roslyn}},
	note = {Accessed: 31-05-2018}
}
@article{Saade2014,
abstract = {Spectral clustering is a standard approach to label nodes on a graph by studying the (largest or lowest) eigenvalues of a symmetric real matrix such as e.g. the adjacency or the Laplacian. Recently, it has been argued that using instead a more complicated, non-symmetric and higher dimensional operator, related to the non-backtracking walk on the graph, leads to improved performance in detecting clusters, and even to optimal performance for the stochastic block model. Here, we propose to use instead a simpler object, a symmetric real matrix known as the Bethe Hessian operator, or deformed Laplacian. We show that this approach combines the performances of the non-backtracking operator, thus detecting clusters all the way down to the theoretical limit in the stochastic block model, with the computational, theoretical and memory advantages of real symmetric matrices.},
archivePrefix = {arXiv},
arxivId = {1406.1880},
author = {Saade, Alaa and Krzakala, Florent and Zdeborov{\'{a}}, Lenka},
doi = {no DOI, URL correct},
eprint = {1406.1880},
issn = {10495258},
pages = {1--9},
title = {{Spectral Clustering of Graphs with the Bethe Hessian}},
url = {http://arxiv.org/abs/1406.1880},
year = {2014}
}

@article{scarselli2008graph,
  title={The graph neural network model},
  author={Scarselli, Franco and Gori, Marco and Tsoi, Ah Chung and Hagenbuchner, Markus and Monfardini, Gabriele},
  journal={IEEE Transactions on Neural Networks},
  volume={20},
  number={1},
  pages={61--80},
  year={2008},
  publisher={IEEE}
}

@Misc{SciPy,
	author =    {Eric Jones and Travis Oliphant and Pearu Peterson and others},
	title =     {{SciPy}: Open source scientific tools for {Python}},
	year =      {2001--},
	url = "http://www.scipy.org/",
	note = {[Online; accessed 31.07.2017]}
}

@article{Sebastian2018,
author = {Sebastian, Panichella and Harald, Proksch},
number = {May},
title = {{Redundancy-free Analysis of Multi-revision Software Artifacts Redundancy-free Analysis of Multi-revision Software Artifacts}},
year = {2018}
}

@article{shervashidze2011weisfeiler,
  title={Weisfeiler-lehman graph kernels},
  author={Shervashidze, Nino and Schweitzer, Pascal and Leeuwen, Erik Jan van and Mehlhorn, Kurt and Borgwardt, Karsten M},
  journal={Journal of Machine Learning Research},
  volume={12},
  number={Sep},
  pages={2539--2561},
  year={2011}
}

@book{Shihab2013,
	abstract = {Bug fixing accounts for a large amount of the software maintenance resources. Generally, bugs are reported, fixed, verified and closed. However, in some cases bugs have to be re-opened. Re-opened bugs increase maintenance costs, degrade the overall user-perceived quality of the software and lead to unnecessary rework by busy practitioners. In this paper, we study and predict re-opened bugs through a case study on three large open source projects-namely Eclipse, Apache and OpenOffice. We structure our study along four dimensions: (1) the work habits dimension (e.g., the weekday on which the bug was initially closed), (2) the bug report dimension (e.g., the component in which the bug was found) (3) the bug fix dimension (e.g., the amount of time it took to perform the initial fix) and (4) the team dimension (e.g., the experience of the bug fixer). We build decision trees using the aforementioned factors that aim to predict re-opened bugs. We perform top node analysis to determine which factors are the most important indicators of whether or not a bug will be re-opened. Our study shows that the comment text and last status of the bug when it is initially closed are the most important factors related to whether or not a bug will be re-opened. Using a combination of these dimensions, we can build explainable prediction models that can achieve a precision between 52.1-78.6 {\%} and a recall in the range of 70.5-94.1 {\%} when predicting whether a bug will be re-opened. We find that the factors that best indicate which bugs might be re-opened vary based on the project. The comment text is the most important factor for the Eclipse and OpenOffice projects, while the last status is the most important one for Apache. These factors should be closely examined in order to reduce maintenance cost due to re-opened bugs. {\textcopyright} 2012 Springer Science+Business Media, LLC.},
	author = {Shihab, Emad and Ihara, Akinori and Kamei, Yasutaka and Ibrahim, Walid M. and Ohira, Masao and Adams, Bram and Hassan, Ahmed E. and Matsumoto, Ken Ichi},
	booktitle = {Empirical Software Engineering},
	doi = {10.1007/s10664-012-9228-6},
	isbn = {1066401292},
	issn = {13823256},
	keywords = {Bug reports,Open source software,Re-opened bugs},
	number = {5},
	pages = {1005--1042},
	title = {{Studying re-opened bugs in open source software}},
	volume = {18},
	year = {2013}
}
@article{siglidis2018grakel,
  title={GraKeL: A Graph Kernel Library in Python},
  author={Siglidis, Giannis and Nikolentzos, Giannis and Limnios, Stratis and Giatsidis, Christos and Skianis, Konstantinos and Vazirgiannis, Michalis},
  journal={arXiv preprint arXiv:1806.02193},
  year={2018}
}
@techreport{slicingsurvey,
 author = {Tip, Frank},
 title = {A Survey of Program Slicing Techniques.},
 year = {1994},
 source = {http://www.ncstrl.org:8900/ncstrl/servlet/search?formname=detail\&id=oai%3Ancstrlh%3Aercim_cwi%3Aercim.cwi%2F%2FCS-R9438},
 publisher = {CWI (Centre for Mathematics and Computer Science)},
 address = {Amsterdam, The Netherlands, The Netherlands},
}
@article{SMOTE,
abstract = {An approach to the construction of classifiers from imbalanced datasets is described. A dataset is imbalanced if the classification categories are not approximately equally rep-resented. Often real-world data sets are predominately composed of " normal " examples with only a small percentage of " abnormal " or " interesting " examples. It is also the case that the cost of misclassifying an abnormal (interesting) example as a normal example is often much higher than the cost of the reverse error. Under-sampling of the majority (nor-mal) class has been proposed as a good means of increasing the sensitivity of a classifier to the minority class. This paper shows that a combination of our method of over-sampling the minority (abnormal) class and under-sampling the majority (normal) class can achieve better classifier performance (in ROC space) than only under-sampling the majority class. This paper also shows that a combination of our method of over-sampling the minority class and under-sampling the majority class can achieve better classifier performance (in ROC space) than varying the loss ratios in Ripper or class priors in Naive Bayes. Our method of over-sampling the minority class involves creating synthetic minority class examples. Experiments are performed using C4.5, Ripper and a Naive Bayes classifier. The method is evaluated using the area under the Receiver Operating Characteristic curve (AUC) and the ROC convex hull strategy.},
author = {Chawla, Nitesh V and Bowyer, Kevin W and Hall, Lawrence O and Kegelmeyer, W Philip},
journal = {Journal of Artificial Intelligence Research},
pages = {321--357},
title = {{SMOTE: Synthetic Minority Over-sampling Technique}},
volume = {16},
year = {2002}
}
@misc{sodump,
  title = {{Stack Exchange Data Dump}},
  howpublished = "\url{https://archive.org/details/stackexchange}",
  year = {2018}, 
  note = "[Online; accessed 05-Sep-2018]"
}
@article{SoftChangeToBugLoc,
abstract = {Various information retrieval (IR) based techniques have been proposed recently to locate bugs automatically at the file level. However, their usefulness is often compromised by the coarse granularity of files and the lack of contextual information. To address this, we propose to locate bugs using software changes, which offer finer granularity than files and provide important contextual clues for bug-fixing. We observe that bug inducing changes can facilitate the bug fixing process. For example, it helps triage the bug fixing task to the developers who committed the bug inducing changes or enables developers to fix bugs by reverting these changes. Our study further identifies that change logs and the naturally small granularity of changes can help boost the performance of IR-based bug localization. Motivated by these observations, we propose an IR-based approach Locus to locate bugs from software changes, and evaluate it on six large open source projects. The results show that Locus outperforms existing techniques at the source file level localization significantly. MAP and MRR in particular have been improved, on average, by 20:1{\%} and 20:5{\%}, respectively. Locus is also capable of locating the inducing changes within top 5 for 41:0{\%} of the bugs. The results show that Locus can significantly reduce the number of lines needing to be scanned to locate the bug compared with existing techniques. {\textcopyright} 2016 ACM.},
author = {Wen, Ming and Wu, Rongxin and Cheung, Shing-chi},
doi = {10.1145/2970276.2970359},
isbn = {9781450338455},
journal = {31st IEEE/ACM International Conference on Automated Software Engineering (ASE 2016)},
keywords = {bug localization,information retrieval,soft-,software changes},
pages = {262--273},
title = {{Locus : Locating Bugs from Software Changes}},
year = {2016}
}
@article{SoftwareRepoVisualisation,
abstract = {Context: version control repositories contain a wealth of implicit information that can be used to answer many questions about a project's development process. However, this information is not directly accessible in the repositories and must be extracted and visualized. Objective: the main objective of this work is to develop a flexible and generic interactive visualization engine called ConceptCloud that supports exploratory search in version control repositories. Method: ConceptCloud is a flexible, interactive browser for SVN and Git repositories. Its main novelty is the combination of an intuitive tag cloud visualization with an underlying concept lattice that provides a formal structure for navigation. ConceptCloud supports concurrent navigation in multiple linked but individually customizable tag clouds, which allows for multi-faceted repository browsing, and scriptable construction of unique visualizations. Results: we describe the mathematical foundations and implementation of our approach and use ConceptCloud to quickly gain insight into the team structure and development process of three projects. We perform a user study to determine the usability of ConceptCloud. We show that untrained participants are able to answer historical questions about a software project better using ConceptCloud than using a linear list of commits. Conclusion: ConceptCloud can be used to answer many difficult questions such as “What has happened in this project while I was away?” and “Which developers collaborate?”. Tag clouds generated from our approach provide a visualization in which version control data can be aggregated and explored interactively.},
author = {Greene, Gillian J. and Esterhuizen, Marvin and Fischer, Bernd},
doi = {10.1016/j.infsof.2016.12.001},
issn = {09505849},
journal = {Information and Software Technology},
title = {{Visualizing and exploring software version control repositories using interactive tag clouds over formal concept lattices}},
year = {2016}
}

@inproceedings{Solorio2008,
  title={part-of-speech tagging for English-Spanish code-switched text},
  author={Solorio, Thamar and Liu, Yang},
  booktitle={Proc. Conf. on Emp. Meth. in Nat. Lang. Proc.},
  pages={1051--1060},
  year={2008},
  organization={Association for Computational Linguistics}
}

@misc{sorevcorpus,
  title = {{StackOverflow: Code Formatting Revisions Corpus}},
  howpublished = "\url{https://github.com/PPPI/POSIT/blob/master/misc.zip}",
  year = {2020}, 
  note = "[Online; 22.01.2020]"
}
@article{Soto2018,
abstract = {Code-switching is the fluent alternation between two or more languages in conversation between bilinguals. Large populations of speakers code-switch during communication, but little effort has been made to develop tools for code-switching, including part-of-speech taggers. In this paper, we propose an approach to POS tagging of code-switched English-Spanish data based on recurrent neural networks. We test our model on known monolin-gual benchmarks to demonstrate that our neural POS tagging model is on par with state-of-the-art methods. We next test our code-switched methods on the Miami Bangor corpus of English-Spanish conversation , focusing on two types of experiments: POS tagging alone, for which we achieve 96.34{\%} accuracy, and joint part-of-speech and language ID tagging, which achieves similar POS tagging accuracy (96.39{\%}) and very high language ID accuracy (98.78{\%}). Finally, we show that our proposed models outperform other state-of-the-art code-switched taggers.},
author = {Soto, Victor and Hirschberg, Julia},
pages = {1--10},
title = {{Joint part-of-speech and Language ID Tagging for Code-Switched Data}},
year = {2018}
}

@article{StableRoommateProblem,
 author = {Irving, Robert W. and Manlove, David F.},
 title = {The Stable Roommates Problem with Ties},
 journal = {J. Algorithms},
 issue_date = {April 2002},
 volume = {43},
 number = {1},
 month = apr,
 year = {2002},
 issn = {0196-6774},
 pages = {85--105},
 numpages = {21},
 url = {http://dx.doi.org/10.1006/jagm.2002.1219},
 doi = {10.1006/jagm.2002.1219},
 acmid = {589924},
 publisher = {Academic Press, Inc.},
 address = {Duluth, MN, USA},
 keywords = {NP-completeness, approximation algorithm, indifference, linear-time algorithm, stable matching problem, super-stability, weak stability},
}

@article{Stahl2017,
abstract = {The importance of traceability in software development has long been recognized, not only for reasons of legality and certification, but also to enable the development itself. At the same time, organizations are known to struggle to live up to traceability requirements, and there is an identified lack of studies on traceability practices in the industry, not least in the area of tooling and infrastructure. This paper presents, investigates and discusses Eiffel, an industry developed solution designed to provide real time traceability in continuous integration and delivery. The traceability needs of industry professionals are also investigated through interviews, providing context to that solution. It is then validated through further interviews, a comparison with previous traceability methods and a review of literature. It is found to address the identified traceability needs and found in some cases to reduce traceability data acquisition times from days to minutes, while at the same time alternatives offering comparable functionality are lacking. In this work, traceability is shown not only to be an important concern to engineers, but also regarded as a prerequisite to successful large scale continuous integration and delivery. At the same time, promising developments in technical infrastructure are documented and clear differences in traceability mindset between separate industry projects is revealed.},
author = {St{\aa}hl, Daniel and Hall{\'{e}}n, Kristofer and Bosch, Jan},
doi = {10.1007/s10664-016-9457-1},
file = {:D$\backslash$:/OneDrive/Documents/Mendeley Desktop/St{\aa}hl, Hall{\'{e}}n, Bosch/Empirical Software Engineering/St{\aa}hl, Hall{\'{e}}n, Bosch{\_}2017{\_}Achieving traceability in large scale continuous integration and delivery deployment, usage and validation o.pdf:pdf},
issn = {15737616},
journal = {Empir. Softw. Eng.},
keywords = {Continuous delivery,Continuous integration,Traceability,Very-large-scale software systems},
mendeley-groups = {Aide-m{\'{e}}moire},
number = {3},
pages = {967--995},
title = {{Achieving traceability in large scale continuous integration and delivery deployment, usage and validation of the eiffel framework}},
volume = {22},
year = {2017}
}
@article{Tao2012,
abstract = {Software evolves with continuous source-code changes. These code changes usually need to be understood by software engineers when performing their daily development and maintenance tasks. However, despite its high importance, such change-understanding practice has not been systematically studied. Such lack of empirical knowledge hinders attempts to evaluate this fundamental practice and improve the corresponding tool support.$\backslash$n$\backslash$nTo address this issue, in this paper, we present a large-scale quantitative and qualitative study at Microsoft. The study investigates the role of understanding code changes during software-development process, explores engineers' information needs for understanding changes and their requirements for the corresponding tool support. The study results reinforce our beliefs that understanding code changes is an indispensable task performed by engineers in software-development process. A number of insufficiencies in the current practice also emerge from the study results. For example, it is difficult to acquire important information needs such as a change's completeness, consistency, and especially the risk imposed by it on other software components. In addition, for understanding a composite change, it is valuable to decompose it into sub-changes that are aligned with individual development issues; however, currently such decomposition lacks tool support.},
author = {Tao, Yida and Dang, Yingnong and Xie, Tao and Zhang, Dongmei and Kim, Sunghun},
doi = {10.1145/2393596.2393656},
isbn = {9781450316149},
journal = {Proc. ACM SIGSOFT 20th Int. Symp. Found. Softw. Eng. - FSE '12},
pages = {1},
title = {{How do software engineers understand code changes?}},
url = {http://dl.acm.org/citation.cfm?doid=2393596.2393656},
year = {2012}
}

@Misc{tensorflow2015-whitepaper,
  author = {Mart\'{\i}n~Abadi and Ashish~Agarwal and Paul~Barham and Eugene~Brevdo and Zhifeng~Chen and Craig~Citro and Greg~S.~Corrado and Andy~Davis and Jeffrey~Dean and Matthieu~Devin and Sanjay~Ghemawat and Ian~Goodfellow and Andrew~Harp and Geoffrey~Irving and Michael~Isard and Yangqing Jia and Rafal~Jozefowicz and Lukasz~Kaiser and Manjunath~Kudlur and Josh~Levenberg and Dan~Man\'{e} and Rajat~Monga and Sherry~Moore and Derek~Murray and Chris~Olah and Mike~Schuster and Jonathon~Shlens and Benoit~Steiner and Ilya~Sutskever and Kunal~Talwar and Paul~Tucker and Vincent~Vanhoucke and Vijay~Vasudevan and Fernanda~Vi\'{e}gas and Oriol~Vinyals and Pete~Warden and Martin~Wattenberg and Martin~Wicke and Yuan~Yu and Xiaoqiang~Zheng},
  note   = {Software available from tensorflow.org},
  title  = {{TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
  year   = {2015},
  url    = {http://tensorflow.org/},
}

@article{tfidf,
	title={Introduction to modern information retrieval},
	author={Salton, Gerard and McGill, Michael J},
	year={1986},
	publisher={McGraw-Hill, Inc.}
}

@inproceedings{Tian2015,
  title={A comparative study on the effectiveness of part-of-speech tagging techniques on bug reports},
  author={Tian, Yuan and Lo, David},
  booktitle={SANER'15},
  pages={570--574},
  year={2015},
  organization={IEEE}
}

@book{TopicModelSurvey,
abstract = {Researchers in software engineering have attempted to improve software devel- opment by mining and analyzing software repositories. Since the majority of the software engineering data is unstructured, researchers have applied Information Retrieval (IR) tech- niques to help software development. The recent advances of IR, especially statistical topic models, have helped make sense of unstructured data in software repositories even more. However, even though there are hundreds of studies on applying topic models to software repositories, there is no study that shows how the models are used in the software engineer- ing research community, and which software engineering tasks are being supported through topic models. Moreover, since the performance of these topic models is directly related to the model parameters and usage, knowing how researchers use the topic models may also help future studies make optimal use of such models. Thus, we surveyed 167 articles from the software engineering literature that make use of topic models.We find that i) most stud- ies centre around a limited number of software engineering tasks; ii) most studies use only basic topic models; iii) and researchers usually treat topic models as black boxes without fully exploring their underlying assumptions and parameter values. Our paper provides a starting point for new researchers who are interested in using topic models, and may help new researchers and practitioners determine how to best apply topic models to a particular software engineering task.},
author = {Chen, Tse-Hsun and Thomas, Stephen W. and Hassan, Ahmed E.},
booktitle = {Empirical Software Engineering},
doi = {10.1007/s10664-015-9402-8},
isbn = {1066401594028},
issn = {1382-3256},
keywords = {1 introduction and motivation,LDA,LSI,Survey,Topic modeling,lda,lsi,survey,topic modeling},
month = {oct},
number = {5},
pages = {1843--1919},
publisher = {Springer US},
title = {{A survey on the use of topic models when mining software repositories}},
volume = {21},
year = {2016}
}

@article{TopicTraceability,
abstract = {Software traceability is a fundamentally important task in software engineering. The need for automated traceability increases as projects become more complex and as the number of artifacts increases. We propose an automated technique that combines traceability with a machine learning technique known as topic modeling. Our approach automatically records traceability links during the software development process and learns a probabilistic topic model over artifacts. The learned model allows for the semantic categorization of artifacts and the topical visualization of the software system. To test our approach, we have implemented several tools: an artifact search tool combining keyword-based search and topic modeling, a recording tool that performs prospective traceability, and a visualization tool that allows one to navigate the software architecture and view semantic topics associated with relevant artifacts and architectural components. We apply our approach to several data sets and discuss how topic modeling enhances software traceability, and vice versa.},
author = {Asuncion, Hazeline U and Asuncion, Arthur U and Taylor, Richard N},
doi = {10.1145/1806799.1806817},
isbn = {978-1-60558-719-6},
issn = {0270-5257},
journal = {2010 ACM/IEEE 32nd International Conference on Software Engineering},
keywords = {latent dirichlet allocation,software architecture,software traceability,topic model},
pages = {95--104},
title = {{Software traceability with topic modeling}},
volume = {1},
year = {2010}
}

@article{TraceabilityEvolution,
abstract = {{\textcopyright} 2016 IEEE.Trace links provide critical support for numerous software engineering activities including safety analysis, compliance verification, test-case selection, and impact prediction. However, as the system evolves over time, there is a tendency for the quality of trace links to degrade into a tangle of inaccurate and untrusted links. This is especially true with the links between source-code and upstream artifacts such as requirements - because developers frequently refactor and change code without updating the links. We present TLE (Trace Link Evolver), a solution for automating the evolution of trace links as changes are introduced to source code. We use a set of heuristics, open source tools, and information retrieval methods to detect common change scenarios across different versions of software. Each change scenario is then associated with a set of link evolution heuristics which are used to evolve trace links. We evaluate our approach through a controlled experiment and also through applying it across 27 releases of the Cassandra Database System. Results show that the trace links evolved using our approach are significantly more accurate than those generated using information retrieval alone.},
author = {Rahimi, Mona and Goss, William and Cleland-Huang, Jane},
doi = {10.1109/ICSME.2016.57},
isbn = {9781509038060},
journal = {Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016},
keywords = {Evolution,Maintenance,Traceability},
pages = {99--109},
title = {{Evolving requirements-to-code trace links across versions of a software system}},
year = {2017}
}

@inproceedings{Treude:2015:TTN:2819009.2819128,
 author = {Treude, Christoph and Sicard, Mathieu and Klocke, Marc and Robillard, Martin},
 title = {TaskNav: Task-based Navigation of Software Documentation},
 booktitle = {Proc. 37th Int. Conf. Soft. Eng. - Volume 2},
 series = {ICSE '15},
 year = {2015},
 location = {Florence, Italy},
 pages = {649--652},
 numpages = {4},
 url = {http://dl.acm.org/citation.cfm?id=2819009.2819128},
 acmid = {2819128},
 publisher = {IEEE Press},
 address = {Piscataway, NJ, USA},
}

@inproceedings{Treude2015portuguese,
  title={Challenges in analyzing software documentation in {Portuguese}},
  author={Treude, Christoph and Prolo, Carlos A and Figueira Filho, Fernando},
  booktitle={Proc. 29th Bra. Sym. Soft. Eng.},
  pages={179--184},
  year={2015},
  organization={IEEE}
}

@article{Trivedi2018,
archivePrefix = {arXiv},
arxivId = {1807.08447},
author = {Trivedi, Rakshit and Sisman, Bunyamin and Dong, Xin Luna and Faloutsos, Christos and Ma, Jun and Zha, Hongyuan},
doi = {arXiv:1807.08447v1},
eprint = {1807.08447},
file = {:D$\backslash$:/OneDrive/untangle-tex/related work/Graph Neural Nets/Trivedi et al.{\_}2018{\_}LinkNBed Multi-Graph Representation Learning with Entity Linkage{\_}Proceedings of the 56th Annual Meeting of the Assoc.pdf:pdf},
journal = {Proc. 56th Annu. Meet. Assoc. Comput. Linguist. (Volume 1 Long Pap.},
mendeley-groups = {Untangle},
pages = {252--262},
title = {{LinkNBed: Multi-Graph Representation Learning with Entity Linkage}},
url = {http://aclweb.org/anthology/P18-1024},
year = {2018}
}

@inproceedings{TwitterLDA,
abstract = {Twitter as a new form of social media can potentially con- tain much useful information, but content analysis on Twitter has not been well studied. In particular, it is not clear whether as an information source Twitter can be simply regarded as a faster news feed that covers mostly the same information as traditional news media. In This paper we empirically compare the content of Twitter with a traditional news medium, New York Times, using unsupervised topic modeling. We use a Twitter-LDA model to discover topics from a representative sample of the entire Twitter.We then use text mining techniques to compare these Twitter topics with topics from New York Times, taking into considera- tion topic categories and types. We also study the relation between the proportions of opinionated tweets and retweets and topic categories and types. Our comparisons show interesting and useful findings for down- stream IR or DM applications.},
author = {Zhao, Wayne Xin and Jiang, Jing and Weng, Jianshu and He, Jing and Lim, Ee-Peng and Yan, Hongfei and Li, Xiaoming},
booktitle = {Proceedings of the 33rd European conference on Advances in information retrieval (ECIR'11)},
doi = {10.1007/978-3-642-20161-5_34},
isbn = {978-3-642-20160-8},
keywords = {microblogging,topic modeling,twitter},
pages = {338--349},
publisher = {Springer, Berlin, Heidelberg},
title = {{Comparing Twitter and Traditional Media using Topic Models}},
year = {2011}
}

@article{velickovic2018graph,
  title="{Graph Attention Networks}",
  author={Veli{\v{c}}kovi{\'{c}}, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Li{\`{o}}, Pietro and Bengio, Yoshua},
  journal={International Conference on Learning Representations},
  year={2018},
  url={https://openreview.net/forum?id=rJXMpikCZ},
  note={accepted as poster},
}

@article{vishwanathan2010graph,
  title={Graph kernels},
  author={Vishwanathan, S Vichy N and Schraudolph, Nicol N and Kondor, Risi and Borgwardt, Karsten M},
  journal={Journal of Machine Learning Research},
  volume={11},
  number={Apr},
  pages={1201--1242},
  year={2010}
}

@ARTICLE{Viterbi1967, 
author={A. Viterbi}, 
journal={IEEE Transactions on Information Theory}, 
title={Error bounds for convolutional codes and an asymptotically optimum decoding algorithm}, 
year={1967}, 
volume={13}, 
number={2}, 
pages={260-269}, 
abstract={The probability of error in decoding an optimal convolutional code transmitted over a memoryless channel is bounded from above and below as a function of the constraint length of the code. For all but pathological channels the bounds are asymptotically (exponentially) tight for rates above<tex>R_{0}</tex>, the computational cutoff rate of sequential decoding. As a function of constraint length the performance of optimal convolutional codes is shown to be superior to that of block codes of the same length, the relative improvement increasing with rate. The upper bound is obtained for a specific probabilistic nonsequential decoding algorithm which is shown to be asymptotically optimum for rates above<tex>R_{0}</tex>and whose performance bears certain similarities to that of sequential decoding algorithms.}, 
keywords={Convolutional codes}, 
doi={10.1109/TIT.1967.1054010}, 
ISSN={0018-9448}, 
month={April},
}

@inproceedings{Vyas2014,
  title={Pos tagging of english-hindi code-mixed social media content},
  author={Vyas, Yogarshi and Gella, Spandana and Sharma, Jatin and Bali, Kalika and Choudhury, Monojit},
  booktitle={Proc. 2014 Conf. on Emp. Meth. in Nat. Lang. Proc. (EMNLP)},
  pages={974--979},
  year={2014}
}

@article{Wallach2009,
	abstract = {Implementations of topic models typically use symmetric Dirichlet priors with fixed concentration parameters, with the implicit assumption that such smoothing parameters have little practical effect. In this paper, we explore several classes of structured priors for topic models. We find that an asymmetric Dirichlet prior over the documenttopic distributions has substantial advantages over a symmet- ric prior, while an asymmetric prior over the topicword distributions provides no real benefit. Approximation of this prior structure through simple, efficient hy- perparameter optimization steps is sufficient to achieve these performance gains. The prior structure we advocate substantially increases the robustness of topic models to variations in the number of topics and to the highly skewed word fre- quency distributions common in natural language. Since this prior structure can be implemented using efficient algorithms that add negligible cost beyond standard inference techniques, we recommend it as a new standard for topic modeling.},
	author = {Wallach, Hanna M and Mimno, David and Mccallum, Andrew},
	doi = {10.1007/s10708-008-9161-9},
	file = {:D$\backslash$:/Downloads/Wallach, Mimno, Mccallum, 2009, Rethinking LDA Why Priors Matter.pdf:pdf},
	isbn = {9781615679119},
	issn = {02699370},
	journal = {Advances in Neural Information Processing Systems 22},
	number = {2},
	pages = {1973--1981},
	pmid = {2508715},
	title = {{Rethinking LDA : Why Priors Matter}},
	volume = {22},
	year = {2009}
}

@article{Watanabe2010,
abstract = {We propose a new approach to the analysis of Loopy Belief Propagation (LBP) by establishing a formula that connects the Hessian of the Bethe free energy with the edge zeta function. The formula has a number of theoretical implications on LBP. It is applied to give a sufficient condition that the Hessian of the Bethe free energy is positive definite, which shows non-convexity for graphs with multiple cycles. The formula clarifies the relation between the local stability of a fixed point of LBP and local minima of the Bethe free energy. We also propose a new approach to the uniqueness of LBP fixed point, and show various conditions of uniqueness.},
archivePrefix = {arXiv},
arxivId = {1103.0605},
author = {Watanabe, Yusuke and Fukumizu, Kenji},
eprint = {1103.0605},
isbn = {9781615679119},
issn = {0014-2956},
journal = {Adv. Neural Inf. Process. Syst.},
keywords = {bethe free energy,function,graph zeta,graphical models,ihara-bass formula,loopy belief propagation},
mendeley-groups = {Untangle},
number = {1988},
pages = {19},
pmid = {10102984},
title = {{Loopy Belief Propagation, Bethe Free Energy and Graph Zeta Function}},
url = {https://arxiv.org/pdf/1002.3307.pdf http://arxiv.org/abs/1002.3307},
volume = {cs.AI},
year = {2010}
} 

@inproceedings{Weisfeiler1968ReductionOA,
  title={Reduction of a graph to a canonical form and an algebra arising during this reduction},
  author={B. Yu. Weisfeiler and A. A. Leman},
  year={1968}
}

@article{weka,
 author = {Hall, Mark and Frank, Eibe and Holmes, Geoffrey and Pfahringer, Bernhard and Reutemann, Peter and Witten, Ian H.},
 title = {The WEKA Data Mining Software: An Update},
 journal = {SIGKDD Explor. Newsl.},
 issue_date = {June 2009},
 volume = {11},
 number = {1},
 month = nov,
 year = {2009},
 issn = {1931-0145},
 pages = {10--18},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1656274.1656278},
 doi = {10.1145/1656274.1656278},
 acmid = {1656278},
 publisher = {ACM},
 address = {New York, NY, USA},
} 


@misc{wikicorpus,
  title = {{Wikipedia:Database download}},
  howpublished = "\url{https://en.wikipedia.org/wiki/Wikipedia:Database_download}",
  year = {2018}, 
  note = "[Online; accessed 05-Sep-2018]"
} 

@inproceedings{winata-charmodel,
    title = "Bilingual Character Representation for Efficiently Addressing Out-of-Vocabulary Words in Code-Switching Named Entity Recognition",
    author = "Winata, Genta Indra  and
      Wu, Chien-Sheng  and
      Madotto, Andrea  and
      Fung, Pascale",
    booktitle = "Proc. Third Workshop Comp. Appr. Ling. Code-Switching",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-3214",
    doi = "10.18653/v1/W18-3214",
    pages = "110--114",
}

@inproceedings{yanardag2015deep,
  title={Deep graph kernels},
  author={Yanardag, Pinar and Vishwanathan, SVN},
  booktitle={Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  pages={1365--1374},
  year={2015}
}

@inproceedings{Yang2016,
abstract = {Program comprehension is an important and difficult task in software development and evolution, which is costly and time-consuming. Some software abbreviated identifiers in the source code can further increase the difficulty of the program comprehension, especially for the junior developers who have less developing expertise for the software system. Moreover, a number of studies focused on applying information retrieval (IR) techniques to analyze the source code identifiers for various software maintenance tasks. These IR techniques would have difficulty in exploring abbreviations in the program. Hence, this paper proposes a novel approach to expand the abbreviations of software identifiers. The proposed approach searches the expansions of abbreviated identifiers considering the searching resources of the program and the Web. An empirical study has been evaluated and demonstrates that our approach can effectively recommend the expansions, which can not only help developers comprehend the program, but also assist IR techniques in further exploiting the natural language information in the program. {\textcopyright} Springer International Publishing AG 2016.},
author = {Yang, Hui and Sun, Xiaobing and Duan, Yucong and Zhao, Han and Li, Bin},
booktitle = {Lect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics)},
doi = {10.1007/978-3-319-46257-8_63},
isbn = {9783319462561},
issn = {16113349},
keywords = {Abbreviated identifier expansion,Information retrieval (IR) techniques,Program comprehension},
pages = {588--595},
title = {{On expanding abbreviated identifiers in the source code}},
volume = {9937 LNCS},
year = {2016}
}

@InProceedings{Yasunaga&al.18.naacl,
  author =  {Michihiro Yasunaga and Jungo Kasai and Dragomir R. Radev},
  title =   {Robust Multilingual part-of-speech Tagging via Adversarial Training},
  year =    {2018},  
  booktitle =   {Proc. NAACL},  
  publisher =   {Association for Computational Linguistics},
}

@article{Yin2018,
abstract = {For tasks like code synthesis from natural language, code retrieval, and code summarization, data-driven models have shown great promise. However, creating these models require parallel data between natural language (NL) and code with fine-grained alignments. Stack Overflow (SO) is a promising source to create such a data set: the questions are diverse and most of them have corresponding answers with high quality code snippets. However, existing heuristic methods (e.g., pairing the title of a post with the code in the accepted answer) are limited both in their coverage and the correctness of the NL-code pairs obtained. In this paper, we propose a novel method to mine high-quality aligned data from SO using two sets of features: hand-crafted features considering the structure of the extracted snippets, and correspondence features obtained by training a probabilistic model to capture the correlation between NL and code using neural networks. These features are fed into a classifier that determines the quality of mined NL-code pairs. Experiments using Python and Java as test beds show that the proposed method greatly expands coverage and accuracy over existing mining methods, even when using only a small number of labeled examples. Further, we find that reasonable results are achieved even when training the classifier on one language and testing on another, showing promise for scaling NL-code mining to a wide variety of programming languages beyond those for which we are able to annotate data.},
archivePrefix = {arXiv},
arxivId = {1805.08949},
author = {Yin, Pengcheng and Deng, Bowen and Chen, Edgar and Vasilescu, Bogdan and Neubig, Graham},
doi = {10.1145/3196398.3196408},
eprint = {1805.08949},
isbn = {9781450357166},
issn = {02705257},
journal = {Proc. - Int. Conf. Softw. Eng.},
mendeley-groups = {Code pseudo-PoS tagging},
pages = {476--486},
title = {{Learning to mine aligned code and natural language pairs from stack overflow}},
year = {2018}
}

@article{Yu2003,
abstract = {We propose a principled account on multiclass spectral clustering. Given a discrete clustering formulation, we first solve a relaxed continuous optimization problem by eigen-decomposition. We clarify the role of eigenvectors as a generator of all optimal solutions through orthonormal transforms. We then solve an optimal discretization problem, which seeks a discrete solution closest to the continuous optima. The discretization is efficiently computed in an iterative fashion using singular value decomposition and nonmaximum suppression. The resulting discrete solutions are nearly global-optimal. Our method is robust to random initialization and converges faster than other clustering methods. Experiments on real image segmentation are reported.},
author = {Yu and Shi},
doi = {10.1109/ICCV.2003.1238361},
isbn = {0-7695-1950-4},
journal = {Proc. Ninth IEEE Int. Conf. Comput. Vis.},
number = {1},
pages = {313--319 vol.1},
title = {{Multiclass spectral clustering}},
url = {http://ieeexplore.ieee.org/document/1238361/},
year = {2003}
}

@inproceedings{Zeng2018,
abstract = {Code-switching language modeling is challenging due to statistics of each individual language, as well as statistics of cross-lingual language are insufficient. To compensate for the issue of statistical insufficiency, in this paper we propose a word-class n-gram language modeling approach of which only infrequent words are clustered while most frequent words are treated as singleton classes themselves. We first demonstrate the effectiveness of the proposed method on our English-Mandarin code-switching SEAME data in terms of perplexity. Compared with the conventional word n-gram language models, as well as the word-class n-gram language models of which entire vocabulary words are clustered, the proposed word-class n-gram language modeling approach can yield lower perplexity on our SEAME dev data sets. Additionally, we observed further perplexity reduction by interpolating the word n-gram language models with the proposed word-class n-gram language models. We also attempted to build word-class n-gram language models using third-party text data with our proposed method, and similar perplexity performance improvement was obtained on our SEAME dev data sets when they are interpolated with the word n-gram language models. Finally, to examine the contribution of the proposed language modeling approach to code-switching speech recognition, we conducted lattice based n-best rescoring.},
author = {Zeng, Zhiping and Xu, Haihua and Chong, Tze Yuang and Chng, Eng Siong and Li, Haizhou},
booktitle = {Proc. - 9th Asia-Pacific Signal Inf. Process. Assoc. Annu. Summit Conf. APSIPA ASC 2017},
doi = {10.1109/APSIPA.2017.8282279},
isbn = {9781538615423},
pages = {1596--1601},
title = {{Improving N-gram language modeling for code-switching speech recognition}},
volume = {2018-February},
year = {2018}
}

@inproceedings{Zhang2016,
abstract = {Defect prediction on projects with limited historical data has attracted great interest from both researchers and prac-titioners. Cross-project defect prediction has been the main area of progress by reusing classifiers from other projects. However, existing approaches require some degree of ho-mogeneity (e.g., a similar distribution of metric values) be-tween the training projects and the target project. Satisfy-ing the homogeneity requirement often requires significant effort (currently a very active area of research). An unsupervised classifier does not require any training data, therefore the heterogeneity challenge is no longer an issue. In this paper, we examine two types of unsupervised classifiers: a) distance-based classifiers (e.g., k-means); and b) connectivity-based classifiers. While distance-based un-supervised classifiers have been previously used in the de-fect prediction literature with disappointing performance, connectivity-based classifiers have never been explored be-fore in our community. We compare the performance of unsupervised classifiers versus supervised classifiers using data from 26 projects from three publicly available datasets (i.e., AEEEM, NASA, and PROMISE). In the cross-project setting, our proposed con-nectivity-based classifier (via spectral clustering) ranks as one of the top classifiers among five widely-used supervised classifiers (i.e., random forest, naive Bayes, logistic regres-sion, decision tree, and logistic model tree) and five unsu-pervised classifiers (i.e., k-means, partition around medoids, fuzzy C-means, neural-gas, and spectral clustering). In the within-project setting (i.e., models are built and applied on the same project), our spectral classifier ranks in the second tier, while only random forest ranks in the first tier. Hence, connectivity-based unsupervised classifiers offer a viable so-lution for cross and within project defect predictions.},
address = {New York, New York, USA},
author = {Zhang, Feng and Zheng, Quan and Zou, Ying and Hassan, Ahmed E.},
booktitle = {Proc. 38th Int. Conf. Softw. Eng. - ICSE '16},
doi = {10.1145/2884781.2884839},
isbn = {9781450339001},
issn = {02705257},
keywords = {cross-project,defect prediction,graph mining,heterogeneity,spectral clustering,unsupervised},
mendeley-groups = {Untangle},
pages = {309--320},
publisher = {ACM Press},
title = {{Cross-project defect prediction using a connectivity-based unsupervised classifier}},
url = {http://dl.acm.org/citation.cfm?doid=2884781.2884839},
year = {2016}
}

@article{zimmermann2005mining,
  title={Mining version histories to guide software changes},
  author={Zimmermann, Thomas and Zeller, Andreas and Weissgerber, Peter and Diehl, Stephan},
  journal={IEEE Transactions on Software Engineering},
  volume={31},
  number={6},
  pages={429--445},
  year={2005},
  publisher={IEEE}
}

@Comment{jabref-meta: databaseType:bibtex;}
