\chapter{Literature Review}
\label{chapter:literature}

In this chapter, I first present the wider literature that concerns developer
activities impacting project health in
\Cref{chapter:literature:sec:project_health}. I then focus on the subproblems
tackled in the thesis and start by presenting Traceability and Commit-Issue
linking literature in \Cref{chapter:literature:sec:am_rel_work}.
\Cref{chapter:literature:sec:flexeme_rel_work} presents the literature around
commit untangling. I conclude this chapter by describing the literature around
mixed-text in \Cref{chapter:literature:sec:posit_rel_work}.

\section{The Many Faces of Project Health}
\label{chapter:literature:sec:project_health}

As project health comprises different aspects of a project --- from technical,
through design, to social --- it is natural that it touches many areas of active
interest from researchers. Some of the more prominently researched areas are
technical debt, human aspects of software engineering. I will first present
technical debt literature and its relation to project health. I then will shift
focus to the area of software evolution and maintenance, with a primary focus on
how it helps projects remain active. I conclude this section by presenting
recent research in human aspects of software engineering, with a focus on
improving the development process by removing negative incentives.

Technical debt primarily arose as a concept from a metaphor relating it to
financial debt and serving to capture the consequences of poor software
development~\cite{cunningham1992wycash}. The core concept was that debt is not
necessarily bad, it can be used as a asset to speed up development at a crucial
point so long as it is repaid. While this provided an intuitive basis for the
concept, the lack of a formal definition left the boundaries fuzzy. 

Tom \etal~\cite{tom2012consolidated} found that this created an apparently
fragmented understanding from a number of both themes and anecdotal accounts
that fail to fully capture the phenomenon. Later, Tom \etal conduct a multivocal
literature review~\cite{tom2013exploration} within which they determine the
dimensions of technical debt, its attributes, and provide a taxonomy for it. Tom
\etal separate technical debt into: code debt, design and architectural debt,
environment debt, knowledge distribution and documentation debt, and testing
debt:
\begin{itemize}
    \item Code debt covers hacks or sloppy code that was written and would incur
    heavy maintenance costs.
    \item Design and architectural debt comes mainly in two flavours: an upfront
    design that did not factor maintenance or scale or piecemeal design that was
    done without the required refactoring; further, this debt could also arise
    as a consequence of a sub-optimal architectural solution, be it sub-optimal
    at the time of design or as technology progresses.
    \item Environmental debt covers software, hardware, dependencies, or even
    lack of automation. It manifests itself as suboptimal use of labour, reduced
    development velocity or even vulnerabilities.
    \item Knowledge distribution and documentation cost covers aspects related
    to having a high `bus factor', when a complex system is maintained by few
    knowledgeable individuals and the process is not documented, a company risks
    a sudden increase of technical debt should the individuals leave.
    \item Testing debt covers the need for manual quality assurance when the
    task can be automated. While it shares the automation aspect with
    Environmental debt, it has the associated risk of brand damage if a faulty
    patch is pushed through to production.
\end{itemize}  

As impact on project health, in the short term, it allows a boost in project
velocity; and should this debt never come due, you may feel free to incur high
amounts~\cite{tom2013exploration}; however, in the long term, it can create
brittle code, increase the cost of new features or products by decreasing
reusability, and even increase production costs by decreasing long term project
velocity as bandwidth is taken by debt repayment or simply attrition from the
sub-optimal products and development pipelines. Tom \etal further found that
this can have an impact on not just productivity, quality, and risk, but also
developer morale as work time is taken on repaying debt and working in a brittle
system that is difficult to reason about.

While Tom \etal's~\cite{tom2013exploration} definition of technical debt covers
some of the impacts from developers on project health: such as low morale, low
documentation, sub-optimal tooling; it does not cover the scenario when the
development process itself has negative incentives. Indeed, a development or
maintenance process, such as bug triaging, may have hidden incentives that, when
each participant behaves optimally for themselves, causes sub-optimal outcomes
for the project, such as wasted development time. The nature of this issue lends
itself well to game-theoretic approaches. Gavidia-Calderon
\etal~\cite{gavidia2020game, gavidia2019assessor} propose that anomalies in the
software development process be modelled using Empirical Game Theory so that a
process intervention can be devised in a way that avoids negative incentives.
They demonstrate their approach using simulation based examples that cover bug
priority inflation, acquisition of technical debt in a simple fix or kugel
example, and the tragedy of the test suite (the accumulation of test cases due
to incorrect developer incentivisation). 

When technical debt cannot be avoided, a risk associated with technical debt is
its interest rate. The work I present in this thesis aims to, first, reduce the
need to accrue technical debt by (semi-)automating tasks that would otherwise be
forgone for a leaner project. When debt is acquired, the work herein aims to
reduce the interest rate. Recasting the perspective into the framework presented
by Gavidia-Calderon \etal, the thesis aims to reduce the cost of actions
associated with good long-term project health in the pay-off matrix, \ie to
ensure that short horizon optimal decisions are also optimal in a long
horizon setting.

\section{Bridging the Gap between Issues and History}
\label{chapter:literature:sec:am_rel_work}

A method of combating accruing knowledge distribution debt is by maintain a high
degree of traceability --- interlinking of development and design artefacts by a
`are related/implements/specifies etc.' link. In this section, I focus on
software traceability as an area, and the position of the commits to issues
traceability task, which we generalise and propose a solution for in
\Cref{chapter:am}, within this wider context.

\subsection{Modern Development}
\label{chapter:literature:sec:am_rel_work:modern_dev}

Modern development increasingly relies on tooling that integrates version
control, issue tracking, wikis, continuous integration and continuous deployment
under a single system. Notable examples are GitHub and Atlassian's JIRA. This
new development paradigm poses new problems and opportunities. Kalliamvakou
\etal~\cite{Kalliamvakou2014} elucidates these, using GitHub as their archetypal
example. A particular opportunity Kalliamvakou~\etal identify is this paradigm's
integration of Version Control and Issue Tracking with the potential to
interlink them. This brings in opportunities for those following in the
footsteps of Bachmann \etal's Missing Links Problem~\cite{MissingLinks}, as they
show that indeed this integration is not fully realised.

Lack of PR-issue links is an ongoing problem in modern software development.  So
much so, Agile practice specifies spring cleaning an issue (a user story in
Agile terminology) backlog. During backlog refinement, developers remove stale
stories and reprioritise and re-estimate remaining stories. When all stories are
stale, this practice discards all sprint-related artefacts --- issues, feature
requests, user stories, as well their links --- in favour of starting the next
sprint from a clean slate~\cite{BacklogRefinement}. Projects must resort to this
spring cleaning all too often~\cite{laurieTrattPersonalCommunciation}. This
practice loses user stories, documentation, issues, discussions, and other such
artefacts associated with a sprint by design; this loss of information comes at
the cost of higher maintenance costs, onboarding costs \etc as the information
has to be recreated or reverse engineered. The loss of documentation it entails
it just one example. This can exacerbate the accumulation of Knowledge
distribution and documentation debt. Adoption of PR-issue inference tooling
promises to reduce the need to resort to this drastic measure by enabling
automatic issue triaging.

\subsection{Software Traceability}
\label{chapter:literature:sec:am_rel_work:tracability}

Requirements engineering (RE) focuses on stakeholders, decision makers, and
their artefacts: requirements, documentation, specification, and design or
architectural documents. These artefacts tend to be natural language, text or
speech, and often go unrecorded. When they are recorded, they exist in multiple
formats, including spreadsheets, email, figures, and printed material. They
further encompass developer artefacts, such as source code, pull-requests,
commits, and issues, but do not focus on them. Further, within an agile context,
documentation is often forgone in favour of staying lean and traveling light.
This further complicates an already difficult task and suggests that tooling
that would target such a paradigm should be light-weight~\cite{Stahl2017}.

\emph{Software traceability} seeks to infer \emph{traces} (\ie links) between
these heterogeneous artefacts~\cite{Cleland-Huang2014}. Missing or hard-to-parse
artefacts greatly complicate trace recovery, which is why much work on
traceability seeks to provide tooling to decision makers to capture or parse
these artefacts and persuade them to use it~\cite{Neumuller06, jiralinkdoc,
ghlinkdoc}. These tools must often record decision maker or developer
interactions, with each other or their tools. They must also avoid being either
disruptive, requiring the developer to switch contexts, or invasive, as when
they require developers to change their workflow or use instrumented
IDEs~\cite{TopicTraceability}, raising privacy and deployability concerns.
Inferring traces over these heterogeneous artefacts, as a consequence of their
heterogeneity, can only leverage abstract, generic features. 

As many of the artefacts within such systems are textual, work in this area
borrowed techniques from Information Retrieval, including ourselves. Borg
\etal~\cite{Borg2014} provide an excellent survey detailing the use of such
methods within traceability. Further, Mills
\etal~\cite{Mills:2017:ATL:3106237.3121280} propose reducing the human effort
required by IR based techniques via a classification step after the recommender
that filters the suggestions. Finally, St{\aa}hl \etal~\cite{Stahl2017} propose
a light-weight framework where different systems can emit traceability events,
facilitating a standardisation of how such links are recorded by minimising the
cost of integrating otherwise incompatible tools within a single eco-system,
which they call the Eiffel framework.

Asuncion and Taylor~\cite{Asuncion:2009:CCL:1556908.1557008} use record-replay
and hypermedia to tackle software traceability as an online problem. Their
online solution cannot employ offline information retrieval techniques (such as
VSM, tf-idf or topic modelling) commonly used in retrospective approaches to
traceability~\cite{4249808}. They focus on requirements and specifications,
architectural modules, source files, and test cases, making no mention of PRs or
issues beyond bug reports. Previous work required expertise in formal
modelling~\cite{Pohl96, Pinheiro96}. To eliminate this barrier to entry, they
instrument developer or decision maker tools, raising the usual deployability
and privacy concerns. Indeed, the solution they present invasively instrumented
specific versions of no less than five different tools, including general
purpose tools like Firefox and MS Word. Many of these tools are under constant
development; updating this instrumentation out-of-tree is extremely expensive,
suggesting that St{\aa}hl~\etal's light-weight approach is more likely to be
maintainable. Asuncion~\etal~\cite{TopicTraceability} in follow-up work
incorporate an LDA model into their framework to improve the interpretability of
their traces. To remain online, they recompute an LDA model on demand for each
graph visualisation and search query.

Additionally, Falessi~\etal~\cite{Falessi2017} have done work towards
quantifying the number of links that are left to be recovered. Their work
assumes an offline, closed-world setting; they provide a framework that can help
an analyst decide when to stop pursuing a traceability maintenance task. The aim
of this work is to provide a quantifiable signal of when the traceability task
should be finished as linking can be considered good enough.

Work in general traceability, due to its broader nature, deals with a profusion
of formats and has, so far, either resorted to intrusive
instrumentation~\cite{Asuncion:2009:CCL:1556908.1557008, TopicTraceability}, or
defining a protocol which tools must implement and communicate
via~\cite{Stahl2017}. The more limited scope of this work allows us to exploit
known structures that developers using platforms such as GitHub naturally
produce: namely issues and pull-requests. This enables Aide-mémoire to rely on
API access from GitHub, or, with minimal additional effort, JIRA, rather than
instrument the developer environment (\Cref{am:sec:imp:deployability}).

\subsection{Commit-Issue Link Inference}
\label{chapter:literature:sec:am_rel_work:cli}

The \emph{missing link problem} is the offline prediction problem of inferring
missing commit-issue links given a version history and issue tracker archive.
Bachman~\etal were the first to formulate and quantify this
problem~\cite{Bird2009,MissingLinks}. Their work aids developers indirectly by
helping researchers and tool-smiths avoid the bias introduced by missing links
that could undermine their techniques or tools. Specifically, they show that by
assuming recorded links to be representative of all links, tools are biased to
use code from more experienced developers, thus not learning from mistakes or
bugs introduced by less experienced contributors. 

Bachman~\etal together with Apache Commons developers manually supplied missing
links and published their Apache Commons corpus. Wo~\etal are the first to
attempt to automatically infer them. They propose ReLink~\cite{relink}, which
measures the similarity of change logs and bug reports with cosine similarity on
tf-idf vectors, learning a threshold for true links. Nguyen~\etal exploited
commit and issue tracker metadata in MLink~\cite{MLink} to improve recall over
ReLink. They evaluated MLink on the Apache Commons corpus, making it the de
facto standard. ReLink and MLink first consider only commit data to form an
initial set of commit-issue links that they then filter. Prechlet and
Pepper~\cite{prechelt2014bflinks} dispense with the initial commit-only stage,
and instead consider both commit and issue data from the start. They argue this
bi-directional inference is more sound. Their BFLinks proposes two link
predictors (based on bug and commit IDs) and a series of filters to reduce the
candidate set of links.

RCLinker~\cite{RCLinker} further improves recall. RCLinker relies on
ChangeScribe~\cite{ChangeScribe} to produce textual descriptions of commits,
especially those that lack commit messages. PRs have their own message and
aggregate multiple commits and their messages.  
This fact alleviates the problem of sparse commit descriptions when considering
issue-PR interlinking. PR descriptions for some projects can be of low quality,
impacting those that would rely on them. Liu \etal~\cite{liu2019automatic}
propose a tool, based on a bi-directional RNN with a copy network, to tackle PR
summarisation, which can help side-step the low-quality PR descriptions.
\Cref{am:sec:reproduction} provides a more detailed description of RCLinker.

Sun~\etal~\cite{FRLink} used non-source files in commits for commit-issue link
inference. They argue that these files are important for capturing developer
intent. They use the standard heuristics, such as checking for camelCase or
snake\_case, to determine the relevancy of a non-source file in a commit. As is
conventional, they implement these heuristics as regexes. They use the resulting
set of non-source files together with the co-committed source files to compute
textual features. They scan a preset and fixed list of the similarity thresholds
to find the maximum F1-score where Recall is at least 0.80. The procedure raises
two unanswered questions. First, how did the authors determine the threshold
granularity? Second, how does training FRLink on F1-score for a task whose
performance is measured in terms of F1-score avoid overfitting? They report
these choices allow FRLink to improve Recall over previous work while matching
or improving F1-score. 

Sun~\etal evaluate FRLink on a new corpus of GitHub projects. This corpus
differs from the Apache corpus used in prior work in the conventions governing
commit messages:  Apache messages tend to be descriptive~\cite{ApachePractice},
while FRLink's GitHub sample tends to contain exact matches, because copying
issue text is common practice~\cite{ruan2019deeplink}. Sun~\etal do not
investigate the effect of this differing practice on their results. They specify
their corpus in sufficient detail to reconstruct it. FRLink, the tool, however,
is not available. When we tried to reproduce FRLink, using its description in
Sun~\etal's paper, we were unable to reproduce the reported results.   
We contacted the authors for help explaining and correcting our reproduction
without response. More recently, Sun~\etal~\cite{PULink} treat existing links as
labels and reformulate the missing link problem into a semi-supervised problem.
As Bachmann~\etal found, existing links are biased; Sun~\etal do not discuss how
they coped with this bias. They report that their solution, PULink, outperforms
FRLink on FRLink's corpus. Like FRLink, PULink is not available.

Ruan~\etal~\cite{ruan2019deeplink} empirically studied the state of commit-issue
linking on GitHub Java projects and found only 42.2\% to be linked. They propose
DeepLink, a neural approach to the missing links problem. DeepLink trains a text
embedding for non-source artefacts and a code embedding for source artefacts
using the skip-gram model~\cite{mikolov2013efficient, mikolov2013distributed},
then passes each of these embeddings separately through an LSTM layer to obtain
the final vector representations. They use cosine similarity to compare the
vectors, choosing the maximum similarity to represent the score of the
commit-issue link. They show an improvement over FRLink in terms of F1-score,
and further show that pre-processing heuristics similar to previous work, such
as ReLink~\cite{relink} or MLink~\cite{MLink}, help DeepLink achieve a higher
F1-score. They also spot that the FRLink corpus had commit logs and issue titles
that are exact matches, introducing bias in the dataset which Ruan~\etal handle,
while FRLink does not. Since they did not evaluate DeepLink on Apache Commons or
against RCLinker and DeepLink is not available, we do not know its performance
relative to RCLinker.

Rath~\etal~\cite{1804.02433} also tackle the missing link problem, but from
within the requirement engineering community and without referencing the line of
work stemming from Bachmann~\etal. The missing link inference work above uses a
vector space model over unigrams for textual features. They opt instead for a
n-gram model. They are the first to perform feature selection, using Weka's
feature auto-selection. They report promising results but on a different dataset
than Apache Commons.

Work focusing on the narrower traceability area of commit-issue linking has so
far been offline, focusing on fixing the situation after the fact, and working
at the commit level. The commit to issue mapping, however, is not
one-to-one~\cite{Kawrykow2011}. Indeed, current practice on GitHub suggests that
the one-to-one mapping be from pull-requests, that may contain a feature branch,
onto issues (\Cref{am:sec:eval:prelim}). Still, \Cref{am:sec:eval:prelim} also
shows that the state of pull-request-issue linking is non-ideal. To address
these issues, we focused Aide-mémoire as an online pull-request-issue linking
tool. Its online nature makes it complementary to offline approaches, while it
has a lower recall, it can still improve the training data of offline approaches
and allow for better link recovery. Its shift to pull-request focuses it on the
modern development practice that is now common on platforms such as GitHub.

\section{Towards Atomic Commits}
\label{chapter:literature:sec:flexeme_rel_work}

A potential source of inaccuracy and noise for the task of maintaining links
between commits and pull-requests is multi-concern commits. A developer, as part
of a bug fix or indeed to facilitate said bug fix, can and often do mix
refactoring changes with fixes. These are then committed as a single
patch. Such commits represent tangled commits. The ideal state of a version
history would be to contain only atomic commits, \ie those that tackle a single
task (would be linked to a single issue or implements a single stakeholder
concern). Changes unrelated to the fix may confuse statistical tools that strive
to maintain traceability links due to the added noise, thus approaches to (1)
detect multi-concern commits and (2) slice multi-concern commits into atomic
patches may aid reduce this source of noise. Further, this can have an added
benefit for developers since tools such as \lstinline+git bisect+ would also
benefit from such a segmentation of commits.

\subsection{Impact of Tangled Commits}
\label{chapter:literature:sec:flexeme_rel_work:impact}

Tao~\etal~\cite{Tao2012} were amongst the first to highlight the problem of
change decomposition in their study on programmer code comprehension; they
highlight the need for decomposition when many files are touched, multiple
features implemented, or multiple bug fixes committed. The last is diagnosed by
Murphy-Hill~\etal~\cite{Murphy-Hill2012} as a deliberate practice to improve
programmer productivity. Tao~\etal conclude that decomposition is required to
aid developer understanding of code changes.

Independently, Herzig~\etal~\cite{Herzig2013, Herzig2016} investigate the impact
of tangled commits on classification and regression tasks within software
engineering research. The authors manually classify a corpora of real-world
changesets as atomic, tangled or unknown, and find that the fraction of tangled
commits in a series of version histories ranges from 7\% to 20\%; they also find
that most projects contain a maximum of four tangled concerns per commit, which
is  consistent with previous findings by Kawrykow and
Robillard~\cite{Kawrykow2011}. They find discover non-atomic commits
significantly impact the accuracy of classification and regression tasks such as
fault localisation.

In this thesis, we borrow the approach that Herzig~\etal~\cite{Herzig2016}
propose for generating a plausible dataset of tangled commits which we use to
compare previous approaches and our proposed approach to untangling commits
(\Cref{flex:sec:eval:corpus}).

\subsection{Untangling Commits into Atomic Patches}
\label{chapter:literature:sec:flexeme_rel_work:untangle}

Research on the impact of both tangled commits and non-essential code changes
prompted an investigation into changeset decomposition. Herzig
\etal~\cite{Herzig2013, Herzig2016} apply confidence voters in concert with
agglomerative clustering to decompose changesets with promising results,
achieving an accuracy of $0.58$-$0.80$ on an artificially constructed dataset
that mimics common causes of tangled commits. In contrast, Kirinuki
\etal~\cite{Kirinuki2014, Kirinuki2017} compile a database of atomic patterns to
aid the identification of tangled commits; they manually classify the resulting
decompositions as True, False, or Unclear, and find more than half of the
commits are correctly identified as tangled. The authors recognise that
employing a database introduces bias into the system and may necessitate
moderation via heuristics, such as ignoring changes that are too fine-grained or
add dependencies.

Other approaches rely on dependency graphs and use-define chains: Roover
\etal~\cite{Roover2017} use a slicing approach to segment commits across a
Program Dependency Graph, and correctly classify commits as (un)tangled in over
90\% of the cases for the systems studied, excluding some projects where they
are hampered by toolchain limitations. They propose, but do not implement, the
use of System Dependency Graphs to reduce some of the limitations of their
approach, such as being solely intraprocedural.

Barnett~\etal~\cite{Barnett2015} implement and evaluate a commit-untangling
prototype. This prototype projects commits onto def-use chains, clusters the
results, then classifies the clusters as trivial or non-trivial. A cluster is
trivial if its def-use chains all fall into the same method. Barnett~\etal
employ a mixed approach to evaluate their prototype. They manually investigated
results with few non-trivial clusters (0-1), finding that their approach
correctly separated 4 of 6 non-atomic commits,  or many non-trivial clusters (>
5), finding that, in all cases, their prototype's sole reliance on def-use
chains lead to excessive clustering. For results containing 2--5 clusters, they
conducted a user-study. They found that 16 out of the 20 developers surveyed
agreed that the presented clusters were correct and complete. This result is
strong evidence that their lightweight and elegant approach is useful,
especially to the tangled commits that Microsoft developers encounter
day-to-day. During the interviews, multiple developers agreed that the changeset
analysed did indeed tangle two different tasks, sometimes even confirming that
developers had themselves separated the commit in question after review. In
addition to validating their prototype, their interviews also found evidence for
the need for commit decomposition tools. Because they use def-use chains and
ignore trivial clusters, Barnett~\etal's approach can miss tangled concerns that
are in the same method. Barnett~\etal's user study itself shows that this can
matter: it reports that some developers disagreed with the classification of
some changesets as trivial.

Dias~\etal~\cite{Dias2015} take a more developer-centric approach and propose
the EpiceaUntangler tool. They instrument the Eclipse IDE and use confidence
voters over fine-grained IDE events that are later converted into a similarity
score via a Random Forest Regressor. This score is used similarly to Herzig
\etal~\cite{Herzig2016}'s metrics, \ie to perform agglomerative clustering. They
take an instrumentation-based approach to harvest information that would
otherwise be lost, such as changes that override earlier ones. This approach
also avoids relying on static analysis. They report a high median success rate
of 91\% when used by developers during a two-week study. While Dias~\etal
sidestep static analysis, they require developers to use an instrumented IDE.

Previous literature proposed approaches were either simple and, by virtue of the
simplicity, efficient~\cite{Barnett2015}, made use of heuristics to construct
their similarities and decompose-recluster commits~\cite{Herzig2016}, or relied
on heavy IDE instrumentation~\cite{Dias2015}. In \Cref{chapter:flexeme}, we
propose an approach that does not rely on heuristics, instead uses similarities
over a multi-version representation of code in a decompose-recluster method
similar to Herzig \etal. While slower than Barnett \etal, it is within 10
seconds (\Cref{flex:sec:impl:deploy}), hence compatible with CI/CD, while being
more accurate (13\% accuracy vs 81\%). Further, Flexeme does not require project
instrumentation and its static analysis can be done incrementally as a
post-commit script.

\subsection{Multiversion Representations of Code}
\label{chapter:literature:sec:flexeme_rel_work:multiversion_repr}

Related work has considered multiversion representations of programs for static
analysis. \citet{kim2006program} investigate the applicability of different
techniques for matching elements between different versions of a program. They
examine different program representations, such as String, AST, CFG, Binary or a
combination of these as well as the tools that work on them on two hypothetical
scenarios. They only consider the ability of the tools to match elements across
versions and leave the compact representation of a multiversion structure as
future work. Some of the conclusions from the matching challenges presented by
\citet{kim2006program} are echoed in Flexeme as well, we make use of the UNIX
diff as it is stored within version histories; however, we also make use of
line-span hints from the compilers for each version of the application to better
facilitate matching nodes within a NFG.

Le~\etal~\cite{Le2014} propose a Multiversion Interprocedural Control Graph
(MVICFG) for efficient and scalable multiversion patch verification over systems
such as the PuTTY SSH client.
Alexandru~\etal~\citep{alexandru2019redundancy} generalise the Le~\etal MVICFG
construction to arbitrary software artefacts by constructing a framework that
creates a multiversion representation of concrete syntax trees for a git
project. They adopt a generic ANTLR parser, allowing them to be language
agnostic, and achieve scalability by state sharing and storing the
multi-revision graph structure in a sparse data structure. They show the
usefulness of such a framework by means of `McCabe’s Complexity', which they
implement in this framework such that it is language agnostic, does not repeat
computations unnecessarily and reuses the data stores in the sparse graph by
propagating from child to parent node. \citet{Sebastian2018} propose a compact,
multiversion AST that cleverly shares state across versions.

Previous work mostly considered multiversion representation of code for static
analysis or the compute of software metrics. We instead focus on multiversion
representations of code as a initial structure from which we can segment atomic
commits. To that end, we propose two new multiversion graph representations of
code, firstly, the \deltaPDG, which generalises Le~\etal's MVICFG to PDGs, and
the \deltaPDGN that further augments \deltaPDGNs with nameflow information
(\Cref{flex:sec:pdgn}), which we hoped to enable us to better approximate the
location of concerns in code via naming conventions.

\subsection{Semantic Slicing of Version Histories}
\label{chapter:literature:sec:flexeme_rel_work:sem_slice_vh}

Features in a system often co-evolve, which tangles the changes made for a one
high-level feature with others in a version history. To resurface
feature-specific changes, they dynamically slice a target version, then walk
backwards in history while they can reverse the intra-version patch without
conflict; at each version they reach, they add any commit that contains a hunk
that touches the current slice to it. The goal of this semantic slicing of
version histories is to find a minimal slice of a version history that captures
the evolution of a feature. Li~\etal~\cite{Li2018, li2015semantic} first
formulated and introduced this problem. Semantic slicing is a form of commit
untangling backwards through history.  This retrospective framing is why they
treat the history as immutable. In this initial solution, Li~\etal treat commits
as atomic so their slices may contain noise introduced by tangled commits. To
reduce this noise, Li~\etal, in more recent work~\cite{Li2019}, unpack commits
into single-file commits into a private, local history.

Flexeme, in contrast, is static and online:  built from the ground up to rewrite
commits as developer make history.  
As such Flexeme and semantic slicing are complementary:  Flexeme would improve
the signal to noise ratio of semantic slicing. An interesting direction for
future work would be to use Flexeme to preprocess version histories prior to
semantically slicing them as with Definer~\cite{Li2019}.

\section{Discerning Text from Code}
\label{chapter:literature:sec:posit_rel_work}

One of the meta-task that can aid the traceability aspect of this research is
the segmentation of English, or natural languages in a more general sense, from
formal languages. While online fora perform a best effort attempt at maintaining
separate formatting for the two modalities of text, this signal remains
noisy~\cite{ponzanelli2014improving}. The work carried out in this thesis seeks
to improve the situation by framing the task as a code-switching phenomenon and
borrowing results from the relevant areas of the Natural Language Processing
(NLP) community (\Cref{chapter:posit}).

In this section, we will focus on first providing how similar techniques were
already employed within the Software Engineering (SE) community, followed by the
more relevant for this task research from the NLP community, concluding with
Ponzanelli~\etal's work on \SO which is closest to POSIT.

\subsection{Part of Speech Tagging in Software Engineering}
\label{chapter:literature:sec:posit_rel_work:pos4se}

In SE research, part-of-speech tagging has been directly applied for identifier
naming~\cite{Binkley2011}, code summarisation~\cite{Haiduc2010a, Haiduc2010b},
concept localisation~\cite{Abebe2010}, traceability-link
recovery~\cite{Capobianco2013}, and bug fixing~\cite{Tian2015}. The main
intuition behind the statistical approaches in this are are due to the
naturalness of code as discussed by Hindle~\etal~\cite{hindle2012naturalness}.

Operating directly on source code (not mixed text), Newman
\etal~\cite{Newman2017} created source code equivalents for lexeme categories
from natural languages. They looked at the behaviour exhibited by Proper Nouns,
Nouns, Pronouns, Adjectives, and Verbs and derived similar notions for
source-code from 1) Abstract syntax trees, 2) how the tokens impact memory, 3)
where they are declared, and 4) what type they have. They report the prevalence
of these categories in source-code. Their goal was to map these code categories
to PoS tags, thereby building a bridge for applying NLP techniques to code for
tasks such as program comprehension. 

Treude~\etal~\cite{Treude2015portuguese} described the challenges of analysing
software documentation written in Portuguese which commonly mixes two natural
languages (Portuguese and English) as well as code. They suggested the
introduction of a new part-of-speech tag called Lexical Item to capture cases
where the ``correct'' tag cannot be determined easily due to language switching.

Instead of adapting code to part-of-speech akin to Newman
\etal~\cite{Newman2017}, in this thesis, we instead wish to treat the problem as
a multi-lingual parsing problem, \ie we want to know what part of speech or part
of code each token is for all languages that we may consider. In POSIT, we
realised a first step towards this by considering the single natural language
and a single C-like source code case. Indeed, it is still an open problem to
tackle situations such as those presented by Treude
\etal~\cite{Treude2015portuguese}.

\subsection{Part of Speech Tagging in Code-Switched Natural Languages}
\label{chapter:literature:sec:posit_rel_work:nlcs}

NLP researchers are growing more interested in code-switching text and speech,
\ie that mixes multiple natural languages, among other reasons due to the higher
availability of corpora. These were made available either by dedicated
collections efforts, such as Miami Bangor~\cite{bangorTalk}, or due to social
media sites, such as twitter, that cause the mixing of English with other
languages~\cite{Vyas2014}. Previously, such data was scarce because
code-switching was stigmatised~\cite{Poplack1980}. 

Focusing specifically on part-of-speech tagging, Solorio and
Liu~\cite{Solorio2008} presented the first statistical approach to the task of
part-of-speech (PoS) tagging code-switching text. On a Spanglish corpus, one
that mixes Spanish and English, they heuristically combine PoS taggers trained
on larger monolingual corpora and obtain 85\% accuracy. We can think of this
approach as a mixtures of experts model. Jamatia~\etal~\cite{Jamatia2015},
working on an English-Hindi corpus gathered from Facebook and Twitter, recreated
Solorio's and Liu's tagger and additionally proposed a tagger built on
Conditional Random Fields. The mixtures of experts model performed better at
72\% vs 71.6\%. In 2018, Soto and Hirschberg~\cite{Soto2018} proposed a neural
network approach, opting to solve two related problems simultaneously:
part-of-speech tagging and Language ID tagging. The second task can be thought
of as a segmentation task, which in our English-code setting can be rendered
using formatting for code-blocks. They combined a biLSTM with a CRF network at
both outputs and fused the two learning targets by simply summing the respective
losses. This network achieves a test accuracy of 90.25\% on the inter-sentential
code-switched dataset from Miami Bangor~\cite{bangorTalk}.

The languages that previous literature considered for code-switching tasks may
have different tag distributions or even tag sets; although not as disjoint as
in our case with POSIT. This provided an initial starting point for promising
architectures. \Cref{posit:sec:impl} details how we combined these approaches to
enables us to solve a version of our joint tagging and segmentation task.

\subsection{Mixed-Text: When Natural Meets Formal}
\label{chapter:literature:sec:posit_rel_work:mixed}

In the context of natural languages mixed with source code and other formal
languages (mixed-text), Ponzanelli~\etal are the first to go beyond using
regular expressions to parse such mixed text. When customising
LexRank~\cite{Ponzanelli2015b}, a summarisation tool for mixed text, they
employed an island grammar that parses Java and stack-trace islands embedded in
natural language, which is relegated to water. They followed up LexRank with
\stormed, a tool that uses an island grammar to parse Java, JSON, and XML
islands in mixed text \SO posts, again relegating natural language to
water~\cite{Ponzanelli2015a}.  \stormed produces heterogeneous abstract syntax
trees (AST), which are ASTs decorated with natural language snippets. They make
both the tool and the resulting corpus available. \stormed, however, remains
reliant on either code-block annotations or heuristics to discern code from
English. POSIT instead learns this segmentation from data, and, after a manual
evaluation, we indeed observed it to be capable of segmenting \SO posts where
submitters forgotten to include the appropriate formatting
(\Cref{posit:sec:application}).