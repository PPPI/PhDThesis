% Encoding: UTF-8

@inproceedings{1804.02433,
  author    = {Michael Rath and Jacob Rendall and Jin L. C. Guo and Jane Cleland-Huang and Patrick Maeder},
  booktitle = {Proceedings of the 40th International Conference on Software Engineering},
  title     = {{Traceability in the Wild: Automatically Augmenting Incomplete Trace Links}},
  year      = {2018},
  doi       = {10.1145/3180155.3180207},
  eprint    = {arXiv:1804.02433}
}

@inproceedings{292398,
  author    = {O. C. Z. Gotel and C. W. Finkelstein},
  booktitle = {Proceedings of IEEE International Conference on Requirements Engineering},
  title     = {An analysis of the requirements traceability problem},
  year      = {1994},
  month     = apr,
  pages     = {94--101},
  doi       = {10.1109/ICRE.1994.292398},
  keywords  = {systems analysis;post-requirements specification traceability;pre-requirements specification traceability;requirements engineering practice;requirements traceability problem analysis;requirements traceability tools;Educational institutions;Guidelines;Project management;Research and development}
}

@article{4249808,
  author   = {J. Cleland-Huang and B. Berenbach and S. Clark and R. Settimi and E. Romanova},
  journal  = {Computer},
  title    = {{Best Practices for Automated Traceability}},
  year     = {2007},
  issn     = {0018-9162},
  month    = jun,
  number   = {6},
  pages    = {27--35},
  volume   = {40},
  doi      = {10.1109/MC.2007.195},
  keywords = {document handling;formal specification;information retrieval;automated traceability;information-retrieval techniques;legacy documents;requirements trace matrix;Best practices;Capability maturity model;Code standards;Maintenance engineering;Manuals;Software maintenance;Standards development;Standards organizations;System testing;Unified modeling language;automated traceability}
}
@inproceedings{Adel2013,
  abstract  = {In this paper, we investigate the appli-cation of recurrent neural network lan-guage models (RNNLM) and factored language models (FLM) to the task of language modeling for Code-Switching speech. We present a way to integrate part-of-speech tags (POS) and language in-formation (LID) into these models which leads to significant improvements in terms of perplexity. Furthermore, a comparison between RNNLMs and FLMs and a de-tailed analysis of perplexities on the dif-ferent backoff levels are performed. Fi-nally, we show that recurrent neural net-works and factored language models can be combined using linear interpolation to achieve the best performance. The final combined language model provides 37.8{\%} relative improvement in terms of perplex-ity on the SEAME development set and a relative improvement of 32.7{\%} on the evaluation set compared to the traditional n-gram language model.},
  author    = {Adel, Heike and Vu, Ngoc Thang and Schultz, Tanja},
  booktitle = {Proc. 51st Annu. Meet. Assoc. Comput. Linguist.},
  isbn      = {9781937284510},
  pages     = {206--211},
  title     = {{Combination of Recurrent Neural Networks and Factored Language Models for Code-Switching Language Modeling}},
  year      = {2013}
}
@article{alexandru2019redundancy,
  title     = {Redundancy-free analysis of multi-revision software artifacts},
  author    = {Alexandru, Carol V and Panichella, Sebastiano and Proksch, Sebastian and Gall, Harald C},
  journal   = {Empirical Software Engineering},
  volume    = {24},
  number    = {1},
  pages     = {332--380},
  year      = {2019},
  publisher = {Springer}
}

@inproceedings{Alghamdi2016,
  author    = {Alghamdi, Fahad and Molina, Giovanni and Diab, Mona and Solorio, Thamar and Hawwari, Abdelati and Soto, Victor and Hirschberg, Julia},
  booktitle = {Proc. Second Work. Comput. Approaches to Code Switch. EMNLP},
  title     = {{Part of Speech Tagging for Code Switched Data}},
  year      = {2016},
  abstract  = {We address the problem of Part of Speech tag-ging (POS) in the context of linguistic code switching (CS). CS is the phenomenon where a speaker switches between two languages or variants of the same language within or across utterances, known as intra-sentential or inter-sentential CS, respectively. Process-ing CS data is especially challenging in intra-sentential data given state of the art monolin-gual NLP technology since such technology is geared toward the processing of one language at a time. In this paper we explore multiple strategies of applying state of the art POS tag-gers to CS data. We investigate the landscape in two CS language pairs, Spanish-English and Modern Standard Arabic-Arabic dialects. We compare the use of two POS taggers vs. a unified tagger trained on CS data. Our results show that applying a machine learning frame-work using two state fof the art POS taggers achieves better performance compared to all other approaches that we investigate.},
  doi       = {10.18653/v1/w16-5812}
}
@article{Allamanis2018,
  abstract        = {Learning tasks on source code (i.e., formal languages) have been considered recently, but most work has tried to transfer natural language methods and does not capitalize on the unique opportunities offered by code's known sematics. For example, long-range dependencies induced by using the same variable or function in distant locations are often not considered. We propose to use graphs to represent both the syntactic and semantic structure of code and use graph-based deep learning methods to learn to reason over program structures. In this work, we present how to construct graphs from source code and how to scale Gated Graph Neural Networks training to such large graphs. We evaluate our method on two tasks: VARNAMING, in which a network attempts to predict the name of a variable given its usage, and VARMISUSE, in which the network learns to reason about selecting the correct variable that should be used at a given program location. Our comparison to methods that use less structured program representations shows the advantages of modeling known structure, and suggests that our models learn to infer meaningful names and to solve the VARMISUSE task in many cases. Additionally, our testing showed that VARMISUSE identifies a number of bugs in mature open-source projects.},
  archiveprefix   = {arXiv},
  arxivid         = {1711.00740},
  author          = {Allamanis, Miltiadis and Brockschmidt, Marc and Khademi, Mahmoud},
  eprint          = {1711.00740},
  journal         = {6th Int. Conf. Learn. Represent. ICLR 2018 - Conf. Track Proc.},
  mendeley-groups = {Untangle},
  pages           = {1--17},
  title           = {{Learning to represent programs with graphs}},
  year            = {2018}
}

@inproceedings{Asuncion:2009:CCL:1556908.1557008,
  author    = {Asuncion, Hazeline U. and Taylor, Richard N.},
  booktitle = {Proceedings of the 2009 ICSE Workshop on Traceability in Emerging Forms of Software Engineering},
  title     = {{Capturing Custom Link Semantics Among Heterogeneous Artifacts and Tools}},
  year      = {2009},
  address   = {Washington, DC, USA},
  pages     = {1--5},
  publisher = {IEEE Computer Society},
  series    = {TEFSE '09},
  acmid     = {1557008},
  doi       = {10.1109/TEFSE.2009.5069574},
  isbn      = {978-1-4244-3741-2},
  numpages  = {5}
}

@article{AuthorTopic,
  author   = {Steyvers, Mark and Smyth, Padhraic and Rosen-Zvi, Michal and Griffiths, Thomas},
  journal  = {Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining},
  title    = {{Probabilistic author-topic models for information discovery}},
  year     = {2004},
  number   = {1990},
  pages    = {315},
  abstract = {We propose a new unsupervised learning technique for ex- tracting information from large text collections. We model documents as if they were generated by a two-stage stochas- tic process. Each author is represented by a probability distribution over topics, and each topic is represented as a probability distribution over words for that topic. The words in a multi-author paper are assumed to be the result of a mixture of each authors' topic mixture. The topic-word and author-topic distributions are learned from data in an unsupervised manner using a Markov chain Monte Carlo al- gorithm. We apply the methodology to a large corpus of 160,000 abstracts and 85,000 authors from the well-known CiteSeer digital library, and learn a model with 300 topics. We discuss in detail the interpretation of the results dis- covered by the system including specific topic and author models, ranking of authors by topic and topics by author, significant trends in the computer science literature between 1990 and 2002, parsing of abstracts by topics and authors and detection of unusual papers by specific authors. An on- line query interface to the model is also discussed that allows interactive exploration of author-topic models for corpora such as CiteSeer.},
  doi      = {10.1145/1014052.1014087},
  isbn     = {1581138889},
  keywords = {gibbs sampling,text modeling,unsupervised learning},
  pmid     = {4086762619115516774}
}
@misc{BacklogRefinement,
  title        = {{Agile Alliance: Backlog refinement}},
  author       = {{Agile Alliance}},
  howpublished = {\url{https://www.agilealliance.org/glossary/backlog-grooming/}},
  note         = {Accessed: 2019-11-26},
  year         = {2019}
}

@article{Baldi2008,
  author   = {Baldi, Pierre F. and Lopes, Cristina V. and Linstead, Erik J. and Bajracharya, Sushil K.},
  journal  = {ACM SIGPLAN Notices},
  title    = {{A theory of aspects as latent topics}},
  year     = {2008},
  issn     = {0362-1340},
  number   = {10},
  pages    = {543},
  volume   = {43},
  abstract = {After more than 10 years, Aspect-Oriented Programming (AOP) is still a controversial idea. While the concept of aspects appeals to everyone's intuitions, concrete AOP solutions often fail to convince researchers and practitioners alike. This discrepancy results in part from a lack of an adequate theory of aspects, which in turn leads to the development of AOP solutions that are useful in limited situations. We propose a new theory of aspects that can be summarized as follows: concerns are latent topics that can be automatically extracted using statistical topic modeling techniques adapted to software. Software scattering and tangling can be measured precisely by the entropies of the underlying topic-over-files and files-over-topics distributions. Aspects are latent topics with high scattering entropy. The theory is validated empirically on both the large scale, with a study of 4,632 Java projects, and the small scale, with a study of 5 individual projects. From these analyses, we identify two dozen topics that emerge as general-purpose aspects across multiple projects, as well as project-specific topics/concerns. The approach is also shown to produce results that are compatible with previous methods for identifying aspects, and also extends them. Our work provides not only a concrete approach for identifying aspects at several scales in an unsupervised manner but, more importantly, a formulation of AOP grounded in information theory. The understanding of aspects under this new perspective makes additional progress toward the design of models and tools that facilitate software development.},
  doi      = {10.1145/1449955.1449807},
  isbn     = {9781605582153},
  keywords = {aspect-oriented programming,scattering,tan-}
}

@misc{bangorTalk,
  author       = {Deuchar, Margaret},
  howpublished = {\url{http://www.bangortalk.org.uk/speakers.php?c= miami}},
  title        = {{BilingBank Spanish-English Miami Corpus}},
  year         = {2010},
  doi          = {10.21415/T5J01D},
  publisher    = {TalkBank}
}

@article{Barnett2015,
  author   = {Barnett, Mike and Bird, Christian and Brunet, Jo{\猃犷提栝蜷予貊孱漉水觑躜钺序镢深舢蔑铈语骠鳟蓬绠糸綮儒祓轭溴鲥祜疱蝮桢祓翳屙箦祧弩刘麸磲糸溴泔眇矬轸轱镦泔溴蝈鲩鬻汨犷珏箦趔遽舶钡轶箢安钒挡捣铛礅弪刘珲篝舶贝疳珏背喘贝待鲲祯礤饼徕篝蜥泗蔑溴义鲩鬻蟋犷轫痫螋犷犷痫瘐灬礤汨犷轶骘聃犰轸狍篚蜥钽瀣狎镦翦疱蜴矧礤镱汨犷珏箦衄箦镦盹溟骈邃骈戾翳狒狎礤犷麸忮泔眄轸翦麸箫躜沐蝈痫箝麸蝙狍犷狒镯殂徙糸镱疹溴蝮翎钿轭泔溴蝈鲩鬻轶盹蝈溟骀殂蹯麒孱翳汨犷珏箦泔铙轶趔镦眭祠轲戾轭溴疱钿孱衄泔溴溟骀弪孱沐螽族轭趄镤蹉锰沼耘颐攘吻庞犷狨麸磲糸翦汨铋聃骘溴泔眇矬轭汨犷珏箦趔犷弼犰踽翦轸彐驽泗轹孱弩翳蝻蹒怙翳聃犷糸翎糸鲥犷犰箝犷聃犰轸狒轹躞弪篝蹁滹卑北肮擅优舶钡车轶忸狗副捶构惫炒谍泪螋殂戾麻珑弪篝徭姹构超徕篝蜥泗疱蝮镱躅溴蝮翎钿痱镧蜥忮汜躞翳妁狎徕戾麸蝈灬翦翳篝蝓泗躜弩镦翳痱镧蜥犷轸孱鲩蝻铐孱麸翳彘桴磲矧殄铘邃泔钽屦趱犰腩秣戾溏徕秕翳黠蜢洚澡痱镡戾镦溟筱秭弪轭轭溟鲩漉犰桴磲矧殄铘邃泔钽屦趔犷狍箝珙轭翳屙麸翳彘轫痨屙孱翎糸镱矧殄铘邃泔躅翦蝠狎趔骘玳鲥痱镧蜥轶翳泔钽屦狍箝珙礤铘痱镡戾懋族鏖祆狎珲翳狒翳箫祯糸镱麸翳轶痱镡戾蝈聃轵弩礤翳镤翳狒栳鲥篝蝻铉痨狨箝忪蝈狍镱轭泔眇镱孱舢族鏖祆殪祯篝蜥翦翳弩殇遽翳蝻蹒屮犴痨筱孱狎轱躞轭犷屮轶糸铉溴箝珙蝈泔鲥蝙簌篝屙汜祆邃呐由遗崎钺祆麇鏖祆弼犰踽翦呐由遗忉箦镱轸躞徵镱蝈犰黠蜢痱镡戾眢秭弪翳遽蝮狨翳矧麻珑弪篝徭娆藻十犷烷翕犷溴颥妈狎狒钱犷族怏翦颥尼祆狍滹卑北肮擅优惫钩炒栋狈轶忸碍副付撤鞍谍轶箢鞍暗钩抖觑躜钺序镢惫钩钡翳深舢蔑铈语骠鳟蓬绠脲黠蜾义鲥蝮孱玳铄弪轭绗泔钽屦衄泔铑邈糸镱轶衄滹磲轭骈蝮屮痱弩箝镱镦泔眇豸狒轱钺飕轭翦铘轶溴箝珙邃骘颥腩秣戾溏忉箦痨狨箝忪蝈狍镱轭绗蝈泔珙轸轱瞵箪殂轭琮礤钿屐妁珧秕痼疹翎铉戾铛礅弪洱疳珏锤箔垂庚痦殇炒栋狈糸綮澡泔钽屦狍箝珙礤铘痱镡戾轭痱镧蜥躅溴蝮翎钿轭琮躜梏麴函徙徜屙殂蝈箦狎汨黹泸矬镦舢泔懑絮忪殂狒轱畀荡胺沟翳瀛泔钽屦舡狍箝珙礤铘痱镡戾憝轭痱镧蜥憝躅溴蝮翎钿轭琨堀得铊趑鸷殄邋痨矧瀹殄邋矧绡祓滹泱屦殂俺黩狃疱虍梏砜狎铛礅弪匠炊氨符堀得铊趑鸷鼢鳟钽忾铎懋铋璁顼霪瘐忭邃炒栋狈堀得铊趑鸷鼢鳟筱镳躞泔懑轭麽螨鲲祯礤贡遽惫钩泪螋殂戾麻珑弪篝徭姹构船狨翳矧麻珑弪篝徭娆藻十犷烷翕犷溴颥妈狎狒钱犷族怏翦颥尼祆狍女觑躜钺蔑眄躅殂狒轱铙镦翳撩妄糸綮序镧蜥躅溴蝮翎钿轭犷翳泔钽屦狍箝珙礤铘痱镡戾睚遽惫勾轶箢鞍氨胺覆铛礅弪谍疳珏凡覆鲲祯礤撤徕篝蜥泗物翦厦弪蝻蝮磲忮骘躅轭翳轶义驽蝈钽涕篝屮趄徙翦骝镯翳骢祆翦狎糸沆瀹撩栳镳翦麸屮痫箦翳泔眇戾翦涕篝蜥翳弪翳犷镱禊泔蝌邈犷扉铍邃蝈驽蝈钽弩滹卑北吹狈挡拱狈党鞍轶忸鞍氨胺覆篱铕蝻沐邃轭珞麻铍戾舶北狨翳矧麻铍戾尼鲥犷儒狎瞵歪趑桢犷提黩殄尼黝怙镫糸綮序镢邋溟铉隔罪螂蔑铈烷町语骠鳟义痫螽陀П饼糸綮身痱秭轭殇孱糸骈弪轭骘蝽狒轹孱弩躞轭疳螋镦箴邋汨轭骘蝽狒轱铨遽舶北徜潋弩五亠螂五亠螂沼笼疳珏舶除瘐忪轶桢撩序弩簖徕篝蜥泗义沐铘箫骠麽蝈溴鲥祜痦孱麸镬栳鲥屮痨镩翦翳黹铋铉镦钺趱蜥灬铉踽珏轭骘蝽狒轱骘躅鏖翳轭箫骠麽蝈犷轸篚痧矧糸铉滹沲礤铘狒轱町燥磲脲翳盹篝镦翳轶轭骘蝽狒轱瞵蝈箦狎汨弪栳鲥潋狩躔镱翳黠螂镦翳钺趱蜥灬铉踽珏痱镢弩箝铉泔眄躅轸骘麸镬犷翦汨铋聃弩项篚汨麸镬痱秭殇弩疳螋镦箴邋汨轭骘蝽狒轱瞵麒殂骈钿狃痨殂狒轱轭轫痱秭轭翳箦狎汨轭镦箫骠麽蝈蝈痫箝麸蜷弩犷屮趄徙糸铉滹磲轭轭骘蝽狒轱骘躅轭殇孱糸骈弪螽疹骘螋躅狒屐翳钺趱蜥灬铉踽珏骘躅轶箫骠麽蝈溟骀弪骝镯翳狒骘躅轭篝犷溽蜾痱矬瀹澡轶溟骀弪孱沐痫翦铘獒祆扉黹趔翳彐驽泗轹孱弩镦镦姝翳瀛箬屐麸镬螽令屙痖蜷汜轭鲥篝殓狒轱骈钿翳狒鏖翳黹铋磲珲殇犷沐犷屮轶糸铉翎珑弪麽泔蝌邈父堀镦翳糸礤麒孱翎珑轭翳黠蜾骘躅轭箫躜沐泔溴殇孱糸骈弪螽澡轭鲥篝殓狒轱翳孱躞弩翳轫痱秭邃疳螋镦箴邋汨轭骘蝽狒轱麸翎灬蜱泔蝠躞镦秭弪贝惮鞍篝蝓泗躜瀛骈屐钺礤螽乞镯疳趑弪铙轭翳翎珞箦鲥蜥蝓戾屙弪珏翳狒箦咫麸躅溴蝮翎钿疳篝躞徵犷麸轫痱秭骢趱蝈钺黹铉滹卑北吹惫傅创碑惫傅捶饼轶忸狗副吹俺暗反俘轶箢安钒挡捣脲黠蜾殇孱糸骈弪犷犰箝蟋钺趱蜥灬铉踽珏痱镢弩箝铉痱镧蜥泔眇蝈桢铙轱铨礤钿屐妁珧秕痼蔑溴痼艴滹酗翎珑轭琮躜梏麴函痫螋犰徙懋矧绡汩翎糸镱沔砜滹殇奖垢荡幢惫傅捶饼泪螋殂戾麻蜾舶肮狨翳矧麻蜾描蜷篝獒犷箩汨磲铑龄蜷犷犷刘铄砰蜷犷孽骀曙桀犷洛蝾篝彘瞵菱蜥栳犷崎祀秭朱徜轫轵犷腻鲠钼醅序屙膈磲螨觑躜钺序镢庞琶朴琵糸綮漆轵犷箩灬钽邃亢麻狍轭迈绛崎尼翎箦趔遽舶肮轶箢氨陡倍刀疳珏辈杯背褒徕篝蜥泗语骠麽蝈孱玳铄弪轭蝈箦狎汨弪栳鲥祜铉忮孱轭翦蝈篝邃轭麒弪犷麒怩珞镢沲轭泔溴犷轭痱邃殂糸铉麒弪翳妁黹玷趱蝾躔铄舢乳篝矧殂犰怩绛镢沲蝈钽溽翎栳忮孱脲麸翳轶蝈箦狎汨迈趄徙腴铉簌篝屙蟋犷泔溴鲥蝮轱栝篝矧殄蟋蝈泔蜾麒孱栾犷怡麒镯怩珞麇蝈骈邃骝镯翳弩箫躜沐蟋溽翎箦趔翳狒蝈灬翦骈戾汨犷珏麸怩骈弩汜忮屮趄徙翦洚澡弩栝篝矧殂犰溽翎箦趔汜忮躞邃麸翦篝棂痫翳弩弩泔钽弪铋铉痱镢弩箦镦怩轭趄镤蹉糸镱犷犰箫麸怩殪篝狒轶糸汜怩痱邃殂糸镱盹溴祗疹骘螋躅狒屐痱镢弩箦犷桴磲铙狎轫疱蜴邈衄犷镱禊骝徙糸镱镦怩骈弩狎徙趱犰禊灬忮祆邃轭箫躜沐泔溴鲥蝮轱栝篝矧殄蟋犷翳躞忮泔礤狯衢灬忪骘篝蹁轭翳屮趄徙翦溽翎箦趔澡聃弩糸镱钺趱蜥祆狎轶弩狎翳怩骈弩蝈泔蜾邃轭翳弩栝篝矧殂犰溽翎箦趔驷轵蝈痱弩孱翎糸镱镦翳骢祆痫瘐灬糸镱镦怩骈弩深翳轶疳疱颥麇轭鲥篝殓狒栝篝矧殂犰溽翎骝镯箦鲥蜥箫骠麽蝈痱镪邈趔犷骈钿篝蝻铉弼殇孱沐镦簌篝屙狒殂忾狍族翳孱轭鲥篝殓狒翳痫翦铘獒彐驽泗镦Ⅴ铈衢颥轫忉灬钽邃溽翎箦趔镱翳疱蜴矧磲钽镦痱邃殂糸镱翦汨铋聃弩族潋狩翳戾篌镱翳狒忾狍轶泸轸殂犰痱镡戾翳狒翳蝈狒孱怙翳翳彐驽泗轹孱弩镦痱镢弩箦翳狒蝈禊镱忾狍邃溽翎箦趔麸怩殪痱邃殂糸镱盹溴祗犷翳珏铄蜥扉徕殪轸镦棂痫翳弩弩翦篝邃镱忾狍邃溽翎滹卑北吹钡沟豆懂钡沟繁洱轶忸狗腑杯栋档腑鞍杯昌痦殇备垢窗贝棱镲臌麻蜾遂彘钐镳弪肮狨翳矧郁弼孱麻蜾犷坯犷遂彘犷配麽蜾田疱螨糸綮吾趱蜥提铉踽珏序镢弩箝铉鏖翳轩翳镱瘐忪轶桢惜义殪禊湾溟猃遽舶肮棱镲臌买彘磲瞰复狨翳矧买彘磲瞵坍犷乞殄漤犷十犷翔祗孱耶犷郁镱瀣卯瘐忪轶桢揍潴黠螋深翦蝾狒轱钺球秕瘕糸綮渺狍箝骈汜糸镱犷蝈珧弩箝镱趄邋簖遽惫复徜潋弩洛祉镱衄昧汩翦蹯殡瀛狎糸沆瀛殇钩北窗滹卑渤胺驳嘲勾洱痫篝邃狒舶岸北岸案喊泛荡痱轱蜷豉褒泪螋殂戾买镢塍汨黹漪舶惫徕篝蜥泗清铄蜥糸鲥盹溴祗骘箫躜沐泔溴狎犷轭翦蝈篝轭篝蝓泗躜邃痱邃殂糸镱痱镡戾憩蝈聃轵轭麸蝈狍镱徕秕怙翳栳蜾簌铘徙糸犷箦磲铘殂泔铙趄衢铘狍麇祆狍徕秕钺趱蜥飕扉脲禊痱镧蜥眢族痱弩孱铒鲥盹溴骘翳轶痱镡戾翳狒躞弩珧狃麸蝈痱弩孱翳轭翦蝽邃獒翦篝狒镦翳珏铄蜥翦秕麴豸硝盹溴珏铄蜥翦泔溴怡轭翦蜢遽鲩铉珧犴磲颦潋轹孱屮疳铙轱篝屦鏖翳珧狃狨珥孱翎糸镱犷铄躜犰礤篌徵疳篌轭篝屦螽令屮疱蜷礤铘犰弼犰踽糸镱箬秣翳狒秕铄盹溴汜珏铄蜥翦箦磲铘殂犰禊礤犷轭珂蹯屮痱弩箝镱蟋秕麴弪骘蝽轭蜥铉镦篝蝻铉忉箦扉铄螽狎汨轹屦蝈骈狎亻鳊狎轹殇备暗案垂褒狨翳矧买镢塍汨黹漪歪蜚犷领灬磲铋蟋烷祠獒溟犷轻躅衄领屮犷溴犷酗祜秭响咫筢钿螨屦蜷铘备暗案垂褒觑躜钺肤深舢蔑铈体狎町义痱弩孱舢擅桃舶惫礤钿屐妁珧秕痼疹翎铉戾疳珏杯泊糸綮清铄蜥糸鲥泔溴盹溴扉铉鏖翳珧狃梵遽舶惫泪螋殂戾迈缣镢箩箦湎钊轶麸蝙狨翳矧亠蹴遂狨描犷珞躅犷凌瞵术铄犷体瀣捧铙屣臊觑躜钺深骘蝽狒轱犷语骠麽蝈藻汨铒祜琦糸綮身痱秭邃怩祜汜扉狒轱忉箦镱泔溴汨犷珏栝篝矧殄犷怩蝈痫螋簖遽舶狈轶箢肮蛋蹈垂疳珏狈翻惫昌鲲祯礤覆徕篝蜥泗蔑铘屮渝鲥蜥轶篚弩矧溴驽泗轭蝈戾狍邃箫骠麽蝈漉蜷铉翳磲轭翦钺钽痂狍狎蝈痫螋邃麸翳溴鲥祜痦孱翦犴婶轶泔篝禊犷糸礤泔铙蹴轭骘溴鲥祜疱蝮麸痱邈轶屐祜汜扉怩珞迈蝈痫螋犷翳泔溴汨犷珏栝篝矧狎骝羼蹂铘禊躞邃犷痱秭殇轭骘蝽狒轱骘殇孱糸纟轭驷蹯祜汜糸镱漉蜷铉翳箫骠麽蝈磲轭翦钺钽痂狍瀹镶赍泗轹婶轶溟骀殂蹯麸篝犷溽蜾辁翳篝戾镦怩蝈痫螋黩轸翦轭钺趱蜥灬铉踽珏麸轫痱秭翳徙沲蜥泫镦怩祜汜扉狒轱町澡镡赍泗轹镦翳轶疳疱轶麸痱镳矬犷彐驽泗轹轭骘蝽狒轱蝈趄殄鲠飙忉箦怩祜汜扉狒轱礤翳镤麸骈钿篚箴殂轱躞骈戾犷礤翳镤骘蝈箫祧轭怩珞湾翳镤深翳轶疳疱颥麇痱镳矬铒鲥轭骘蝽狒轱蝈趄殄鲠飙忉箦怩祜汜扉狒轱狃痱镝汨翦蝽邃迈田汜扉狒轱躞轭深翦珧狒邃令犰箝绿闪┊硝痱镳矬邃绿闪轭翦珧狒弩犷犰邃溽翎怡豸殪辁轭翦趔篝徙趄徙弩犷泔眄孱趔轭怩蝈痫螋蟋篝蝓泗躜邃轭骘蝽狒轱镦箫躜沐骈戾蟋犷翳箫躜沐泔溴汨犷珏栝篝矧族轫痱秭邃翳珧犷蹯狎轸镦怩祜汜扉狒轱骝镯翳骈戾戾鲥麸翳礤翳镤戾鲥怡屮翦钿轭痱弼轱躞怩蝈痫箝麸蝙溽翎义篚祠族弼犰踽翦翳彐驽泗轹孱弩镦秕狃痱镝汨忉箦镱屮疱蜷礤铘躞轭翳蝈镳孱箫躜沐痱镪邈趔钺礤禊馏疱泗尸幼袁犷谪轭绠深翦蝽镦翳礤犷狯弪徵痱邈轶轱瞵镱狯弪徵秕狃痱镝汨轫痱秭弩翳礤趄殂镦迈缣镢狒矧绿臻椰乱则徙弪另崽玑犷翳痱屐轫轭狎鲥蝮轱镦绿闪怡荡堀床堀嘲堀驳堀犷钡堀蝈箴邈糸鲥禊狒翳骈戾戾鲥镦怩祜汜扉狒轱町蔑钽祯箝镱蔑眇狎邃鏖翳痱轱麸镬蟋翳蝈篚祠箬秣邃翳狒绿闪秕麴弪骘蝽翳弩雉桢礤翳镤螽族犷犰邃翳轭骒蹂钽镦遽汨筱矧镦绿闪骝镯鲠蜷秕泔礅轭狒轱铙忉箦镱翳犷犰邃轭骘蝽狒轱町硝痱镳矬邃孱栳钽屙孱箝珙殒殂犷綮轫痱秭邃翳徙沲蜥泫燥轫痱秭翳珧犷蹯狎轸戾鲥镦怩祜汜扉狒轱瞵铄狃痱镝汨狒翳礤翳镤戾鲥轶痱镳矬邃犷轸痫翦铘獒轶弼犰踽翦洚滹卑卑倍戤轭骟镦舶倍北鞍昌轶忸狗副炊烦苟创庚脲黠蜾迈祜汜扉狒轱瞵迈蝈痫螋蟋蔑溴汨犷珏栝篝矧深骘蝽狒轱蝈趄殄鲠飕湾翳镤犷犰箝蟋郁徙趄徙弩瘐忪轶桢澎箦鲩弪庐之篱铕蝻沐邃轭珞迈缬泔豸狨翳矧廉援午貔孱犷援援午貔孱犷十领孙驷栝犷犬之午貔孱犷援萎午貔孱怙镫糸綮舶北捕翳膳排撩深翦蝾狒轱钺蔑铈弪孱沐镱刘麸磲翦语骠麽蝈蓬玳铄弪轭劣舶北糸綮麸痖悱忉箦狃痱镝汨骘钺蝌秣轭翳箦狎汨箴徙镦怩珑骈戾骝镯怩蝈痫螋遽舶北盹铘铒霈疳珏捕抄卜昌滹卑北肮劣女舶北侗鞍岸昌轶箢惫掣闯鞍脲黠蜾痱镧蜥溴怩珑轭缁箫骠麽蝈孱玳铄弪轭缁翦犷犰箝蠡迈缬泔豸烩蹒蝈痫螋烩蹒琦泔溴烩蹒琦骈戾蠡箦狎汨箴徙寤箫骠麽蝈溴鲥祜痦孱艋箫躜沐泔溴霍屮趱犰泔铘孱趔霍镳殂忉箦狃痱镝汨恍蝈溟泗轱犰顼蜷翳眢挥镦赭狎犰顼蜷翳眢挥镦赭狎簌篝屙蠡御钽栩镱辁狒轱罨则衢铋铉恢邈麸蝮荒彐邈田汜扉狒轱罨燥痖惋溴扉铉篱铕蝻沐邃轭珞迈缭蜷徵瀣狨翳矧钱籴铉犷援阼犷犷庐体妪怙镫糸綮舶贝膳排掣翳令铛犰蔑眇豸弪语骠麽蝈犷琉痨殂狒轱铙蔑铈弪孱沐糸綮燥麽蜾渝黹狨麸磲糸迈则獒珏犷渝鲥蜷豉序邃殂糸镱箩箦镱燥痖惋溴犷王祠榄驽狒躜镦迈义痫螋簖遽舶贝盹铘牾飕疳珏狗卑洱滹卑北肮孟托恿卯舶贝倍脲黠蜾痱镧蜥溴怩珑轭缁瘐忪殂滹磲轭箫骠麽蝈惑镦赭狎磲轭翦钺钽寤陪扉痼寤惋殪灬晃弭忮犷蠡怩骈轭缁怩蝈痫箝麸蝙烩蹒蝈箫祯糸镱烩蹒箦鲥蜷豉痱邃殂糸镱昏轶麸蜷汜怩蝈痫螋蠡镳孱箫躜沐痱镪邈趔惑屙獒豸镯狒殂怩趄獒珏惑镦赭狎磲轭翦钺钽寤麸痖屮趄徙糸镱霍镳殂盹溴旎零沲蜥泫幻镯瘐翦怩珞黄遽趱蝈屮趄徙糸镱恍蝈溟泗轹盹溴祗挥镢獒铄赭矧箦蝣殂弩挥镦赭狎寤皱泗矧蠡怩趄獒珏汇矧蝈泗轹箫骠麽蝈磲轭翦钺钽寤眭祠榄驽狒躜寤箦鲥蜷豉痱邃殂糸镱霍镳殂盹溴忑泪螋殂戾冕痫忾犷泔舶背狨翳矧冕痫忾犷泔情秭犷铋犷条汩岈令潋遽腻犷响轹弭铿绎沣犷嗅铋汨屐灬令铋忉戾犷嗅铋汨屐灬渝忉篝獒铒觑躜钺曙躜钺镦语骠麽蝈碰镬豸轱犷序镢弩簖糸綮身痱秭轭梢忉箦趄徙遽忾扉豉蝈泔鲥蝙鲩铒躅忉箦轭溴轭镦箫骠麽蝈狎糸驷泗簖遽舶背铛礅弪俘疳珏反抄范昌鲲祯礤驳瘐忪轶桢组戾项扉铄涕怛狎泪螋殂戾冕蝣犰栾舶钡狨翳矧冕蝣犰栾熙铒裔盹犷领礤殇岈曙篼堙妪曙荥猃犷儒铗轳蹂蟋绣潋裔铉屐犷轴蜥钿岈歪蜷曙荥猃稞觑躜钺十御篝语骠鳟糸綮乞镯箫躜沐泔溴殇孱糸骈弪麸钺趱蜥灬铉踽珏翦蝽簖遽舶钡轶箢氨洞辈辈疳珏北翻辈庚鲲祯礤卑褒徕篝蜥泗序镧蜥泔眇蝈桢铙轱翦汨铋聃弩镦翦屮痨矧痱镧蜥殇孱糸骈弪蟋麸轭驽腩秣戾溏徕秕痱镧蜥眢澡蝈戾鲠钽镦箫躜沐泔溴殇孱糸骈弪狍镱蝈戾鲠铘箫躜沐镦轭骘蝽狒轱徕秕痱镧蜥眢轶犰蝈徜弩翎忪轶桢轭翳扉翦蜥趱蝈狍麇祆狍翳彘溟蝈泗轫疳泗镱骢趱蝈泔眇蝈桢铙轱翎箅螽惋篝痱镧蜥眄轭灬铉踽珏孱骘蜚箫礤泔铙趄衢铙镱殇孱糸骈弪篝蜷铉ㄥ绠麒轸箴徙弩矧泔眄狍狎铒犰祜麇洎领箫痱镧蜥眄弪镦翦躞黠蜾泔礅轭狒轱铙犷徕怛弼獒糸镱蟋麸溴鲩箦篝蜷铉翳狒蝈痱弩孱箝铉戾矧眭祠轲戾滹磲轭泔钽屦趔轭矧溴麸轭泸遽箦痱镧蜥眄轭扉铉蹰篝殂彐骈汩孱泫ㄣ镱鲥盹蝈箦磲铘殂黩轸轭戾篌┊澡弩篝蜷铉滹铒犰麽躞屮痨殂轸磲螂麸溟篝轭珲轶翳翦蝽躞邃ㄥ绠冕礤烀狍矧躅溴蝮泔蝈螬箫翦汨铋聃弩镦翦蝈驽蝌邃狍栳蜾箴扉趑轭狎铒孱秕玷澡轶疳疱轭趄镤蹉弩涕铉踽汉射羽扉趑弪溟泗轱钺蝙忉箦犰顼蜷翳骘箴扉趑轭犷屮疳钿轭篝蜷铉翳狒泔眇矬眭祠榄翦蝽殇孱糸骈弪螽婶屮痨矧弩翳躞镦珏铄蜥痱镧蜥眄轭犷徕怛弼獒糸镱溟泗轱钺蜷弩怩犰箫沲篝镯溟泗轱钺蝙狨麸磲糸汜祆珏铄蜥翦骝镯箫骠麽蝈钺趱蜥灬铉踽珏泔铘孱衄痱镱麸轭沆蹁狃痨殂狒轱滹磲轭翦蝽犷箴邈殒殂徕怛弼獒糸镱螽澡轶狃痱镝汨麽狃痨殄麸赭箫骠麽蝈疳汶徵弩黩轸翦轭矛徙栝弼轭姝礤狍躜镦狎秕钿拱堀骘泔蝌邈綮箴扉趑轭犷屮疳钿轭殇孱糸骈弪螽泔眇狎轶镱鏖翳沲蝌孱篝狒瀛镦翳瀛狎狃痱镝汨弩轶犰箫痱弩孱翦洚滹卑卑倍戤牦螽舶贝卑氨除脲黠蜾射孱糸骈弪箴扉趑轭绗吾趱蜥灬铉踽珏痱镢弩箝铉序镧蜥泔眇蝈桢铙轱铨泪螋殂戾描犷珏鱼蜷忮狨翳矧涕钺蝈蟓轴篑蹂歪蜷犷蔑螋弩蔑条轶棋蝾犷滹犷琉镱翦梳轵犷酗箬鲠铢氍腻铢簖觑躜钺序镢邋溟铉深翦蝾狒轱钺蔑铈弪孱沐镱语骠麽蝈蓬玳铄弪轭琮糸綮描犷珏鱼蜷忮燥镬骘刘麸磲糸汜祆清铄蜥糸铉蔑眄轸湾篌徵弩遽舶钡轶箢安钒挡捣疳珏钒弓繁昌鲲祯礤昌徕篝蜥泗孽蜷铉箫骠麽蝈磲轭翦钺钽弩翎箅蟋泔眄轸礤篌徵弩狎犷轫痫螋犷箫躜沐镦轭骘蝽狒轱瞵腩秣戾溏瀣犷滹沲礤铘狒轱翳狒溴鲥祜疱蝮蝈禊躔镱蕊麇鲥颥翳铛礅弪犷钺趱蝈镦溽殪徙糸鲩糸弩犷轭翦蝌躔糸镱汜轭骒蹂钽翳聃犰轸镦蝈篚祠轭泔眄轸礤篌徵弩澡轶骘蝽犰溴盹铙趄狒轱疳疱痱弩孱趔描犷珏鱼蜷忮麸镬骘狨麸磲糸汜祆珏铄蜥糸铉泔眄轸礤篌徵弩描犷珏鱼蜷忮轶狯衢灬忪狒梏麴函鼢鳟泱黜邃醑箦礤蝓汨犷珏筱蜷忮ㄅ沆轲箦痨蹒轭轭篝蝓泗轱铙溴盹犷翳箫躜沐泔溴┊滹卑北肮擅优舶钡膊过轶忸狗副捶构惫炒谍脲黠蜾蔑溴汨犷珏蟋蔑眄轸礤篌徵瀣吁眄狎辁狒轱铨泪螋殂戾渺屐犷洵弱犷绮氨船狨翳矧渺屐犷洵弱犷绗梳铄犷秋翦飕响禊犷柔弩梳铄弱骀磲犷望堍猃溴颥嗅趄殂犷陂箜犷令潋遽觑躜钺葡优舶贝序镢邋溟铉镦翳镱契趱蝈镦语骠麽蝈蓬玳铄弪轭ǔ遏擅优舶贝糸綮语骠麽蝈则徙遽忾扉豉则孱潴犷契趱蝈拈蝈泗轱铙遽舶贝疳珏档豆徕篝蜥泗语骠麽蝈趄徙遽忾扉豉轶箫蹒梏徭翦颥弭镦翦屐躞轹聃犰轸轭箫骠麽蝈轭翦铙轹簌篝屙螽义聃轵邃轭筢驽豉泸轸殂犰簌篝屙怡磲铢沐螋殒轭怙溟弩篚汨狍翳沼棋溴蜥瘤獒糸镱刘翳矧轸箫骠麽蝈趄徙遽忾扉豉轶犷弩箦铘獒屐屙孱镦翳箫骠麽蝈溴鲥祜痦孱痱镢弩螽深痱徙糸沐趄徙遽忾扉豉轶镦翦泔钿蹉翦轭犷徜栾悻徭翦颦翳瀛驷泗磲铑弪犷洮翳弪彐矧瀣轸忮铄骈趔狎铒犰麽骢祆蝈犰辁邃霄弪翳疳篝溴汜溴蝈箦狎汨弪栳鲥骘沲箦镱箴邈殒殂狎遽镦翳趄徙遽忾扉豉痱镡戾憩溴鲥祜痖铉盹蝈箫痂轶糸汜翦麸镬轭绗痱镯雉轭篝蜥翦玳痨犷铋铉狃痨轭轭骘蝽狒轱蝈趄殄鲠翦汨铋聃弩汜疳忪镦箦黹狨麸磲糸铉翳趄徙泸遽糸镱犷磲轭翦钺钽痱镢弩蟋溴鲥祜痖铉铄趄徙聃弪灬铉踽珏犷鲩篚犰辁狒轱翦汨铋聃弩翳狒躞趄徙扉铍蟋犷狃痨轭趄徙遽忾扉豉轭箴邈殒殂滹磲轭篚汨狍惋溴尿轹孱腻鲥祜痦孱衄痱镤蹉扉铄簌篝屙蟋犷徵殪痱镪邈孱鲩蝻铐孱趔深翳轶疳疱颥麇怩殪躔镱痱轱怙澌镦黠螂麸栝玷扉玷翳篝狒瀛镦翳瀛狎轭箫骠麽蝈趄徙遽忾扉豉犷麸痱弩孱泔眇屐扉铉狎遽镦蝈箦狎汨翳狒铄邃麸忮徜潋弩箦洚滹卑北吹驳钩父伯驳钩腹饼轶忸狗腑杯吹俺哺兜待脲黠蜾蝻徜磲瓞箫骠麽蝈趄徙遽忾扉豉篱钽镬戾泗轱铥沆屐犷洳氨掺蜥沐徕殪轸糸綮则徙遽忾扉豉轭徵殪痱镪邈趔狨翳矧渺屐犷洵弱犷绗梳铄怙镫糸綮语骠麽蝈犷御篝屙则徙遽忾扉豉疳珏捕淡卜谍遽舶辈瘐忪轶桢羽蜷铉弪理轶沱蔑溴蔑眄孱裘矧瘐蟋狨翳矧熊掎蜍汊酏徙栝序镦轵绣趄犷尼箬俞铘犷犷则艴溴描蜷篝镳犷箩蝌裴蜢札栾黟踱扉箬邃荃蜢梏麴蠛玳翳踱泔懑行猩邢由辕忪镡磲篝弪溽翎泔蝠矧岑祯汩洚轲铒翦巯铎轭寤徙沐篌邃泊梳瞽舶舶蔟糸綮蔑溴蔑眄孱蔑蝠躞遽舶惫泪螋殂戾蔑桢畋苟艾狨翳矧梳泔蔑桢铨觑躜钺配蹉狒轱钺犷畜汨镬镧殂犰湾狍躜屙孱酏糸綮蔑彐骈汩孱镦羚蝈屙孱骘物黹钺鱼犰弩遽惫栋铛礅弪饼疳珏撤炊鲲祯礤舶滹卑北贩鞍背倍创栋安鞍氨按屦蜷铘梏麴蠛滹楫矧绡卑北贩鞍背倍创栋安鞍氨按篱铕蝻沐邃轭珞蔑礅轭轭缬趄蹉趱蝈组翳逃涩狨翳矧歪戾糸悻曙钺翳犷僧犷歪蜚躞令潋獒铨怙镫糸綮序镢邋溟铉镦翳渤蜾深翦蝾狒轱钺蔑铈弪孱沐镱语骠麽蝈蓬玳铄弪轭琮糸綮吁痧矧糸铉序镧蜥蔑眇蝈桢铙轱阵轭渝磲铘殂犷郁蝓泗躜犰深骘蝽狒轱铨遽舶氨徜潋弩揍箬轭玺镱拿沼笼疳珏卑抄北昌瘐忪轶桢膳排蔑眇豸弪语汩弭箦蜷弩擅优О饼徙黹掣贝复轶忸碍范沟卑蛋俘祜汜糸镱燥蝻铘铿项翎蜷铿冕钺溽铛眇徵弩卑泪螋殂戾蔑眄孱裘镨弪犷沐渝磲铘殂娱黹灬蜷豉狨翳矧籴铉清躅箦镫犷阼犷绗葬犷体瀣蛮躅珀屣铉觑躜钺序镢邋溟铉深翦蝾狒轱钺蔑眇豸弪语骠麽蝈犷琉痨殂狒轱铙蔑铈弪孱沐糸綮燥麽蜾箦黹狨麸磲糸怩趄獒珏犷箦鲥蜷豉痱邃殂糸镱忉箦镱麸痖盹溴犷眭祠榄驽狒躜镦怩蝈痫螋簖遽舶贝轶箢胺嘲潮捣疳珏狗卑洱徕篝蜥泗迈骈轭轶犷弩箦铘獒徙糸鲩豉轭翳箫骠麽蝈磲轭翦钺钽瀣忮汜躞盹篝镦翳箫骠麽蝈簌篝屙栳鲥躅狯镩溽忪溴驽泗螽阻孱铄怩珞狎篚忭轸翦洮趄獒珏蝮栳鲥麸骈钿犷狍箝珙狃痱镳蜷狒溴鲥祜疱蝮麸骈翳怩珞蕊麇鲥颥殒翳怩珞狎狒骈蝮狍箝珙邃麸轭狃痱镳蜷狒溴鲥祜疱蝮翳妁磲灬翦栳鲥麸忮蝈狍箝珙邃麸雉桢溴鲥祜疱蝮澡狒轭泸遽箦翳糸礤犷泔篝骘骈轭怩珞澡弪彐矧瀣骈钿轭狃痱镳蜷狒溴鲥祜疱蝮忮泔礤脲麸怩蝈箫祯糸镱阻孱趄獒珏蝮狍箝珙铄怩蝈痫螋轸轶铄沐篌狎麸溴汩溴栾聃殂腱翳怩蝈痫螋箬秕熹忮徜潋弩箦洚澡躞翳怩箦鲥蜷豉轶犷轫痫螋犷驷泗矧轭怩骈轭绠深翳轶疳疱颥麇痱镳矬铒鲥礤翳镤骘翳怩趄獒珏犷怩箦鲥蜷豉痱邃殂糸镱崎蝮衄麇屮趄徙麸痖悒螬骝镯栝篝矧殂犰怩蝈痫螋轭翳怩蝈痫箝麸蝙犷骈钿怩蝈痫螋蝈灬翦麸遽汨麸痖惝阻孱铄怩蝈痫螋狎蜷鲥蟋麇溴汩溴翳麸痖悒螬麸麒殂翳蝈痫螋忮祜铉螽澡孱麇豸殪辁眭祠榄驽狒躜麸殇孱糸纟泔蝌弩痫钿轭蝈痫螋翳狒栳鲥翳筢礤眭祠榄驽狒躜ㄥ绠蔑眇镱孱衄痱镤蹉衄痱轱蜷豉犷箦鲥蜷豉鏖翳翳铄怩蝈痫螋澡躞玳鲥铄怩蝈痫螋麇狎徕戾麸蝈泔眄孱翳盹篝狃痱镳蜷狒溴鲥祜疱麸骈遽汨怩犷痱邃殂轸箦鲥蜷豉燥弼犰踽翦秕狃痱镝汨麇铒镱禊礤狍躜邃翳彐驽泗轹孱弩镦秕篝蹁怡躞轭徕秕嘲鞍顼熹孱怩蝈痫螋屮趄徙翦骝镯翳蝈镳孱箫躜沐痱镪邈趔ㄅ沆轲箦惋殪灬犷五忮犷螬怩犰箫泔眇狎邃箫礤蝈灬翦篝蹁殄螽澡蝈篚祠箬秣翳狒秕狃痱镝汨轶扉脲禊麸彐驽泗轹屐蝈泔眄孱翳狃痱镳蜷狒溴鲥祜疱麸骈翳玳鲥怩犷痱邃殂轸箦鲥蜷豉滹卑北肮孟托恿卯舶贝倍轶忸狗腑杯捶构车返庚脲黠蜾怩趄獒珏泔蝌邈糸鲥箫骠麽蝈磲轭翦钺钽瀣眭祠榄驽狒躜瀣箦鲥蜷豉痱邃殂糸镱麸痖盹溴忑泪螋殂戾蔑钽屦籼镢犰轶狒轱瞵狨翳矧菱邂瀣吁蜥驽体眄犷领殂犷翦令轸犷蔑蜥岈令钺犷燥铄祆岈嗅镬稞觑躜钺曙躜钺镦御篝屙犷语骠麽蝈糸綮吁痧矧糸铉泔钽屦祜汜糸镱翳蝻蹒殇孱糸骈弪疳蝮轭犷镱麸祜琦屮趄徙糸镱遽舶背轶箢氨洞辈辈盹铘铒霈铛礅弪北疳珏补惫补掣鲲祯礤付滹卑卑倍戤牦螽舶背胺鞍过泪螋殂戾弥颅棠连狨翳矧藻璎馘阻犷五黜犷尼鲩犷族祆轭绗歪犷五犴犷凝觑躜钺龄鲠钽弩轭五躜犰深骘蝽狒轱序镢弩箝铉御篝屙惫ㄎ尚舶岸糸綮蔑祆狃箦轴蜷狒轱钺箩弩獒深驽蝈钽领顼蜷翳骘提翦铘拈蜷汨戾领祜汜糸镱遽舶胺轶箢卑垂挡蹈疳珏背党背栋徕篝蜥泗提翦铘拈蜷汨戾犰祜汜糸镱ㄌ牧轶箩弩獒铄赭矧翳狒栳蝈沐铘禊玑轭邃眭汨痫瘐灬蜷豉轭狃痨殂狒轱铙蜥铉轭骝镯滹沲礤铘盹溴扉铉麸泔眇豸弪鲩箝镱孽麸翳灬蜱筱犰钺趱蝈镦翳弩狃痨殂狒轱铙沲蝌孱轭驽蝈钽痱镢邃躜弩扉脲鲠蜷狒轱钺箩弩犷情忖筢眇扉铉栳鲥忮孱骘躅灬汶轭绠深翳轶疳疱麇痱镳矬翳泔祆狃箦鲠蜷狒轱钺箩弩獒轭驽蝈钽犰顼蜷翳骘棠连犷箬秣翳狒轸轶泔眇豸狒轱钺祆彐骈汩孱衄遽簌麸轫痨屙孱犷箝珙殒殂犷綮盹蝈徙沲蜥翦翳犷篝犷溽蜾鲠蜷狒轱钺箩弩獒轭驽蝈钽骘棠廉滹卑脖渤动徜岫补沟洱轶忸碍捕箔惫刀腑昌脲黠蜾棠梁泔祆狃箦鲠蜷狒轱钺箩弩葬祜屮疳铙轱瞵狃痱秫轫狒轱铨郎钚蝻沐邃轭珞腻屦体狎铋铉骑蛟蜥沐蟋狨翳矧十酋犷十描孱犷十渺屐犷洵弱犷琮怙镫糸綮舶狈膳排撩彻翳深翦蝾狒轱钺蔑铈弪孱沐镱语骠麽蝈蓬玳铄弪轭ㄉ糜农糸綮渝磲铘殂犰禊蓬栳钽邃语骠麽蝈则徙遽忾扉豉阵轭腻屦体狎铋铉藻汨铋聃弩遽舶狈盹铘磲疳珏抄贝滹卑北肮擅优舶狈过屦蜷铘备按安闯个饼脲黠蜾歪汨轭戾狎铋铉晃狒躜犰灬铉踽珏痱镢弩箝铉灰邈躜蝈铘铄躜犰铄赭矧塍挥屙犷糸泱挥镦赭狎寤郁犷溽蜾蠡则衢铋铉荒邋体狎铋铉灰邈躜蝈铘五躜犰五赭矧牖渝磲铘殂义痱弩孱翎糸镱辉蜥沐徕殪轸篱铕蝻沐邃轭珞拈狍舶钡狨翳矧拈狍歪螋轭犷箩沣桢祆楝领忮螋犷秋躞轱蟋清矧玳矬犷冕篌秕尼黹孱犷孽汜篌瀣郁屦栳铄怙镫糸綮舶钡膳排膊钿深舢蔑铈语骠鳟令犰碰镬义孱玳铄弪轭绗恿闻舶钡序镢糸綮疹翎铉扉铉骈铄珧衢铄泔溴汨犷珏簖遽舶钡盹铘磲颥疳珏炒杯车褒瘐忪轶桢膳排徕篝蜥泗伶翦黠螂轭骘箫礤糸礤溴鲥祜疱蝮泔眄轸翳彘泔溴汨犷珏麸鲥蝮轱泔铘蝻簌篝屙阻孱滹轭箫翳妁镦翦怩钿戾躅蝈灬翦汨犷珏ㄥ绠怩骈犷蝈驷泗矧轭绌轭箝铉戾泔眄轸翳躞泸遽糸铉箫汜祆邃翎铉戾泔眄轸予狎轭翎铉戾泔眄轸轶痱镡戾磲糸忮汜躞轸磲脲蝈鲩鬻蝈鲥蝮轱瞵犷轭翦珧狒轱镦翳弩泔眄轸栳蜾弪犷栝篝矧殂犰犷犰箦镦翳痱镪邈戾篌蝈扉徕戾义箦狎汨弪栳鲥黠螂邃狒躅翎铉扉铉屮轶糸铉泔眄轸蟋楫瀹骈钿轭麒殂疳螋镦泔眄轸蝈灬翦麸麒殂翎箅深翳轶疳疱颥麇泔铘蜷怩翦麸翳轶扉铄镦黠螂轭赭麽蠛ū瘐忪殂禊狯衢灬忪溽翎箦镦躅翎铉戾泔溴汨犷珏蟋泸遽翦鏖翳翳桢祓镦赭溴鲥祜疱蝮麒徙沲蜥翦禊箴扉翳彘泔溴汨犷珏轭麸箦戽泔铘衢铄翎箅秭弪疱蜷镤镦骘躜盹铘梵ú铒鲥狃痱镝汨硼殂遽疹翎铉戾颥麸桢祓溴鲥祜疱蝮箬狎躅翎铉戾泔眄轸ㄡ脶狒镯殂泔眄轸螬怡躞轭骈铄珧衢铄泔溴汨犷珏轭骘蝽狒轱町硼殂遽疹翎铉戾轶忉箦犷翦篝邃镱翳瘐忪殂禊狯衢灬忪溽翎箦衄犷骢螋桢弼犰踽翦怡溴痨稆轭轸麸溴鲥祜疱蝮麒躞邃轸骘麇咫螽族蝈泔蜾邃礤溟犷篚沣弩蜥翦镦贡堀犷狯弪徵镱镦返堀轭狨麸磲糸汜祆泸遽糸铉沆躞翦蝮镦躅翎铉戾骈铄珧衢铄泔溴汨犷珏螽狎汨轹屦蝈骈狎亻鳊狎轹殇钡安岸返俘滹卑北肮恿闻耶舶钡钒副复待屦蜷铘钡安岸返俘轶忸狗副捶构复豆谍轶箢钡炒党当礤钿屐妁珧秕痼疹翎铉戾躜梏麴函殄邋痨矧瀹殄邋矧绡滹沲礤铘钒副复疮篱铕蝻沐邃轭珞拈骀弪孱糁邈麸蛞屦蝈箦铘狒轱铙狨翳矧钱冕痫忾犷泔犷廉腻条汩犷耶响轹弭犷廉嗅铋汨屐灬犷赢嗅铋汨屐灬怙镫糸綮舶肮倍翳罪螂轭蔑铈弪孱沐镱义鲥蝮蓬玳铄弪轭琮糸綮则徙遽忾扉豉义泔鲥蝙阵轭熙礤蜷汜令犰箝簖遽舶肮盹铘镢衄疳珏惫淡舶待滹卑北肮酌遗舶肮贝轶箢卑沟背蛋脲黠蜾轭骘蝽狒轱蝈趄殄鲠旎箫骠麽蝈孱玳铄弪轭缁箫躜沐泔溟铉皇孱箦瞽予犷铒礤翳镤会螋殒徙灬铉踽珏汇镤寤轭骘蝽狒轱蝈趄殄鲠旎灬翦铘箦磲铘殂轭溴轭缁铛礤蜷汜犷犰箝蠡箫骠麽蝈滹沲礤铘狒轱罨趄徙遽忾扉豉蝈泔鲥蝙祸邈麸箴徙盹溴旎鲥泗矧忉箦梢盹溴祗荒镢蹴孱翎糸镱簧钿屮轭缁深骘蝽狒轱蝈趄殄鲠旎深翦蝠镬狒轱罨提蜱筱犰轭翦珧狒轱罨熙礤蜷汜犷犰箝蠡义鲥蝮孱玳铄弪轭缁语骠麽蝈孱玳铄弪轭缁语骠麽蝈磲轭翦钺钽寤羽扉铄宦箴扉铄蠡彭痖蜷汜郁蹁殄蠡深骘蝽狒轱义趄殄鲠旎叔铙孱予犷铒礤翳镤惶狒孱渝磲铘殂深溴轭缁熙礤蜷汜令犰箝蠡则徙遽忾扉豉义泔鲥蝙恢邈麸羽徙惋溴忑棱镲臌娘钺熹田铉尼翎狨翳矧儒溴脲颥娘钺熹耶犷绎忮螋漠情忖镱簖瘐忪轶桢组戾墉深翦蝮汩孱沐糸綮田铉轸蹁轭犰尼翎令犰箝簖遽舶岸轶忸狗赴捶贝舶卜过泪螋殂戾裴蜢漆殪邃身痨屙孱粜蝈溟泗轱瞵狨翳矧崎酊珏蜥熹冕黹祜犷体糸弪彭磲铛屐犷崎铍屐篝彘瞵令翳镱觑躜钺义聃轵屙孱趔蓬玳铄弪轭琮糸綮裴蜢驷殪躜痱邃殂糸镱轭驽狒躜蝈聃弩磲钺珏礤铘簌篝屙蠛令屮翦钿邃篝蹁遽舶辈轶箢肮捶扯安铛礅弪昌疳珏北翻背昌鲲祯礤狈徕篝蜥泗项扉铄驽狒躜蝈聃弩磲钺珏礤铘簌篝屙狎痫瘐灬麸镬骘玑翳弪轭篝犭彖镬溴蝮汨犷珏蝈聃弩趔漉蜷铉簌篝屙弼镬豸轱町腻汩溟铉麒殂驽狒躜蝈聃弩趔蝈聃轵狒翦铘轱犷栾眭汨躔骝镱犷犰箝麸疱蜴矧镱翳屙轶犷轫痫螋犷痱镡戾轭翳轶泔铘屮艉麸扉趑戾躔骝镱犷犰箝磲蝈篚祠轭轭徜羼踽翦骢钽糸镱犰轸殄忮轭溴鲥祜疱洮泔篝禊汨犷珏蟋犷麽篝邃溴鲥祜痦孱彐骘螋麸眭汨躔骝镱犷犰箝轶麽篝镦糸礤犷蝈箫躜沐螽裴蜢痱邃殂糸镱徕秕麒殂驽狒躜蝈聃弩趔狎盹篝扉脲禊麸驷殪漉麸轭篚骀殂殄铘矧轭徜羼踽翦躔骝镱犷犰箝泔蹯驷汩扉翎翦篚汨溴汩箝镱螽硝镡赍泗轹轶麸篝蹁麒弭桢轸轶痫篌殁戾麸磲脲篚汨痱邃殂糸镱狨麸磲糸汜祆骝镯翳汨狎徙翦蜷篝殂镦翳镱扉铄溟筱躞箝镱镱驽狒躜蝈聃弩趔澡轶疳疱痱弩孱趔篝蹁镦驽狒躜蝈聃弩驷殪躜弩轭箦鲥灬蜱痱镪邈趔犷狨麸磲翦麸镬轫痨屙孱翦骝犴鬻矧骘泔铙趄蹉糸铉驷殪躜痱邃殂糸镱盹溴祗犷泔眇狎轶镱镦翳疱蜴矧磲钽镦翳溟骀弪孱痱邃殂糸镱翦汨铋聃弩骘翳弩痱镪邈趔澡泔眇狎轶镱蝈扉弩镱泔篝忮铄骈盹溴骘狍箦篌轭翳鲠祯镦徜溟糸镱犰躔骝镱犷犰箝螽深翳轶盹溴飕翳鲠祯镦徜溟糸镱犰躔骝镱犷犰箝溴疱钿镱轸痱镡徕殪轸镦篚沣弩轭痱弼孱糸铉驷殪躜弩犷镱翳蝈灬糸鲥泔篝镦翳驷殪躜弩轸痱弼孱趔泔眇狎邃麸轸秣泔篝族箬秣翳狒骘蝈狍镱徕戾弩糸磲糸镱镦翳弩赭疳蜥礤翦蝮狨麸磲翦痱邃殂糸镱盹溴祗痱秭殇盹蝈鲠祯翳犷箦镦忉箦扉铄骘磲铢驷殪躜豉疱犷痱镪邈趔澡轶篚珑弩趔狨麸磲翦驷殪躜痱邃殂糸镱漉蜷铉蝈聃轵屙孱趔屐殂轸狒轱麸忮痱镯轶轭狃痱镝汨骘珲殇轭蝈聃轵屙孱趔孱玳铄弪轭彐骘螋轭镱扉铄箦趑轭珞滹卑卑胺蟀胺抖氨箔氨蛋俘轶忸狗副吹贩肮渤待脲黠蜾蔑篝忮铄骈镦蝈聃轵屙孱趔孱玳铄弪轭绗裴蜢驷殪躜痱邃殂糸镱棋狒躜蝈聃弩趔磲钺珏礤铘簌篝屙蟋庆镡犰箫骠麽蝈溴鲥祜痦孱衄橡孱箫躜沐篱铕蝻沐邃轭珞蓬箪孱舶肮徕篝蜥泗刘麸磲翦箫骠麽蝈孱玳铄弪轭麸镬ㄥ绠痱镧蜥箦狎汨泔钽弪祜汜糸镱泔溴蝈躞瀣聃犰轸狍箦篌礤铘弭惝轭泸遽箝铉禊蝈禊镱钺趱蜥灬铉踽珏轭骘蝽狒轱骝镯泔眄孱趔犷殇孱糸骈弪轭泔溴澡骈蝮篝屦轭犷犰轭黠蜾骝镯殇孱糸骈弪蝈聃轵弩箴扉趑轭殇孱糸骈弪轭麸翳彘泔铙糸趱孱黠蜾螽疹扉脲钺趱蜥灬铉踽珏蟋麒弪箴徙犷瘐钽趱狒轱狎躞邃麸溴扉铄狒黠蜾蟋殇孱糸骈弪汜铑雉泔铘衢箴徙弩项泔眄镱麽麸箴扉殇孱糸骈弪轶麸骘祆秣痱镧蜥眄轭灬铉踽珏钺黹铉泔铞孱糸镱螽骑屮犴痨瀣梳鲠痱镧蜥眄弪镦翦躞汜礤汜箦麒弪黠蜾狎溴扉铄狒邃怡躔疱蜚狍戾趑弪矧铒瞽犰痂徕弭殂汨狎徙翦蝮蕊麇鲥颥痱镧蜥眄弪犰箫泸遽翦殇孱糸骈弪怡泔钽狒孱狒轭箦聃孱沐镦黠蜾麸珏翳弪鏖翳铒溟筱弪铋忪溴扉铄狒轱瞵麒殂痫箦汨犰戾铉弩麸狨麸磲糸殇孱糸骈弪箴扉趑轭绠深翳轶疳疱颥麇痱弩孱犷犰顼蜷翳麸狨麸磲糸汜祆箴扉殇孱糸骈弪轭麸箦聃孱沐镦黠蜾怡黹铋铉黠蜾骝羼蹂钽殄轭箫躜沐泔溴组翳翳弩黠蜾骝羼蹂钽殄蟋秕殇孱糸骈弪箴扉趑弪躞弩筱矧轭翦汨铋聃麸狨麸磲糸汜祆箦戾泗翳盹篝狃痱镳蜷狒疳螋轸轱铋铉骘犷殇孱糸骈弪深犷弼犰踽糸镱镦秭弪赴鞍殇孱糸骈弪骝镯镳孱箫躜沐梳鲠痱镧蜥眢秕俞眭蜥狃痱镝汨秕麴弪骘蝽翳屮轶糸铉篝狒镦翳狎翦汨铋聃弩狨翳矧蓬箪孱膨殂犷乳祆彭殪犷酗祆镢氍田蜷犷珠赆予犷脲颥水怙镫糸綮序镢舶肮遏膳排深舢罪螂蔑铈烷町语骠鳟义痫螽陀舶肮滹卑北肮陀耶舶肮蛋豆锤昌轶忸狗副床创炒钩褒轶箢脖栋备挡疳珏繁赴糸綮烷铋铉箫躜沐泔溴麸狨麸磲糸汜祆箴扉殇孱糸骈弪骘箫骠麽蝈犷犰箝簖遽舶肮篱铕蝻沐邃轭珞朋桦弼狎椴氨艾狨翳矧朋桦弼狎楝提戾惋躞狯辇怙镫糸綮序镢罪螂蔑铈义鲥蝮蓬绠酌遗糸綮涕铉蹰篝殂潋轹孱蝈驷泗矧轭镦箫躜沐泔溴殇孱糸骈弪簖遽舶卑疳珏补翻嘲褒徕篝蜥泗射孱糸骈弪狎犷轫痫螋犷箫躜沐镦轭骘蝽狒轱漉蜷铉痱镧蜥躅溴蝮翎钿轭犷磲轭翦钺钽瀹序镧蜥眄弪镦翦躞殇孱糸骈弪麸怩殪翳彘礤铘犰盹溴祗镦翳箫骠麽蝈狎糸驷泗螽族栳鲥疱蜴矧礤痱屐轫轭狎篝蹁麸屮犴轭翳蝈灬糸镱忮赭邋翳翦蝽轭殇孱糸骈弪蟋翳彘箴蝈徜轭孱糸糸弩犷驷蹯痱镱孱弩螽族轭趄镤蹉邃翦蝽孱趄镳犷泔铘屮舡泔鲥蜥珏麸礤狍躜栾筱狒翦蝈翦蝽狎徙蝻篌痱镧蜥孱糸糸弩犷栾躅蝈灬翦狎翳礤翳镤犷狒趄殁豸弩泔铘衢铋铉翳弩翦蝽螽硝蝈篚祠箬秣邃翳狒礤翳镤犷狒趄殁豸弩泔铘衢铋铉翦蝽鏖翳栝玷孱趄镳犷泔铘屮舡泔鲥蜥珏狎盹蝈驷蹯舡痱镱瀹族痨犷麸怩殪镱翳轶篝蹁怡屮趄徙糸铉扉铉蹰篝殂轭骘蝽狒轱骘蝽礤翳镤犷沆狍箦螽阵轭翳轶轭骘蝽狒轱瞵麇痨犷麸弩翎忪轶趄徙遽忾扉豉扉铍骝镯滹磲轭泔钽屦趔麸箫躜沐泔溴犷麸痱镳矬扉铉蹰篝殂忉箦蝈驷泗矧轭绠滹卑北肮酌遗舶卑创轶忸狗赴范沟幢渤谍轶箢卑沟背蛋篱铕蝻沐邃轭珞砒痨衢铋铉腻驽泗蟋狨翳矧援犬描孱犷赢桩澡镯狍犷彤吾玑痧犷犷廉女柔篌犷怙镫糸綮舶辈刽膳排罪螂轭蔑铈弪孱沐镱烷铋铉语骠麽蝈义痫箝麸蜷弩ㄍ右糸綮砒痨衢铋铉箫骠麽蝈溴驽泗躞轭麸痖盹溴祗遽舶辈盹铘牾瞵疳珏备弓惫庚滹卑北肮陀耶舶辈恫泊哺褒轶箢脖栋备挡脲黠蜾箫骠麽蝈驷蹯麸戾蜥钽寤箫骠麽蝈礤趄殂蠡箫骠麽蝈聃犰轸惑翎糸篝殂犰犷犰箝蠡陪扉痼寤莎翎箅蠡惋殪灬崎蝈骘煌禊罨怩箝铄篌祜玳慊泔溴扉铄蠡泔溴聃犰轸讳彐邈舡痱镱孱弩蠡栝篝矧殂犰礤趄殂蠡箫骠麽蝈溴驽泗蠡箫骠麽蝈溴鲥祜痦孱艋箫骠麽蝈痱镪邈艋箫躜沐泔溴孱糸糸弩惑翎糸篝殂犰麸痖盹溴扉铉翦汨铋聃寤篝蝓泗躜犰礤趄殂蠡蔑蝌屐狒轱罨崎蝈蠡乳篝矧皇狯峄湾狍躜屙孱艋语骠麽蝈簌篝屙蠡泔溴聃犰轸惑镦赭狎泔钽弪铙霍镳殂盹溴扉铉篱铕蝻沐邃轭珞漆祆弪椴氨艾狨翳矧漆祆弪楝十耶犷弱汨狎洮彤犷提骘躜汜溴彤犷五怩衄卯犷序轭沐之犷尼铿彤怙镫糸綮膳排深舢蔑铈序镧虍蔑眇虍糸綮刘麸磲糸屮趄徙糸镱镦罪蜾五舡扉脲殇孱糸骈弪铄赭矧骝镯箫骠麽蝈遽舶卑盹铘牾瞵疳珏喘背瘐忪轶桢膳排徕篝蜥泗灬蜱疳螋镦翳糸礤犰祜汜翦麸箫骠麽蝈磲轭翦钺钽轶溴溟汜翦麸翳痱镧蜥泔眇蝈桢铙轱町歪铢狃痱镝汨弩翳狒躞弩翳痱镧蜥篝蝓泗躜矧翳屮翦蝾犰滹沲礤铘狒轱栳鲥忮孱泸遽翦麸狍箝篝痱镧蜥泔眇蝈桢铙轱町蕊麇鲥颥翳殇孱糸骈弪镦翳痱镧蜥狎犷轫痫螋犷箫躜沐镦轭骘蝽狒轱翳狒轶篝殪铒鏖溴禊躞邃骘翳轶瘐蝠矬瀹深翳轶狎糸沆瀣麇痱镳矬犷狃痱镝汨忉箦躔镱吾趱蜥提铉踽珏序镢弩箝铉翦汨铋聃弩翳狒狨麸磲糸汜祆屮趄徙趔犷矧玑铋弩泔钽屦趔骝镯箫骠麽蝈殇孱糸骈弪轭罪蜾五舡扉脲篝蝓泗躜翳狒麇汜祆戾殂犰鲩鬻螽澡弩戾殂犰鲩鬻玳鲥躞彐蹯轭箝玷镱犷秭弪犰箫骠麽蝈狎汨轸邈趱蝈犷汜忮躞邃麸轫痱秭蝈篚祠镦磲铢箫骠麽蝈孱玳铄弪轭翎箅螽澡痱镳矬犰轶弼犰踽翦徵衢铙泔蝠躞镦泊镳孱箫躜沐痱镧蜥眢荇屮翥镳蜷玷酏舶卑膳排滹卑北肮擅忻舶卑辈轶忸狗赴范沟幢背洱轶箢卑共副掣痦殇渤赋勾勾躜梏麴函殄邋痨矧瀹殄邋矧绡滹沲礤铘档脖犯朝棱镲臌驽狒躜暹孱玳铄弪轭绗狨翳矧缩桀歪犷岁屐飕曙桀箫铨瘐忪轶桢靡序弩簖糸綮棋狒躜蓬玳铄弪轭犷渝戾泗轱詈序徙糸汜琉痱镝汨骘序邃殂糸鲥惋溴祗遽舶惫篱铕蝻沐邃轭珞棋蝌犷翦惫复狨翳矧棋蝌犷翦叔犷铄犷萧翦铙翦轭酸蜢十犷揍蝌孱曙凝怙镫糸綮体泗物翦蔑眇豸鱼楫ㄩ钽祯溟铉吁怏弪体泗物翦硫糸娈深翦祆体泗物翦麻镩铈矧磲糸泱糸綮澡痱镧蜥溴疱钿孱沐珧狃犷轸躞轭镳糸黹狒轱铨遽惫复疳珏辈淡背昌鲲祯礤倍涛糜徕篝蜥泗深翳轶疳疱麇痱弩孱犷轭翦蝽邃獒翦痱镧蜥蝈痱弩孱翎糸镱汜祆邃翳痱镧蜥溴疱钿孱沐珧狃茺ㄐ那荦翳狒磲脲屮痨殂轸怙翳翳溽翎犷泔铘蝻溴疱钿孱沐骘遽汨镳弪狒轱轭痱镧蜥懋尼翎溴疱钿孱沐栳鲥忮孱躞邃麸蝈痱弩孱镱禊翳蝈戾鲠铘溽翎骒秣蝈灬糸镱箬轲镦痱镧蜥懋蔑铘蝻溴疱钿孱沐狎轭趄镤蹉邃麸犷犰镧秕箪蝈痱弩孱镱禊翳弩箦铘獒泔铘蝻骒秣蝈灬糸镱箬轲镦痱镧蜥懋蔑铘蝻溴疱钿孱沐狎溴蜷鲥骝镯翳躞踽泔铘蝻骒秣珧狃璁歪铢趄徜轸轱钺镳糸黹狒轱铙镳弪狒盹蝈彐骈汩孱綮镱翳茺心钱荦娱钽溴疱钿孱沐轭翳茺心躯荦泔铑邈泔眇豸狒轱钺祆蝈灬翦疳螋镦翳痱镧蜥憩箝铉戾麽祀镦翳弩溴疱钿孱沐轶篚骀殂殄铘麸疱蜴矧磲铢镳糸黹狒轱铙澡茺心躯荦犰祜黧趄犷箧矧磲糸镱篚汨狍鲥泗矧辁狒轱瞵翳狒痱弼轱躞禊蝈聃轵邃箴邈獒趄遽繇孱镦泔铘蝻溴疱钿孱沐麸忮疱蜴矧礤轭磲铑弪翳狒轶躅殒矧骘怙翳泔铘蝻犷溽翎溴疱钿孱沐螽序镧蜥趄犷箧矧磲糸镱翳狒蝈聃轵轭翦蜥泗轱镦翳赭溴疱钿孱沐豉疱汜犰箫忮遽箝禊栳钿戾鏖翳秕蝈痱弩孱翎糸镱馏犷屮犴痨瀣犷轭泸屙孱翎狃痱镝汨麸盹溟纟轭溽翎溴疱钿孱沐蝈篚祠轭骝镯怛犷汨溴戾糸镱矧祜镳躅蝻祆轭轶轭趄镤蹉邃澡茺心躯荦篚痧矧趔轭泸屙孱翎镳糸黹狒轱瞵疱蝽轸糸铉趄犷箧矧磲糸镱麸忮趄殓珏蝈怡镱犷雉桢犷狃痨殄镱禊麸徭驽泗邃溴疱钿孱沐螽滹卑卑胺抄荡碍辈共淡边吵轶忸狗赋荡氨补驳俘轶箢倍北吵垂脲黠蜾某坌蝻珧犴黹铉提铉踽珏筝序镢弩箫蝮泔眇殪弪蟋提铉踽珏蟋绣蜴矧磲钽龄溟糸镱犰隋罪蜾犷需蜥箦蠛尼翎怛犷汨溴戾糸镱泔溴盹糸镱溴怩珑轭绗溴疱钿孱沐犷犰箝蟋轭泸屙孱翎溽翎骒秣犷犰箝蟋轭翦蝽邃獒翦痱镧蜥蝈痱弩孱翎糸镱轭翦蝾犰痱镧蜥骘蝽祜镳骢箝镱祜镳躅蝻祆轭绗铒溴箴扉趑轭绗镳糸黹狒轱清铄蜥藻蝽蠛领顼蜷翳眢疳蜥祆屐轶憩箪殂轭绗鲥泗矧辁狒轱铨礤钿屐妁珧秕痼疹翎铉戾躜梏麴蠛鼢鳟泱豸屮狍邃醑pingali/CS395T/2009fa/papers/ferrante87.pdf}
}

@article{FixingCommits,
  author        = {Tufano, Michele and Bavota, Gabriele and Poshyvanyk, Denys and {Di Penta}, Massimiliano and Oliveto, Rocco and {De Lucia}, Andrea},
  journal       = {Journal of Software: Evolution and Process},
  title         = {{An empirical study on developer-related factors characterizing fix-inducing commits}},
  year          = {2016},
  issn          = {2047-7473},
  month         = aug,
  number        = {12},
  pages         = {1172--1192},
  volume        = {26},
  abstract      = {Background: software engineering research (SE) lacks theory and methodologies for addressing human aspects in software development. Development tasks are undertaken through cognitive processing activities. Affects (emotions, moods, feelings) have a linkage to cognitive processing activities and the productivity of individuals. SE research needs to incorporate affect measurements to valorize human factors and to enhance management styles. Objective: analyze the affects dimensions of valence, arousal, and dominance of software developers and their real-time correlation with their self-assessed productivity (sPR). Method: repeated measurements design with 8 participants (4 students, 4 professionals), conveniently sampled and studied individually over 90 minutes of programming. The analysis was performed by fitting a linear mixed- effects (LME) model. Results: valence and dominance are positively correlated with the sPR. The model was able to express about 38{\%} of deviance from the sPR. Many lessons were learned when employing psychological measurements in SE and for fitting LME. Conclusion: this article demonstrates the value of applying psychological tests in SE and echoes a call to valorize the human, individualized aspects of software developers. It reports a body of knowledge about affects, their classification, their measurement, and the best practices to perform psychological measurements in SE with LME models.},
  archiveprefix = {arXiv},
  arxivid       = {1408.1293},
  doi           = {10.1002/smr.1797},
  eprint        = {1408.1293},
  isbn          = {9781450330565},
  keywords      = {Conceptual schema,Design pattern detection,Object-relational mapping,Reverse engineering},
  pmid          = {67195556}
}

@article{fonseca2015evaluating,
  author    = {Fonseca, Erick R and Rosa, Jo{\a}o Lu{\'\i}s G and Alu{\'\i}sio, Sandra Maria},
  journal   = {Journal of the Brazilian Computer Society},
  title     = {{Evaluating word embeddings and a revised corpus for part-of-speech tagging in Portuguese}},
  year      = {2015},
  number    = {1},
  pages     = {2},
  volume    = {21},
  doi       = {10.1186/s13173-014-0020-x},
  publisher = {SpringerOpen}
}

@article{FRLink,
  author   = {Sun, Yan and Wang, Qing and Yang, Ye},
  journal  = {Information and Software Technology},
  title    = {{FRLink: Improving the recovery of missing issue-commit links by revisiting file relevance}},
  year     = {2017},
  issn     = {0950-5849},
  pages    = {33--47},
  volume   = {84},
  abstract = {Context: Though linking issues and commits plays an important role in software verification and maintenance, such link information is not always explicitly provided during software development or maintenance activities. Current practices in recovering such links highly depend on tedious manual examination. To automatically recover missing links, several approaches have been proposed to compare issue reports with log messages and source code files in commits. However, none of such approaches looked at the role of non-source code complementary documents in commits; nor did they consider the distinct roles each piece of the source code played in the same commit. Objective: We propose to revisit the definition of relevant files contributing to missing link recovery. More specifically, our work extends existing approaches from two perspectives: (1) Inclusion extension: incorporating complementary documents (i.e., non-source documents) to learn from more relevant data; (2) Exclusion extension: analyzing and filtering out irrelevant source code files to reduce data noise. Method: We propose a File Relevance-based approach (FRLink), to implement the above two considerations. FRLink utilizes non-source documents in commits, since they typically clarify code changes details, with similar textual information from corresponding issues. Moreover, FRLink differentiates the roles of different source code files in a single commit and discards files containing no similar code terms as those in issues based on similarity analysis. Results: FRLink is evaluated on 6 projects and compared with RCLinker, which is the latest state-of-the-art approach in missing link recovery. The result shows that FRLink outperforms RCLinker in F-Measure by 40.75{\%} when achieving the highest recalls. Conclusion: FRLink can significantly improve the performance of missing link recovery compared with existing approaches. This indicates that in missing link recovery studies, sophisticated data selection and processing techniques necessitate more discussions due to the increasing variety and volume of information associated with issues and commits.},
  doi      = {10.1016/j.infsof.2016.11.010}
}

@inproceedings{gensim,
  author    = {Radim {\v R}eh{\r u}{\v r}ek and Petr Sojka},
  booktitle = {{Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks}},
  title     = {{Software Framework for Topic Modelling with Large Corpora}},
  year      = {2010},
  address   = {Valletta, Malta},
  month     = may,
  pages     = {45--50},
  publisher = {ELRA},
  day       = {22},
  language  = {English},
  url       = {http://is.muni.cz/publication/884893/en}
}

@misc{ghlinkdoc,
  title        = {{GitHub: Autolinked references and URLs}},
  author       = {{GitHub}},
  howpublished = {\url{https://help.github.com/articles/autolinked-references-and-urls/}},
  note         = {Accessed: 2017-08-20},
  year         = {2017}
}

@inproceedings{GHTorrent,
  author    = {Gousios, Georgios and Spinellis, D.},
  booktitle = {2012 9th IEEE Working Conference on Mining Software Repositories (MSR)},
  title     = {{GHTorrent: Github's data from a firehose}},
  year      = {2012},
  month     = jun,
  pages     = {12--21},
  publisher = {IEEE},
  doi       = {10.1109/MSR.2012.6224294},
  isbn      = {978-1-4673-1761-0}
}
@article{Goldwater2007,
  author  = {Goldwater, Sharon and Griffiths, Tom},
  journal = {Proc. 45th Annu. Meet. Assoc. Comput. Linguist.},
  pages   = {744--751},
  title   = {{A fully Bayesian approach to unsupervised part-of-speech tagging}},
  url     = {http://aclweb.org/anthology/P07-1094},
  year    = {2007}
}

@misc{GoogleGHTorrent,
  title        = {{Google Cloud Public Table of GitHub Projects}},
  howpublished = {\url{https://bigquery.cloud.google.com/dataset/ghtorrent-bq:ght}},
  author       = {Gousios, Georgios and Spinellis, D.},
  note         = {{Contents from 2.9M public, open source licensed repositories on GitHub; Accessed: 2017-08-10}},
  year         = {2017}
}
@article{Herzig2013,
  author          = {Herzig, Kim and Zeller, Andreas},
  journal         = {IEEE Int. Work. Conf. Min. Softw. Repos.},
  title           = {{The impact of tangled code changes}},
  year            = {2013},
  issn            = {2160-1852},
  pages           = {121--130},
  abstract        = {When interacting with version control systems, developers often commit unrelated or loosely related code changes in a single transaction. When analyzing the version history, such tangled changes will make all changes to all modules appear related, possibly compromising the resulting analyses through noise and bias. In an investigation of five open-source Java projects, we found up to 15{\%} of all bug fixes to consist of multiple tangled changes. Using a multi-predictor approach to untangle changes, we show that on average at least 16.6{\%} of all source files are incorrectly associated with bug reports. We recommend better change organization to limit the impact of tangled changes.},
  doi             = {10.1109/MSR.2013.6624018},
  isbn            = {9781467329361},
  keywords        = {Bias,Data quality,Mining software repositories,Noise,Tangled code changes},
  mendeley-groups = {Untangle}
}

@article{Herzig2016,
  author   = {Herzig, Kim and Just, Sascha and Zeller, Andreas},
  journal  = {Empir. Softw. Eng.},
  title    = {{The impact of tangled code changes on defect prediction models}},
  year     = {2016},
  issn     = {1573-7616},
  number   = {2},
  pages    = {303--336},
  volume   = {21},
  abstract = {When interacting with version control systems, developers often commit unrelated or loosely related code changes in a single transaction. When analyzing the version history, such tangled changes will make all changes to all modules appear related, possibly compromising the resulting analyses through noise and bias. In an investigation of five open-source Java projects, we found up to 15{\%} of all bug fixes to consist of multiple tangled changes. Using a multi-predictor approach to untangle changes, we show that on average at least 16.6{\%} of all source files are incorrectly associated with bug reports. We recommend better change organization to limit the impact of tangled changes.},
  doi      = {10.1007/s10664-015-9376-6},
  isbn     = {9781467329361},
  keywords = {Data noise,Defect prediction,Untangling}
}

@book{hogg_tanis_zimmerman_2020,
  place     = {Hoboken, NJ},
  title     = {Probability and statistical inference},
  publisher = {Pearson},
  author    = {Hogg, Robert V. and Tanis, Elliot A. and Zimmerman, Dale L.},
  year      = {2020}
}

@article{HOLLAND1983,
  author  = {Paul W. Holland and Kathryn Blackmond Laskey and Samuel Leinhardt},
  journal = {Social Networks},
  title   = {{Stochastic blockmodels: First steps}},
  year    = {1983},
  issn    = {0378-8733},
  number  = {2},
  pages   = {109--137},
  volume  = {5},
  doi     = {10.1016/0378-8733(83)90021-7},
  url     = {http://www.sciencedirect.com/science/article/pii/0378873383900217}
}

@inproceedings{Host2007,
  author    = {H{\o}st, Einar W. and {\O}stvold, Bjarte M.},
  booktitle = {SCAM 2007 - Proc. 7th IEEE Int. Work. Conf. Source Code Anal. Manip.},
  title     = {{The programmer's lexicon, volume I: The verbs}},
  year      = {2007},
  month     = sep,
  pages     = {193--202},
  publisher = {IEEE},
  abstract  = {Method names make or break abstractions: good ones communicate the intention of the method, whereas bad ones cause confusion and frustration. The task of naming is subject to the whims and idiosyncracies of the individual since programmers have little to guide them except their personal experience. By analysing method implementations taken from a corpus of Java applications, we establish the meaning of verbs in method names based on actual use. The result is an automatically generated, domain-neutral lexicon of verbs, similar to a natural language dictionary, that represents the common usages of many programmers},
  doi       = {10.1109/SCAM.2007.18},
  isbn      = {0769528805},
  url       = {http://ieeexplore.ieee.org/document/4362913/}
}

@article{HowManyRF,
  author  = {Mayumi Oshiro, Thais and Santoro Perez, Pedro and Baranauskas, Jos},
  journal = {Lecture notes in computer science},
  title   = {{How Many Trees in a Random Forest?}},
  year    = {2012},
  month   = jul,
  volume  = {7376},
  doi     = {10.1007/978-3-642-31537-4_13}
}
@article{InternetScaleMining,
  author   = {Linstead, Erik and Bajracharya, Sushil and Ngo, Trung and Rigor, Paul and Lopes, Cristina and Baldi, Pierre},
  journal  = {Data Mining and Knowledge Discovery},
  title    = {{Sourcerer: Mining and searching internet-scale software repositories}},
  year     = {2009},
  issn     = {1384-5810},
  number   = {2},
  pages    = {300--336},
  volume   = {18},
  abstract = {Large repositories of source code available over the Internet, or within large organizations, create new challenges and opportunities for data mining and statistical machine learning. Here we first develop Sourcerer, an infrastructure for the automated crawling, parsing, fingerprinting, and database storage of open source software on an Internet-scale. In one experiment, we gather 4,632 Java projects from SourceForge and Apache totaling over 38 million lines of code from 9,250 developers. Simple statistical analyses of the data first reveal robust power-law behavior for package, method call, and lexical containment distributions. We then develop and apply unsupervised, probabilistic, topic and author-topic (AT) models to automatically discover the topics embedded in the code and extract topic-word, document-topic, and AT distributions. In addition to serving as a convenient summary for program function and developer activities, these and other related distributions provide a statistical and information-theoretic basis for quantifying and analyzing source file similarity, developer similarity and competence, topic scattering, and document tangling, with direct applications to software engineering an software development staffing. Finally, by combining software textual content with structural information captured by our CodeRank approach, we are able to significantly improve software retrieval performance, increasing the area under the curve (AUC) retrieval metric to 0.92 roughly 1030{\%} better than previous approaches based on text alone. A prototype of the system is available at: http://sourcerer.ics.uci.edu .},
  doi      = {10.1007/s10618-008-0118-x},
  isbn     = {1384-5810},
  keywords = {Author-topic probabilistic modeling,Code retrieval,Code search,Mining software,Program understanding,Software analysis}
}
@misc{jiralinkdoc,
  title        = {{JIRA: Link JIRA issues to Confluence pages automatically}},
  author       = {{JIRA}},
  howpublished = {\url{https://www.atlassian.com/blog/confluence/link-jira-issues-to-confluence-pages-automatically}},
  note         = {Accessed: 2017-08-20},
  year         = {2017}
}

@inproceedings{Johnson:2013:WDS:2486788.2486877,
  author    = {Johnson, Brittany and Song, Yoonki and Murphy-Hill, Emerson and Bowdidge, Robert},
  booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
  title     = {{Why Don\&\#039;T Software Developers Use Static Analysis Tools to Find Bugs?}},
  year      = {2013},
  address   = {Piscataway, NJ, USA},
  pages     = {672--681},
  publisher = {IEEE Press},
  series    = {ICSE '13},
  acmid     = {2486877},
  isbn      = {978-1-4673-3076-3},
  location  = {San Francisco, CA, USA},
  numpages  = {10},
  url       = {http://dl.acm.org/citation.cfm?id=2486788.2486877}
}
@inproceedings{Kalliamvakou2014,
  abstract  = {We are now witnessing the rapid growth of decentralized source code management (DSCM) systems, in which every developer has her own repository. DSCMs facilitate a style of collaboration in which work output can flow sideways (and privately) between collaborators, rather than always up and down (and publicly) via a central repository. Decentralization comes with both the promise of new data and the peril of its misinterpretation. We focus on git, a very popular DSCM used in high-profile projects. Decentralization, and other features of git, such as automatically recorded contributor attribution, lead to richer content histories, giving rise to new questions such as ldquoHow do contributions flow between developers to the official project repository?rdquo However, there are pitfalls. Commits may be reordered, deleted, or edited as they move between repositories. The semantics of terms common to SCMs and DSCMs sometimes differ markedly, potentially creating confusion. For example, a commit is immediately visible to all developers in centralized SCMs, but not in DSCMs. Our goal is to help researchers interested in DSCMs avoid these and other perils when mining and analyzing git data.},
  address   = {New York, New York, USA},
  author    = {Kalliamvakou, Eirini and Gousios, Georgios and Blincoe, Kelly and Singer, Leif and German, Daniel M. and Damian, Daniela},
  booktitle = {Proc. 11th Work. Conf. Min. Softw. Repos. - MSR 2014},
  doi       = {10.1145/2597073.2597074},
  isbn      = {9781450328630},
  keywords  = {Mining software repositories,bias,code reviews,git,github},
  pages     = {92--101},
  publisher = {ACM Press},
  title     = {{The promises and perils of mining GitHub}},
  url       = {http://dl.acm.org/citation.cfm?doid=2597073.2597074},
  year      = {2014}
}

@inproceedings{Kalliamvakou2015,
  author          = {Kalliamvakou, Eirini and Damian, Daniela and Blincoe, Kelly and Singer, Leif and German, Daniel M},
  booktitle       = {Proc. - Int. Conf. Softw. Eng.},
  title           = {{Open source-style collaborative development practices in commercial projects using GitHub}},
  year            = {2015},
  pages           = {574--585},
  volume          = {1},
  abstract        = {Researchers are currently drawn to study projects hosted on GitHub due to its popularity, ease of obtaining data, and its distinctive built-in social features. GitHub has been found to create a transparent development environment, which together with a pull request-based workflow, provides a lightweight mech- anism for committing, reviewing and managing code changes. These features impact how GitHub is used and the benefits it provides to teams' development and collaboration. While most of the evidence we have is from GitHub's use in open source software (OSS) projects, GitHub is also used in an increasing number of commercial projects. It is unknown how GitHub supports these projects given that GitHub's workflow model does not intuitively fit the commercial development way of working. In this paper, we report findings from an online survey and interviews with GitHub users on how GitHub is used for collaboration in commercial projects. We found that many commercial projects adopted practices that are more typical of OSS projects including reduced communication, more independent work, and self-organization. We discuss how GitHub's transparency and popular workflow can promote open collaboration, allowing organizations to increase code reuse and promote knowledge sharing across their teams.},
  doi             = {10.1109/ICSE.2015.74},
  isbn            = {9781479919345},
  issn            = {0270-5257},
  keywords        = {GitHub,collaboration,commercial projects,coordination,open source,practices,pull requests,workflow},
  mendeley-groups = {Aide-m{\'{e}}moire}
}

@article{Kawrykow2011,
  abstract = {Numerous techniques involve mining change data captured in software archives to assist engineering efforts, for example to identify components that tend to evolve together. We observed that important changes to software artifacts are sometimes accompanied by numerous non-essential modifications, such as local variable refactorings, or textual differences induced as part of a rename refactoring. We developed a tool-supported technique for detecting non-essential code differences in the revision histories of software systems. We used our technique to investigate code changes in over 24,000 change sets gathered from the change histories of seven long-lived open-source systems. We found that up to 15.5{\&}{\#}x025; of a system's method updates were due solely to non-essential differences. We also report on numerous observations on the distribution of non-essential differences in change history and their potential impact on change-based analyses.},
  author   = {Kawrykow, David and Robillard, Martin P.},
  doi      = {10.1145/1985793.1985842},
  isbn     = {9781450304450},
  issn     = {0270-5257},
  journal  = {Proceeding 33rd Int. Conf. Softw. Eng. - ICSE '11},
  keywords = {differenc-,mining software repositories,software change analysis},
  pages    = {351},
  title    = {{Non-essential changes in version histories}},
  url      = {http://portal.acm.org/citation.cfm?doid=1985793.1985842},
  year     = {2011}
}
@inproceedings{kim2006program,
  author    = {Kim, Miryung and Notkin, David},
  booktitle = {Proceedings of the 2006 international workshop on Mining software repositories},
  title     = {Program element matching for multi-version program analyses},
  year      = {2006},
  pages     = {58--64},
  doi       = {10.1145/1137983.1137999}
}

@inproceedings{Kirinuki2014,
  abstract        = {Although there is a principle that states a commit should only include changes for a single task, it is not always respected by developers. This means that code repositories often include commits that contain tangled changes. The presence of such tangled changes hinders analyzing code repositories because most mining software repository (MSR) approaches are designed with the assumption that every commit includes only changes for a single task. In this paper, we propose a technique to inform developers that they are in the process of committing tangled changes. The proposed technique utilizes the changes included in the past commits to judge whether a given commit includes tangled changes. If it determines that the proposed commit may include tangled changes, it offers suggestions on how the tangled changes can be split into a set of untangled changes.},
  address         = {New York, New York, USA},
  author          = {Kirinuki, Hiroyuki and Higo, Yoshiki and Hotta, Keisuke and Kusumoto, Shinji},
  booktitle       = {Proc. 22nd Int. Conf. Progr. Compr. (ICPC 2014)},
  doi             = {10.1145/2597008.2597798},
  isbn            = {9781450328791},
  keywords        = {mining software repositories,tan-,understanding commits},
  mendeley-groups = {Untangle},
  pages           = {262--265},
  publisher       = {ACM Press},
  title           = {{Hey! are you committing tangled changes?}},
  url             = {http://dl.acm.org/citation.cfm?doid=2597008.2597798},
  year            = {2014}
}

@article{Kirinuki2017,
  author   = {Kirinuki, Hiroyuki and Higo, Yoshiki and Hotta, Keisuke and Kusumoto, Shinji},
  journal  = {Proc. - Asia-Pacific Softw. Eng. Conf. APSEC},
  title    = {{Splitting commits via past code changes}},
  year     = {2017},
  issn     = {1530-1362},
  pages    = {129--136},
  abstract = {{\textcopyright} 2016 IEEE. It is generally said that we should not perform code changes formultiple tasks in a single commit. Such code changes are called tangledones. Committing tangled changes is harmful to developers. Forexample, it is costly to merge a part of tangled changes with othercommits. Moreover, the presence of such tangled changes hindersanalyzing code repositories. That is because most of the miningsoftware repository approaches are designed under the assumption thatevery commit includes only changes for a single task. In this paper, wepropose a technique which informs developers that they are about tocommit tangled changes. The technique also suggests how to split agiven commit into multiple commits by using past code changes. Theproposed technique allows developers to determine whether they acceptthe suggestion or commit as it stands. By providing such support todevelopers, they can avoid committing tangled changes.},
  doi      = {10.1109/APSEC.2016.028},
  isbn     = {9781509055753},
  keywords = {Change pattern,Repository mining,Source code analysis}
}


@inproceedings{lakshminarayanan2014mondrian,
  author    = {Lakshminarayanan, Balaji and Roy, Daniel M and Teh, Yee Whye},
  booktitle = {Advances in neural information processing systems},
  title     = {{Mondrian forests: Efficient online random forests}},
  year      = {2014},
  pages     = {3140--3148}
}

@inproceedings{Lapata2001,
  author    = {Lapata, Maria},
  booktitle = {Proc. Second Meet. North Am. Chapter Assoc. Comput. Linguist. Lang. Technol.},
  title     = {{A corpus-based account of regular polysemy: the case of context-sensitive adjectives}},
  year      = {2001},
  pages     = {1--8},
  abstract  = {In this paper we investigate polysemous adjectives whose meaning varies depending on the nouns they modify (e.g., fast). We acquire the meanings of these adjectives from a large corpus and propose a probabilistic model which provides a ranking on the set of possible interpretations. We identify lexical semantic information automatically by exploiting the consistent correspondences between surface syntactic cues and lexical meaning. We evaluate our results against paraphrase judgments elicited experimentally from humans and show that the model's ranking of meanings correlates reliably with human intuitions: meanings that are found highly probable by the model are also rated as plausible by the subjects.},
  doi       = {10.3115/1073336.1073345}
}

@inproceedings{Lawrie2011,
  abstract  = {Maintaining modern software requires significant tool support. Effective tools exploit a variety of information and techniques to aid a software maintainer. One area of recent interest in tool development exploits the natural language information found in source code. Such Information Retrieval (IR) based tools compliment traditional static analysis tools and have tackled problems, such as feature location, that otherwise require considerable human effort. To reap the full benefit of IR-based techniques, the language used across all software artifacts (e.g., requirements, design, change requests, tests, and source code) must be consistent. Unfortunately, there is a significant proportion of invented vocabulary in source code. Vocabulary normalization aligns the vocabulary found in the source code with that found in other software artifacts. Most existing work related to normalization has focused on splitting an identifier into its constituent parts. The next step is to expand each part into a (dictionary) word that matches the vocabulary used in other software artifacts. Building on a successful approach to splitting identifiers, an implementation of an expansion algorithm is presented. Experiments on two systems find that up to 66{\%} of identifiers are correctly expanded, which is within about 20{\%} of the current system's best-case performance. Not only is this performance comparable to previous techniques, but the result is achieved in the absence of special purpose rules and not limited to restricted syntactic contexts. Results from these experiments also show the impact that varying levels of documentation (including both internal documentation such as the requirements and design, and external, or user-level, documentation) have on the algorithm's performance.},
  author    = {Lawrie, Dawn and Binkley, Dave},
  booktitle = {IEEE Int. Conf. Softw. Maintenance, ICSM},
  doi       = {10.1109/ICSM.2011.6080778},
  isbn      = {9781457706646},
  issn      = {1063-6773},
  keywords  = {natural language processing,program comprehension,source code analysis tools},
  pages     = {113--122},
  title     = {{Expanding identifiers to normalize source code vocabulary}},
  year      = {2011}
}

@article{lda,
  author        = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
  journal       = {Journal of Machine Learning Research},
  title         = {{Latent Dirichlet Allocation}},
  year          = {2003},
  issn          = {0035-8711},
  month         = sep,
  abstract      = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
  archiveprefix = {arXiv},
  arxivid       = {1111.6189},
  doi           = {10.1111/j.1365-2966.2012.21196.x},
  eprint        = {1111.6189},
  isbn          = {9781577352815},
  pmid          = {21362469}
}

@inproceedings{LDAAsClustering,
  author    = {Gorla, Alessandra and Tavecchia, Ilaria and Gross, Florian and Zeller, Andreas},
  booktitle = {Proceedings of the 36th International Conference on Software Engineering},
  title     = {{Checking App Behavior Against App Descriptions}},
  year      = {2014},
  address   = {New York, NY, USA},
  pages     = {1025--1035},
  publisher = {ACM},
  series    = {ICSE 2014},
  acmid     = {2568276},
  doi       = {10.1145/2568225.2568276},
  isbn      = {978-1-4503-2756-5},
  keywords  = {Android, clustering, description analysis, malware detection},
  location  = {Hyderabad, India},
  numpages  = {11}
}

@inproceedings{LDASourceRetrival,
  author    = {S. K. Lukins and N. A. Kraft and L. H. Etzkorn},
  booktitle = {2008 15th Working Conference on Reverse Engineering},
  title     = {{Source Code Retrieval for Bug Localization Using Latent Dirichlet Allocation}},
  year      = {2008},
  month     = oct,
  pages     = {155--164},
  doi       = {10.1109/WCRE.2008.33},
  issn      = {1095-1350},
  keywords  = {indexing;information retrieval;software engineering;LDA-based static technique;automatic bug localization;automating bug localization;information retrieval;latent Dirichlet allocation;latent semantic indexing;source code retrieval;Aging;Computer bugs;Costs;Indexing;Information retrieval;Large scale integration;Linear discriminant analysis;Reverse engineering;Software maintenance;Software systems;LDA;LSI;bug localization;information retrieval;latent Dirichlet allocation;latent semantic indexing;program comprehension}
}
@article{LDATime,
  author     = {Thomas, Stephen W. and Adams, Bram and Hassan, Ahmed E. and Blostein, Dorothea},
  journal    = {Sci. Comput. Program.},
  title      = {{Studying Software Evolution Using Topic Models}},
  year       = {2014},
  issn       = {0167-6423},
  month      = feb,
  pages      = {457--479},
  volume     = {80},
  acmid      = {2564429},
  address    = {Amsterdam, The Netherlands, The Netherlands},
  doi        = {10.1016/j.scico.2012.08.003},
  issue_date = {February, 2014},
  keywords   = {Latent Dirichlet allocation, Mining software repositories, Software evolution, Topic model},
  numpages   = {23},
  publisher  = {Elsevier North-Holland, Inc.}
}
@article{Le2014,
  author   = {Le, Wei and Pattison, Shannon D.},
  journal  = {Proc. 36th Int. Conf. Softw. Eng. - ICSE 2014},
  title    = {{Patch verification via multiversion interprocedural control flow graphs}},
  year     = {2014},
  issn     = {0270-5257},
  pages    = {1047--1058},
  abstract = {Software development is inherently incremental; however, it is challenging to correctly introduce changes on top of existing code. Recent studies show that 15{\%}-24{\%} of the bug fixes are incorrect, and the most important yet hard-to-acquire information for programming changes is whether this change breaks any code elsewhere. This paper presents a framework, called Hydrogen, for patch verification. Hydrogen aims to automatically determine whether a patch correctly fixes a bug, a new bug is introduced in the change, a bug can impact multiple software releases, and the patch is applicable for all the impacted releases. Hydrogen consists of a novel program representation, namely multiversion interprocedural control flow graph (MVICFG), that integrates and compares control flow of multiple versions of programs, and a demand-driven, path-sensitive symbolic analysis that traverses the MVICFG for detecting bugs related to software changes and versions. In this paper, we present the definition, construction and applications of MVICFGs. Our experimental results show that Hydrogen correctly builds desired MVICFGs and is scalable to real-life programs such as libpng, tightvnc and putty. We experimentally demonstrate that MVICFGs can enable efficient patch verification. Using the results generated by Hydrogen, we have found a few documentation errors related to patches for a set of open-source programs.},
  doi      = {10.1145/2568225.2568304},
  isbn     = {9781450327565},
  keywords = {multiversion,patch verification,software changes},
  url      = {http://dl.acm.org/citation.cfm?doid=2568225.2568304}
}

@article{Li2016,
  abstract        = {Graph-structured data appears frequently in domains including chemistry, natural language semantics, social networks, and knowledge bases. In this work, we study feature learning techniques for graph-structured inputs. Our starting point is previous work on Graph Neural Networks (Scarselli et al., 2009), which we modify to use gated recurrent units and modern optimization techniques and then extend to output sequences. The result is a flexible and broadly useful class of neural network models that has favorable inductive biases relative to purely sequence-based models (e.g., LSTMs) when the problem is graph-structured. We demonstrate the capabilities on some simple AI (bAbI) and graph algorithm learning tasks. We then show it achieves state-of-the-art performance on a problem from program verification, in which subgraphs need to be described as abstract data structures.},
  archiveprefix   = {arXiv},
  arxivid         = {1511.05493},
  author          = {Li, Yujia and Zemel, Richard and Brockschmidt, Marc and Tarlow, Daniel},
  eprint          = {1511.05493},
  journal         = {4th Int. Conf. Learn. Represent. ICLR 2016 - Conf. Track Proc.},
  mendeley-groups = {Untangle},
  number          = {1},
  pages           = {1--20},
  title           = {{Gated graph sequence neural networks}},
  year            = {2016}
}

@Article{li2019graph,
  author  = {Li, Yujia and Gu, Chenjie and Dullien, Thomas and Vinyals, Oriol and Kohli, Pushmeet},
  journal = {arXiv preprint arXiv:1904.12787},
  title   = {Graph matching networks for learning the similarity of graph structured objects},
  year    = {2019},
  eprint  = {1904.12787v2},
}

@article{Linstead2007,
  abstract = {We develop and apply statistical topic models to software as a means of extracting concepts from source code. The effec- tiveness of the technique is demonstrated on 1,555 projects from SourceForge and Apache consisting of 113,000 files and 19 million lines of code. In addition to providing an auto- mated, unsupervised, solution to the problem of summariz- ing program functionality, the approach provides a proba- bilistic framework with which to analyze and visualize source file similarity. Finally, we introduce an information-theoretic approach for computing tangling and scattering of extracted concepts, and present preliminary results.},
  author   = {Linstead, Erik and Rigor, Paul and Bajracharya, Sushil and Lopes, Cristina and Baldi, Pierre},
  doi      = {10.1145/1321631.1321709},
  isbn     = {9781595938824},
  journal  = {Proc. ASE},
  keywords = {mining software,program understanding,topic models},
  number   = {April 2016},
  pages    = {461},
  title    = {{Mining concepts from code with probabilistic topic models}},
  year     = {2007}
}

@misc{linuxKernelCommit,
  title        = {{Linux Kernel Commit Message Practice}},
  author       = {{Linux Kernel}},
  howpublished = {\url{https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/Documentation/process/submitting-patches.rst?id=bc7938deaca7f474918c41a0372a410049bd4e13\#n664}},
  note         = {Accessed: 2020-06-19},
  year         = {2020}
}

@article{liu2019automatic,
  author  = {Liu, Zhongxin and Xia, Xin and Treude, Christoph and Lo, David and Li, Shanping},
  journal = {arXiv preprint arXiv:1909.06987},
  title   = {{Automatic Generation of Pull Request Descriptions}},
  year    = {2019}
}

@inproceedings{LSIAsClustering,
  author    = {J. I. Maletic and N. Valluri},
  booktitle = {14th IEEE International Conference on Automated Software Engineering},
  title     = {{Automatic software clustering via Latent Semantic Analysis}},
  year      = {1999},
  month     = oct,
  pages     = {251--254},
  doi       = {10.1109/ASE.1999.802296},
  keywords  = {automatic programming;computational linguistics;natural languages;software reusability;software tools;statistical analysis;LEDA library;LSA;Latent Semantic Analysis;MINIX operating system;automatic software clustering;corpus based statistical method;documentation;internal documentation;natural language;normal application domain;program source code;semantic meaning;software components;software reuse;Application software;Computer science;Documentation;Identity-based encryption;Matrix decomposition;Natural languages;Operating systems;Read only memory;Sparse matrices;Statistical analysis}
}

@article{LSIAsClustering2,
  author   = {Adrian Kuhn and Stphane Ducasse and Tudor Grba},
  journal  = {Information and Software Technology},
  title    = {{Semantic clustering: Identifying topics in source code}},
  year     = {2007},
  issn     = {0950-5849},
  note     = {12th Working Conference on Reverse Engineering},
  number   = {3},
  pages    = {230--243},
  volume   = {49},
  doi      = {10.1016/j.infsof.2006.10.017},
  keywords = {Reverse engineering, Clustering, Latent Semantic Indexing, Visualization}
}

@inproceedings{LSIAsClustering3,
  author    = {A. Kuhn and S. Ducasse and T. Girba},
  booktitle = {12th Working Conference on Reverse Engineering (WCRE'05)},
  title     = {Enriching reverse engineering with semantic clustering},
  year      = {2005},
  month     = nov,
  doi       = {10.1109/WCRE.2005.16},
  issn      = {1095-1350},
  keywords  = {formal specification;indexing;information retrieval;program diagnostics;programming language semantics;reverse engineering;structured programming;artifacts clustering;information retrieval;latent semantic indexing;reverse engineering;semantic clustering;software system;source code semantic;system structure;Computational modeling;Computer simulation;Indexing;Information analysis;Information retrieval;Large scale integration;Reverse engineering;Software systems;Vocabulary;Web server;clustering;concept location;reverse engineering;semantic analysis}
}

@inproceedings{LSIForTestPrioritization,
  author    = {M. M. Islam and A. Marchetto and A. Susi and G. Scanniello},
  booktitle = {2012 16th European Conference on Software Maintenance and Reengineering},
  title     = {{A Multi-Objective Technique to Prioritize Test Cases Based on Latent Semantic Indexing}},
  year      = {2012},
  month     = mar,
  pages     = {21--30},
  doi       = {10.1109/CSMR.2012.13},
  issn      = {1534-5351},
  keywords  = {Java;program testing;software fault tolerance;Java applications;fault discovery;latent semantic indexing;multiobjective test prioritization technique;random prioritization technique;single objective function;single-objective prioritization techniques;source code;system requirements;test case prioritization;traceability links;Business;Indexing;Large scale integration;Semantics;Software;Testing;Weight measurement;Regression Testing;Requirements;Test Case Prioritization;Testing;Traceability}
}
@article{Maalej2010,
  author   = {Maalej, Walid and Happel, Hans-J{\"{o}}rg},
  journal  = {7th IEEE Working Conference on Mining Software Repositories (MSR 2010)},
  title    = {{Can Development Work Describe Itself?}},
  year     = {2010},
  issn     = {0270-5257},
  pages    = {191--200},
  abstract = {Work descriptions are informal notes taken by developers to summarize work achieved in a particular session. Existing studies indicate that maintaining them is a distracting task, which costs a developer more than 30 min. a day. The goal of this research is to analyze the purposes of work descriptions, and find out if automated tools can assist developers in efficiently creating them. For this, we mine a large dataset of heterogeneous work descriptions from open source and commercial projects. We analyze the semantics of these documents and identify common information entities and granularity levels. Information on performed actions, concerned artifacts, references and new work, shows the work management purpose of work descriptions. Information on problems, rationale and experience shows their knowledge sharing purpose. We discuss how work description information, in particular information used for work management, can be generated by observing developers' interactions. Our findings have many implications for next generation software engineering tools.},
  doi      = {10.1109/MSR.2010.5463344},
  isbn     = {978-1-4244-6802-7},
  keywords = {Collaborative work,Costs,Information analysis,Planing,Predictive models,Software engineering,Vocabulary,automated tools,development work,heterogeneous work descriptions,knowledge management,knowledge sharing,software development management,software engineering tools,work management purpose}
}
@inproceedings{Madani2011,
  author    = {Madani, Nioosha and Guerrouj, Latifa and {Di Penta}, Massimiliano and Gu{\'{e}}h{\'{e}}neuc, Yann Ga{\"{e}}l and Antoniol, Giuliano},
  booktitle = {Proc. Eur. Conf. Softw. Maint. Reengineering, CSMR},
  title     = {{Recognizing words from source code identifiers using speech recognition techniques}},
  year      = {2011},
  pages     = {68--77},
  abstract  = {The existing software engineering literature has empirically shown that a proper choice of identifiers influences software understandability and maintainability. Researchers have noticed that identifiers are one of the most important source of information about program entities and that the semantic of identifiers guide the cognitive process. Recognizing the words forming identifiers is not an easy task when naming conventions (e.g., Camel Case) are not used or strictly followed and{\&}{\#}x02013;or when these words have been abbreviated or otherwise transformed. This paper proposes a technique inspired from speech recognition, i.e., dynamic time warping, to split identifiers into component words. The proposed technique has been applied to identifiers extracted from two different applications: JHotDraw and Lynx. Results compared to manually-built oracles and with Camel Case algorithm are encouraging. In fact, they show that the technique successfully recognizes words composing identifiers (even when abbreviated) in about 90{\&}{\#}x025; of cases and that it performs better than Camel Case. Furthermore, it was able to spot mistakes in the manually-built oracle.},
  doi       = {10.1109/CSMR.2010.31},
  isbn      = {9780769543215},
  issn      = {1534-5351},
  keywords  = {Program comprehension,Source code identifiers}
}
@book{Manning:2008:IIR:1394399,
  author    = {Manning, Christopher D. and Raghavan, Prabhakar and Sch\"{u}tze, Hinrich},
  publisher = {Cambridge University Press},
  title     = {{Introduction to Information Retrieval}},
  year      = {2008},
  address   = {New York, NY, USA},
  isbn      = {0521865719, 9780521865715},
  chapter   = {8}
}
@article{marcus1993building,
  author = {Marcus, Mitchell and Santorini, Beatrice and Marcinkiewicz, Mary Ann},
  title  = {{Building a large annotated corpus of English: The Penn Treebank}},
  year   = {1993}
}

@article{Maskeri2008,
  abstract = {One of the difficulties in maintaining a large software system is the absence of documented business domain topics and correlation between these domain topics and source code. Without such a correlation, people without any prior appli- cation knowledge would find it hard to comprehend the func- tionality of the system. Latent Dirichlet Allocation (LDA), a statistical model, has emerged as a popular technique for discovering topics in large text document corpus. But its ap- plicability in extracting business domain topics from source code has not been explored so far. This paper investigates LDA in the context of comprehending large software systems and proposes a human assisted approach based on LDA for extracting domain topics from source code. This method has been applied on a number of open source and propri- etary systems. Preliminary results indicate that LDA is able to identify some of the domain topics and is a satisfactory starting point for further manual refinement of topics},
  author   = {Maskeri, Girish and Sarkar, Santonu and Heafield, Kenneth},
  doi      = {10.1145/1342211.1342234},
  isbn     = {9781595939173},
  journal  = {Proceedings of the 1st conference on India software engineering conference - ISEC '08},
  keywords = {LDA,maintenance,program comprehension},
  pages    = {113},
  title    = {{Mining business topics in source code using latent dirichlet allocation}},
  year     = {2008}
}
@article{Mi2016,
  abstract = {Background: Bug fixing is a long-term and time-consuming activity. A software bug experiences a typical life cycle from newly reported to finally closed by developers, but it could be reopened afterwards for further actions due to reasons such as unclear description given by the bug reporter and developer negligence. Bug reopening is neither desirable nor could be completely avoided in practice, and it is more likely to bring unnecessary workloads to already-busy developers. Aims: To the best of our knowledge, there has been a little previous work on software bug reopening. In order to fur-ther study in this area, we perform an empirical analysis to provide a comprehensive understanding of this special area. Method: Based on four open source projects from Eclipse product family, they are CDT, JDT, PDE and Platform, we first quantitatively analyze reopened bugs from perspec-tives of proportion, impacts and time distribution. After initial exploration on their characteristics, we then quali-tatively summarize root causes for bug reopening, this is carried out by investigating developer discussions recorded in Eclipse Bugzilla. Results: Results show that 6{\%}-10{\%} of total bugs will lead to reopening eventually. Over 93{\%} of reopened bugs place serious influence on the normal opera-tion of the system being developed. Several key reasons for bug reopening have been identified in our empirical study. Conclusions: Although reopened bugs have significant im-pacts on both end users and developers, it is quite possible to reduce bug reopening rate through the adoption of ap-propriate methods, such as promoting effective and efficient communication among bug reporters and developers, which is supported by empirical evidence in this study.},
  author   = {Mi, Qing and Keung, Jacky},
  doi      = {10.1145/2915970.2915986},
  isbn     = {9781450336918},
  journal  = {Proceedings of the 20th International Conference on Evaluation and Assessment in Software Engineering - EASE '16},
  keywords = {bug reports,bug tracking system,empirical software engineering,open source,projects,reopened bugs},
  pages    = {1--10},
  title    = {{An empirical analysis of reopened bugs based on open source projects}},
  year     = {2016}
}

@InProceedings{mikolov2013distributed,
  author    = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  booktitle = {Advances in neural information processing systems},
  title     = {Distributed representations of words and phrases and their compositionality},
  year      = {2013},
  pages     = {3111--3119},
  eprint    = {1310.4546v1},
}

@Article{mikolov2013efficient,
  author  = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  journal = {arXiv preprint arXiv:1301.3781},
  title   = {Efficient estimation of word representations in vector space},
  year    = {2013},
  eprint  = {1301.3781v3},
}

@article{MissingLinks,
  abstract = {Empirical studies of software defects rely on links between bug databases and program code repositories. This linkage is typically based on bug-fixes identified in developer-entered commit logs. Unfortunately, developers do not always report which commits perform bug-fixes. Prior work suggests that such links can be a biased sample of the entire population of fixed bugs. The validity of statistical hypotheses-testing based on linked data could well be affected by bias. Given the wide use of linked defect data, it is vital to gauge the nature and extent of the bias, and try to develop testable theories andmodels of the bias. To do this, we must establish ground truth: manually analyze a complete version history corpus, and nail down those commits that fix defects, and those that do not. This is a difficult task, requiring an ex- pert to compare versions, analyze changes, find related bugs in the bug database, reverse-engineer missing links, and finally record their work for use later. This effort must be repeated for hundreds of commits to obtain a useful sam- ple of reported and unreported bug-fix commits. We make several contributions. First, we present Linkster, a tool to facilitate link reverse-engineering. Second, we evaluate this tool, engaging a core developer of the Apache HTTP web server project to exhaustively annotate 493 commits that occurred during a six week period. Finally, we analyze this comprehensive data set, showing that there are serious and consequential problems in the data.},
  author   = {Bachmann, Adrian and Bird, Christian and Rahman, Foyzur and Devanbu, Premkumar and Bernstein, Abraham},
  doi      = {10.1145/1882291.1882308},
  isbn     = {9781605587912},
  journal  = {Proceedings of the eighteenth ACM SIGSOFT international symposium on Foundations of software engineering - FSE '10},
  keywords = {apache,are widely used in,bias,case study,commit,especially bug reports and,logs,manual annotation,software engineering research,software process data,the,tool},
  pages    = {97},
  title    = {{The missing links}},
  year     = {2010}
}
@article{MLink,
  abstract  = {The links between the bug reports in an issue-tracking system and the corresponding fixing changes in a version repository are not often recorded by developers. Such linking information is crucial for research in mining software repositories in measuring software defects and maintenance efforts. However, the state-of-the-art bug-to-fix link recovery approaches still rely much on textual matching between bug reports and commit/change logs and cannot handle well the cases where their contents are not textually similar.$\backslash$n$\backslash$nThis paper introduces MLink, a multi-layered approach that takes into account not only textual features but also source code features of the changed code corresponding to the commit logs. It is also capable of learning the association relations between the terms in bug reports and the names of entities/components in the changed source code of the commits from the established bug-to-fix links, and uses them for link recovery between the reports and commits that do not share much similar texts. Our empirical evaluation on real-world projects shows that MLink can improve the state-of-the-art bug-to-fix link recovery methods by 11--18{\%}, 13--17{\%}, and 8--17{\%} in F-score, recall, and precision, respectively.},
  address   = {New York, New York, USA},
  author    = {Nguyen, Anh Tuan and Nguyen, Tung Thanh and Nguyen, Hoan Anh and Nguyen, Tien N.},
  doi       = {10.1145/2393596.2393671},
  isbn      = {9781450316149},
  journal   = {Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering - FSE '12},
  keywords  = {bug-to-fix links,bugs,fixes,mining software repository},
  pages     = {1},
  publisher = {ACM Press},
  title     = {{Multi-layered approach for recovering links between bug reports and fixes}},
  year      = {2012}
}

@article{Mockus2002,
  author   = {Mockus, Audris and Fielding, Roy T and Herbsleb, James D},
  journal  = {ACM Trans. Softw. Eng. Methodol.},
  title    = {{Two Case Studies of Open Source Software Development: Apache and Mozilla}},
  year     = {2002},
  number   = {3},
  pages    = {309--346},
  volume   = {11},
  abstract = {According to its proponents, open source style software development has the capacity to compete successfully, and perhaps in many cases displace, traditional commercial development methods. In order to begin investigating such claims, we examine data from two major open source projects, the Apache web server and the Mozilla browser. By using email archives of source code change history and problem reports we quantify aspects of developer participation, core team size, code ownership, productivity, defect density, and problem resolution intervals for these OSS projects. We develop several hypotheses by comparing the Apache project with several commercial projects. We then test and refine several of these hypotheses, based on an analysis of Mozilla data. We conclude with thoughts about the prospects for high-performance commercial/open source process hybrids.},
  doi      = {10.1145/567793.567795},
  isbn     = {1049-331X},
  keywords = {OSS},
  url      = {http://portal.acm.org/citation.cfm?doid=567793.567795}
}
@inproceedings{Moonen2002,
  author    = {Moonen, L.},
  booktitle = {Proc. - IEEE Work. Progr. Compr.},
  title     = {{Lightweight impact analysis using island grammars}},
  year      = {2002},
  pages     = {219--228},
  publisher = {IEEE Comput. Soc},
  volume    = {2002-Janua},
  abstract  = {Impact analysis is needed for the planning and estimation of software maintenance projects. Traditional impact analysis techniques tend to be too expensive for this phase, so there is need for more lightweight approaches. We present a technique for the generation of lightweight impact analyzers from island grammars. We demonstrate this technique using a real-world case study in which we describe how island grammars can be used to find account numbers in the software portfolio of a large bank. We show how we have implemented this analysis and achieved lightweightness using a reusable generative framework for impact analyzers.},
  doi       = {10.1109/WPC.2002.1021343},
  isbn      = {0769514952},
  issn      = {1092-8138},
  keywords  = {Costs,Databases,Documentation,Navigation,Performance analysis,Portfolios,Robustness,Software maintenance,Software performance,Software systems},
  url       = {http://ieeexplore.ieee.org/document/1021343/}
}

@article{Murphy-Hill2012,
  author        = {Murphy-Hill, Emerson and Parnin, Chris and Black, Andrew P.},
  journal       = {IEEE Transactions on Software Engineering},
  title         = {{How we refactor, and how we know it}},
  year          = {2012},
  issn          = {0098-5589},
  number        = {1},
  pages         = {5--18},
  volume        = {38},
  abstract      = {Refactoring is widely practiced by developers, and considerable research and development effort has been invested in refactoring tools. However, little has been reported about the adoption of refactoring tools, and many assumptions about refactoring practice have little empirical support. In this paper, we examine refactoring tool usage and evaluate some of the assumptions made by other researchers. To measure tool usage, we randomly sampled code changes from four Eclipse and eight Mylyn developers and ascertained, for each refactoring, if it was performed manually or with tool support. We found that refactoring tools are seldom used: 11 percent by Eclipse developers and 9 percent by Mylyn developers. To understand refactoring practice at large, we drew from a variety of data sets spanning more than 39,000 developers, 240,000 tool-assisted refactorings, 2,500 developer hours, and 12,000 version control commits. Using these data, we cast doubt on several previously stated assumptions about how programmers refactor, while validating others. Finally, we interviewed the Eclipse and Mylyn developers to help us understand why they did not use refactoring tools and to gather ideas for future research.},
  archiveprefix = {arXiv},
  arxivid       = {arXiv:1011.1669v3},
  doi           = {10.1109/TSE.2011.41},
  eprint        = {arXiv:1011.1669v3},
  isbn          = {9781424434527},
  keywords      = {Refactoring,floss refactoring,refactoring tools,root-canal refactoring},
  pmid          = {25246403}
}

@article{nerur2005challenges,
  title     = {Challenges of migrating to agile methodologies},
  author    = {Nerur, Sridhar and Mahapatra, RadhaKanta and Mangalaraj, George},
  journal   = {Communications of the ACM},
  volume    = {48},
  number    = {5},
  pages     = {72--78},
  year      = {2005},
  publisher = {Citeseer}
}

@inproceedings{Neumuller06,
  author    = {C. Neumuller and P. Grunbacher},
  booktitle = {21st IEEE/ACM International Conference on Automated Software Engineering (ASE'06)},
  title     = {{Automating Software Traceability in Very Small Companies: A Case Study and Lessons Learned}},
  year      = {2006},
  month     = sep,
  pages     = {145--156},
  doi       = {10.1109/ASE.2006.25},
  issn      = {1938-4300},
  keywords  = {program diagnostics;software architecture;software tools;APIS traceability environment;IT industry;software company;software traceability;Automation;Business;Computer industry;Databases;Engineering management;ISO standards;Inhibitors;Laboratories;Software engineering;Standards development}
}

@article{Newman2017,
  abstract = {A set of lexical categories, analogous to part-of-speech categories for English prose, is defined for source-code identifiers. The lexical category for an identifier is determined from its declaration in the source code, syntactic meaning in the programming language, and static program analysis. Current techniques for assigning lexical categories to identifiers use natural-language part-of-speech taggers. However, these NLP approaches assign lexical tags based on how terms are used in English prose. The approach taken here differs in that it uses only source code to determine the lexical category. The approach assigns a lexical category to each identifier and stores this information along with each declaration. srcML is used as the infrastructure to implement the approach and so the lexical information is stored directly in the srcML markup as an additional XML element for each identifier. These lexical-category annotations can then be later used by tools that automatically generate such things as code summarization or documentation. The approach is applied to 50 open source projects and the soundness of the defined lexical categories evaluated. The evaluation shows that at every level of minimum support tested, categorization is consistent at least 79{\%} of the time with an overall consistency (across all supports) of at least 88{\%}. The categories reveal a correlation between how an identifier is named and how it is declared. This provides a syntax-oriented view (as opposed to English part-of-speech view) of developer intent of identifiers.},
  author   = {Newman, Christian D and Alsuhaibani, Reem S and Collard, Michael L and Maletic, Jonathan I},
  isbn     = {9781509055012},
  journal  = {SANER'17},
  keywords = {Natural Language Processing,identifier analysis,part-of-speech tagging,program comprehension},
  title    = {{Lexical Categories for Source Code Identifiers}},
  year     = {2017}
}

@techreport{Ng2007,
  abstract        = {Despite many empirical successes of spectral clustering methods-algorithms that cluster points using eigenvectors of matrices derived from the data-there are several unresolved issues. First, there are a wide variety of algorithms that use the eigenvectors in slightly different ways. Second, many of these algorithms have no proof that they will actually compute a reasonable clustering. In this paper, we present a simple spectral clustering algorithm that can be implemented using a few lines of Matlab. Using tools from matrix perturbation theory, we analyze the algorithm, and give conditions under which it can be expected to do well. We also show surprisingly good experimental results on a number of challenging clustering problems.},
  author          = {Ng, Andrew Y and Jordan, Michael I},
  mendeley-groups = {Untangle},
  title           = {{On Spectral Clustering: Analysis and an algorithm}},
  url             = {https://papers.nips.cc/paper/2092-on-spectral-clustering-analysis-and-an-algorithm.pdf}
}

@article{nielsen1993response,
  title     = {Response times: the three important limits},
  author    = {Nielsen, Jakob},
  journal   = {Usability Engineering},
  year      = {1993},
  publisher = {Academic Press}
}

@inproceedings{NonSourceInMLR,
  address   = {New York, New York, USA},
  author    = {Sun, Yan and Wang, Qing and Li, Mingshu},
  booktitle = {Proceedings of the 10th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement - ESEM '16},
  doi       = {10.1145/2961111.2962605},
  isbn      = {9781450344272},
  keywords  = {mining soft-,missing link recovery,non-source documents,software maintenance,ware repositories},
  pages     = {1--10},
  publisher = {ACM Press},
  title     = {{Understanding the Contribution of Non-source Documents in Improving Missing Link Recovery}},
  year      = {2016}
}

@misc{Octoverse2016,
  title        = {{GitHub Octoverse 2016}},
  author       = {{GitHub}},
  howpublished = {\url{https://octoverse.github.com/}},
  note         = {Accessed: 2017-08-07},
  year         = {2016}
}

@misc{ohes,
  title        = {{OpenHub statistics for elasticsearch}},
  author       = {{OpenHub}},
  howpublished = {\url{https://www.openhub.net/p/elasticsearch/analyses/latest/languages_summary}},
  note         = {Accessed: 2017-10-30}
}

@inproceedings{Oliveto2010,
  author    = {R. Oliveto and M. Gethers and D. Poshyvanyk and A. De Lucia},
  booktitle = {2010 IEEE 18th International Conference on Program Comprehension},
  title     = {{On the Equivalence of Information Retrieval Methods for Automated Traceability Link Recovery}},
  year      = {2010},
  month     = jun,
  pages     = {68--71},
  doi       = {10.1109/ICPC.2010.20},
  issn      = {1092-8138},
  keywords  = {information retrieval;principal component analysis;Jensen-Shannon method;automated traceability link recovery;information retrieval methods;latent Dirichlet allocation;principal component analysis;statistical analysis;vector space model;Computer science;Documentation;Indexing;Informatics;Information retrieval;Large scale integration;Linear discriminant analysis;Mathematics;Principal component analysis;Software maintenance;Empirical Studies;Information Retrieval;Traceability Recovery}
}

@misc{online_corpora,
  title        = {{Datasets as pickled python objects}},
  author       = {P\^ar\cb{t}achi, Profir-Petru and Barr, Earl T. and White, David R.},
  howpublished = {\url{https://figshare.com/s/83c448eb518b3d04651f}},
  note         = {Accessed: 2020-02-25},
  year         = {2017}
}

@article{OnlineLDA,
  author    = {Matthew Hoffman and Francis R. Bach and David M. Blei},
  title     = {{Online Learning for Latent Dirichlet Allocation}},
  year      = {2010},
  pages     = {856--864},
  booktitle = {Advances in Neural Information Processing Systems 23},
  editor    = {J. D. Lafferty and C. K. I. Williams and J. Shawe-Taylor and R. S. Zemel and A. Culotta},
  publisher = {Curran Associates, Inc.}
}

@Article{petrov2011universal,
  author  = {Petrov, Slav and Das, Dipanjan and McDonald, Ryan},
  journal = {arXiv preprint arXiv:1104.2086},
  title   = {A universal part-of-speech tagset},
  year    = {2011},
  eprint  = {1104.2086v1},
}

@article{Pinheiro96,
  author   = {F. A. C. Pinheiro and J. A. Goguen},
  journal  = {IEEE Software},
  title    = {An object-oriented tool for tracing requirements},
  year     = {1996},
  issn     = {0740-7459},
  month    = mar,
  number   = {2},
  pages    = {52--64},
  volume   = {13},
  doi      = {10.1109/52.506462},
  keywords = {formal specification;object-oriented programming;systems analysis;continuous evolution;large scale development;object oriented tool;requirements tracing;tracing requirements;tracing tool;CD recording;Disk recording;Joining processes;Programming;Social factors;Software systems;Tellurium;Virtual manufacturing}
}

@inproceedings{Pirapuraj2017,
  abstract  = {Massive amount of source codes are available free and open. Reusing those open source codes in projects can reduce the project duration and cost. Even though several Code Search Engines (CSE) are available, finding the most relevant code can be challenging. In this paper we propose a framework that can be used to overcome the above said challenge. The proposed solution starts with a Software Architecture (Class Diagram) in XML format and extracts information from the XML file, and then, it fetches relevant projects using three types of crawlers from GitHub, SourceForge, and GoogleCode. Then it finds the most relevant projects among the vast amount of downloaded projects. This research considers only Java projects. All java files in every project will be represented in Abstract Syntax Tree (AST) to extract identifiers (class names, method names, and attributes name) and comments. Action words (verbs) are extracted from comments using Part of Speech technique (POS). Those identifiers and XML file information need to be analyzed for matching. If identifiers are matched, marks will be given to those identifiers, likewise marks will be added together and then if the total mark is greater than 50{\%}, the.java file will be considered as a relevant code. Otherwise, WordNet will be used to get synonym of those identifiers and repeat the matching process using those synonyms. For connected word identifiers, camel case splitter and N-gram technique are used to separate those words. The Stanford Spellchecker is used to identify abbreviated words. The results indicate successful identification of relevant source codes.},
  author    = {Pirapuraj, P. and Perera, Indika},
  booktitle = {3rd Int. Moratuwa Eng. Res. Conf. MERCon 2017},
  doi       = {10.1109/MERCon.2017.7980465},
  isbn      = {9781509064915},
  keywords  = {Class Diagram,Code reuse,N-gram technique,PoS Tagging,Software Architecture,Source code identification,WordNet},
  pages     = {105--110},
  title     = {{Analyzing source code identifiers for code reuse using NLP techniques and WordNet}},
  year      = {2017}
}

@article{pLSI,
  author        = {Hofmann, Thomas},
  journal       = {Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval},
  title         = {{Probabilistic latent semantic indexing}},
  year          = {1999},
  issn          = {0003-2700},
  pages         = {50--57},
  abstract      = {Probabilistic Latent Semantic Indexing is a novel approac v h to automated documen t indexing whic h is based on a sta? tistical laten t class model for factor analysis of coun t data? Fitted from a training corpus of text documen ts b y a gen? eralization of the Expectation Maximization algorithm? the utilized model is able to deal with domain?speci?c synon ym y as w ell as with polysemous w ords? In con trast to standard Laten t Seman tic Indexing ?LSI? b y Singular V alue Decom? position? the probabilistic v arian t has a solid statistical foun? dation and de?nes a proper generativ e data model? Retriev al experimen ts on a n um ber of test collections indicate sub? stan tial performance gains o er direct term matc v hing meth? odsasw ell as o er LSI? In particular? the com v bination of models with di?eren t dimensionalities has pro en to be ad? v v an tageous? ?},
  archiveprefix = {arXiv},
  arxivid       = {2073829},
  doi           = {10.1021/ac801303x},
  eprint        = {2073829},
  isbn          = {1581130961},
  pmid          = {18989936}
}

@inproceedings{Pohl96,
  author    = {K. Pohl},
  booktitle = {Proceedings of the Second International Conference on Requirements Engineering},
  title     = {{PRO-ART: enabling requirements pre-traceability}},
  year      = {1996},
  month     = apr,
  pages     = {76--84},
  doi       = {10.1109/ICRE.1996.491432},
  keywords  = {software quality;software tools;systems analysis;PRO-ART 2.0;PRO-ART requirements engineering environment;automated trace capture;high-quality software systems development;requirements pre-traceability;requirements traceability;scalability problems;selective trace retrieval;three-dimensional requirements engineering framework;tool interoperability approach;trace information structuring;trace-repository;Buildings;Computer errors;Contracts;Design engineering;Environmental management;Information retrieval;Logic design;Maintenance engineering;Scalability;Software systems}
}

@inproceedings{ponzanelli2014holistic,
  title     = {Holistic recommender systems for software engineering},
  author    = {Ponzanelli, Luca},
  booktitle = {Companion Proc. 36th Int. Conf. Soft. Eng.},
  pages     = {686--689},
  year      = {2014}
}

@article{Ponzanelli2015a,
  author    = {Ponzanelli, Luca and Mocci, Andrea and Lanza, Michele},
  journal   = {IEEE Int. Work. Conf. Min. Softw. Repos.},
  title     = {{StORMeD: Stack overflow ready made data}},
  year      = {2015},
  issn      = {2160-1860},
  pages     = {474--477},
  volume    = {2015-Augus},
  abstract  = {Stack Overflow is the de facto Question and Answer (Q{\&}A) website for developers, and it has been used in many approaches by software engineering researchers to mine useful data. However, the contents of a Stack Overflow discussion are inherently heterogeneous, mixing natural language, source code, stack traces and configuration files in XML or JSON format. We constructed a full island grammar capable of modeling the set of 700,000 Stack Overflow discussions talking about Java, building a heterogeneous abstract syntax tree (H-AST) of each post (question, answer or comment) in a discussion. The resulting dataset models every Stack Overflow discussion, providing a full H-AST for each type of structured fragment (i.e., JSON, XML, Java, Stack traces), and complementing this information with a set of basic meta-information like term frequency to enable natural language analyses. Our dataset allows the end-user to perform combined analyses of the Stack Overflow by visiting the H-AST of a discussion.},
  doi       = {10.1109/MSR.2015.67},
  isbn      = {9780769555942},
  keywords  = {H-ast,Island parsing,Unstructured data},
  publisher = {IEEE}
}

@article{Ponzanelli2015b,
  author          = {Ponzanelli, Luca and Mocci, Andrea and Lanza, Michele},
  journal         = {IEEE Int. Work. Conf. Min. Softw. Repos.},
  title           = {{Summarizing complex development artifacts by mining heterogeneous data}},
  year            = {2015},
  issn            = {2160-1860},
  pages           = {401--405},
  volume          = {2015-Augus},
  abstract        = {Summarization is hailed as a promising approach to reduce the amount of information that must be taken in by the person who wants to understand development artifacts, such as pieces of code, bug reports, emails, etc. However, existing approaches treat artifacts as pure textual entities, disregarding the heterogeneous and partially structured nature of most artifacts, which contain intertwined pieces of distinct type, such as source code, diffs, stack traces, human language, etc. We present a novel approach to augment existing summarization techniques (such as LexRank) to deal with the heterogeneous and multidimensional nature of complex artifacts. Our preliminary results on heterogeneous artifacts suggest our approach outperforms the current text-based approaches.},
  doi             = {10.1109/MSR.2015.49},
  isbn            = {9780769555942},
  keywords        = {Holistic,Stack overfliow,Summarization},
  mendeley-groups = {Code pseudo-PoS tagging}
}

@article{Poplack1980,
  author  = {Poplack, Shana},
  journal = {Linguistics},
  title   = {{Sometimes Ill start a sentence in Spanish Y TERMINO EN ESPAOL: toward a typology of code-switching}},
  year    = {1980},
  month   = jan,
  pages   = {581--618},
  volume  = {18},
  doi     = {10.1515/ling.1980.18.7-8.581}
}

@misc{Porter1980,
  abstract      = {The automatic removal of suffixes from words in English is of particular interest in the field of information retrieval. An algorithm for suffix stripping is described, which has been implemented as a short, fast program in BCPL....},
  archiveprefix = {arXiv},
  arxivid       = {http://dx.doi.org/10.1108/BIJ-10-2012-0068},
  author        = {Porter, M.F.},
  booktitle     = {Program: electronic library and information systems},
  doi           = {10.1108/eb046814},
  eprint        = {/dx.doi.org/10.1108/BIJ-10-2012-0068},
  isbn          = {1558604545},
  issn          = {0033-0337},
  number        = {3},
  pages         = {130--137},
  pmid          = {16143652},
  primaryclass  = {http:},
  title         = {{An algorithm for suffix stripping}},
  volume        = {14},
  year          = {1980}
}

@inproceedings{Pratapa2018,
  abstract  = {We compare three existing bilingual word embedding approaches, and a novel approach of training skip-grams on synthetic code-mixed text generated through linguistic models of code-mixing, on two tasks-sentiment analysis and POS tagging for code-mixed text. Our results show that while CVM and CCA based embeddings perform as well as the proposed embedding technique on semantic and syntactic tasks respectively, the proposed approach provides the best performance for both tasks overall. Thus, this study demonstrates that existing bilingual embedding techniques are not ideal for code-mixed text processing and there is a need for learning multilingual word embedding from the code-mixed text.},
  author    = {Pratapa, Adithya and Choudhury, Monojit and Sitaram, Sunayana},
  booktitle = {Proc. 2018 Conf. Empir. Methods Nat. Lang. Process.},
  pages     = {3067--3072},
  title     = {{Word Embeddings for Code-Mixed Language Processing}},
  url       = {http://aclweb.org/anthology/D18-1344},
  year      = {2018}
}

@article{prechelt2014bflinks,
  author    = {Prechelt, Lutz and Pepper, Alexander},
  journal   = {International scholarly research notices},
  title     = {{Bflinks: Reliable Bugfix Links via Bidirectional References and Tuned Heuristics}},
  year      = {2014},
  volume    = {2014},
  publisher = {Hindawi}
}

@misc{ProjectRepo,
  title        = {{Aide-mmoire: Accurate Issue Links at Pull Request submission}},
  author       = {P\^ar\cb{t}achi, Profir-Petru and Barr, Earl T. and White, David R.},
  howpublished = {\url{https://github.com/PPPI/a-m/}},
  note         = {Accessed: 2017-08-14},
  year         = {2017}
}

@article{Pu2011,
  abstract        = {This research was motivated by our interest in understanding the criteria for measuring the success of a recommender system from users' point view. Even though existing work has suggested a wide range of criteria, the consistency and validity of the combined criteria have not been tested. In this paper, we describe a unifying evaluation framework, called ResQue (Recommender systems' Quality of user experience), which aimed at measuring the qualities of the recommended items, the system's usability, usefulness, interface and interaction qualities, users' satisfaction with the systems, and the influence of these qualities on users' behavioral intentions, including their intention to purchase the products recommended to them and return to the system. We also show the results of applying psychometric methods to validate the combined criteria using data collected from a large user survey. The outcomes of the validation are able to 1) support the consistency, validity and reliability of the selected criteria; and 2) explain the quality of user experience and the key determinants motivating users to adopt the recommender technology. The final model consists of thirty two questions and fifteen constructs, defining the essential qualities of an effective and satisfying recommender system, as well as providing practitioners and scholars with a cost-effective way to evaluate the success of a recommender system and identify important areas in which to invest development resources.},
  author          = {Pu, Pearl and Chen, Li},
  isbn            = {9781450306836},
  keywords        = {ACM Classification Keywords H12 [User/Machine Syst,Design,H52 [User Interfaces]: evaluation/methodology,Human Factors Keywords Recommender systems,e-Commerce recommender,post-study questionnaire,quality of user experience,user-centered design General Terms Measurement},
  mendeley-groups = {Aide-m{\'{e}}moire},
  pages           = {157--164},
  journal         = {Proceedings of the fifth ACM conference on Recommender systems},
  title           = {{A User - Centric Evaluation Framework for Recommender Systems}},
  year            = {2011}
}

@inproceedings{PULink,
  author    = {Y. Sun and C. Chen and Q. Wang and B. Boehm},
  booktitle = {2017 32nd IEEE/ACM International Conference on Automated Software Engineering (ASE)},
  title     = {Improving missing issue-commit link recovery using positive and unlabeled data},
  year      = {2017},
  month     = oct,
  pages     = {147--152},
  doi       = {10.1109/ASE.2017.8115627},
  keywords  = {learning (artificial intelligence);pattern classification;software maintenance;automatic link recovery approaches;improving missing issue-commit link recovery;link recovery model;missing links;positive data;traditional classifier;unlabeled link data;Feature extraction;Indexes;Metadata;Software maintenance;Training}
}

@phdthesis{R2Fix,
  abstract  = {Many bugs, even those that are known and documented in bug reports, remain in mature software for a long time due to the lack of the development resources to fix them. We propose a general approach, R2Fix, to automatically generate bug-fixing patches from free-form bug reports. R2Fix combines past fix patterns, machine learning techniques, and semantic patch generation techniques to fix bugs automatically. We evaluate R2Fix on three projects, i.e., the Linux kernel, Mozilla, and Apache, for three important types of bugs: buffer overflows, null pointer bugs, and memory leaks. R2Fix generates 57 patches correctly, 5 of which are new patches for bugs that have not been fixed by developers yet. We reported all 5 new patches to the developers, 4 have already been accepted and committed to the code repositories. The 57 correct patches generated by R2Fix could have shortened and saved up to an average of 63 days of bug diagnosis and patch generation time. 2013 IEEE.},
  author    = {Liu, Chen and Yang, Jinqiu and Tan, Lin and Hafiz, Munawar},
  booktitle = {A thesis presented to the University of Waterloo in fulfillment of the thesis requirement for the degree of Master of Applied Science in Electrical and Computer Engineering},
  doi       = {10.1109/ICST.2013.24},
  isbn      = {978-0-7695-4968-2},
  issn      = {2159-4848},
  keywords  = {automated bug fixing,automated program repair,bug report classification,fix pattern study},
  school    = {University of Waterloo},
  title     = {{R2Fix: Automatically generating bug fixes from bug reports}},
  year      = {2013}
}

@inproceedings{RCLinker,
  author    = {Le, Tien Duy B and Linares-V{\'{a}}squez, Mario and Lo, David and Poshyvanyk, Denys},
  booktitle = {IEEE International Conference on Program Comprehension},
  title     = {{RCLinker: Automated Linking of Issue Reports and Commits Leveraging Rich Contextual Information}},
  year      = {2015},
  month     = may,
  pages     = {36--47},
  publisher = {IEEE},
  volume    = {2015-Augus},
  abstract  = {Links between issue reports and their corresponding commits in$\backslash$nversion control systems are often missing. However, these links are$\backslash$nimportant for measuring the quality of various parts of a software$\backslash$nsystem, predicting defects, and many other tasks. A number of existing$\backslash$napproaches have been designed to solve this problem by automatically$\backslash$nlinking bug reports to source code commits via comparison of textual$\backslash$ninformation in commit messages with textual contents in the bug reports.$\backslash$nYet, the effectiveness of these techniques is oftentimes sub optimal$\backslash$nwhen commit messages are empty or only contain minimum information, this$\backslash$nparticular problem makes the process of recovering trace ability links$\backslash$nbetween commits and bug reports particularly challenging. In this work,$\backslash$nwe aim at improving the effectiveness of existing bug linking techniques$\backslash$nby utilizing rich contextual information. We rely on a recently proposed$\backslash$ntool, namely Change Scribe, which generates commit messages containing$\backslash$nrich contextual information by using a number of code summarization$\backslash$ntechniques. Our approach then extracts features from these automatically$\backslash$ngenerated commit messages and bug reports and inputs them into a$\backslash$nclassification technique that creates a discriminative model used to$\backslash$npredict if a link exists between a commit message and a bug report. We$\backslash$ncompared our approach, coined as RCLinker (Rich Context Linker), to$\backslash$nMLink, which is an existing state-of-the-art bug linking approach. Our$\backslash$nexperiment results on bug reports from 6 software projects show that$\backslash$nRCLinker can outperform MLink in terms of F-measure by 138.66{\%}.},
  doi       = {10.1109/ICPC.2015.13},
  isbn      = {9781467381598},
  keywords  = {ChangeScribe,Classification,Feature Extraction,Recovering Missing Links}
}

@inproceedings{refinym2018dash,
  abstract  = {Source code is bimodal: it combines a formal, algorithmic channel and a natural language channel of identiiers and comments. In this work, we model the bimodality of code with name lows, an assignment low graph augmented to track identiier names. Conceptual types are logically distinct types that do not always coincide with program types. Passwords and URLs are example conceptual types that can share the program type string. Our tool, RefiNym, is an unsupervised method that mines a lattice of conceptual types from name lows and reiies them into distinct nominal types. For string, RefiNym inds and splits conceptual types originally merged into a single type, reducing the number of same-type variables per scope from 8.7 to 2.2 while eliminating 21.9{\%} of scopes that have more than one same-type variable in scope. This makes the code more self-documenting and frees the type system to prevent a developer from inadvertently assigning data across conceptual types. CCS CONCEPTS {\textperiodcentered} Software and its engineering  Data types and structures;},
  author    = {Dash, Santanu Kumar and Allamanis, Miltiadis and Barr, Earl T},
  booktitle = {Proc. 2018 26th ACM Jt. Meet. Eur. Softw. Eng. Conf. Symp. Found. Softw. Eng. - ESEC/FSE 2018},
  doi       = {10.1145/3236024.3236042},
  isbn      = {9781450355735},
  keywords  = {Information-theoretic Clustering,Type Refinement},
  pages     = {107--117},
  title     = {{RefiNym: using names to refine types}},
  year      = {2018}
}

@inproceedings{RetForBugLoc,
  author    = {Rao, Shivani and Kak, Avinash},
  booktitle = {Proceeding of the 8th working conference on Mining software repositories - MSR '11},
  title     = {{Retrieval from software libraries for bug localization}},
  year      = {2011},
  pages     = {43},
  abstract  = {From the standpoint of retrieval from large software libraries for the purpose of bug localization, we compare five generic text models and certain composite variations thereof. The generic models are: the Unigram Model (UM), the Vector Space Model (VSM), the Latent Semantic Analysis Model (LSA), the Latent Dirichlet Allocation Model (LDA), and the Cluster Based Document Model (CBDM). The task is to locate the files that are relevant to a bug reported in the form of a textual description by a software developer. We use for our study iBUGS, a benchmarked bug localization dataset with 75 KLOC and a large number of bugs (291). A major conclusion of our comparative study is that simple text models such as UM and VSM are more effective at correctly retrieving the relevant files from a library as compared to the more sophisticated models such as LDA. The retrieval effectiveness for the various models was measured using the following two metrics: (1) Mean Average Precision; and (2) Rank-based metrics. Using the SCORE metric, we also compare the retrieval effectiveness of the models in our study with some other bug localization tools.},
  doi       = {10.1145/1985441.1985451},
  isbn      = {9781450305747},
  issn      = {0270-5257},
  keywords  = {bug localization,information,latent dirichlet allocation,latent semantic analysis,retrieval,software engineering}
}

@misc{RFE_scipy,
  author       = {{Scikit-learn}},
  title        = {{Recursive Feature Elimination: SciKit Implementation}},
  howpublished = {\url{https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html}},
  note         = {Accessed: 2020-06-17},
  year         = {2020}
}

@article{Rigby2014,
  author   = {Rigby, Peter C. and German, Daniel M. and Cowen, Laura and Storey, Margaret-Anne},
  journal  = {ACM Trans. Softw. Eng. Methodol.},
  title    = {{Peer Review on Open-Source Software Projects}},
  year     = {2014},
  issn     = {1049-331X},
  number   = {4},
  pages    = {1--33},
  volume   = {23},
  abstract = {Peer review is seen as an important quality-assurance mechanism in both industrial development and the open-source software (OSS) community. The techniques for performing inspections have been well studied in industry; in OSS development, software peer reviews are not as well understood. To develop an empirical understanding of OSS peer review, we examine the review policies of 25 OSS projects and study the archival records of six large, mature, successful OSS projects. We extract a series of measures based on those used in traditional inspection experiments. We measure the frequency of review, the size of the contribution under review, the level of participation during review, the experience and expertise of the individuals involved in the review, the review interval, and the number of issues discussed during review. We create statistical models of the review efficiency, review interval, and effectiveness, the issues discussed during review, to determine which measures have the largest impact on review efficacy. We find that OSS peer reviews are conducted asynchronously by empowered experts who focus on changes that are in their area of expertise. Reviewers provide timely, regular feedback on small changes. The descrip-tive statistics clearly show that OSS review is drastically different from traditional inspection.},
  doi      = {10.1145/2594458},
  url      = {http://dl.acm.org/citation.cfm?doid=2668018.2594458}
}

@inproceedings{Robillard:2003:FTL:776816.776969,
  author    = {Robillard, Martin P. and Murphy, Gail C.},
  booktitle = {Proceedings of the 25th International Conference on Software Engineering},
  title     = {{FEAT: A Tool for Locating, Describing, and Analyzing Concerns in Source Code}},
  year      = {2003},
  address   = {Washington, DC, USA},
  pages     = {822--823},
  publisher = {IEEE Computer Society},
  series    = {ICSE '03},
  acmid     = {776969},
  isbn      = {0-7695-1877-X},
  location  = {Portland, Oregon},
  numpages  = {2}
}

@article{Roover2017,
  author = {Roover, De and Muylaert, Ward},
  title  = {{Untangling Source Code Changes Using Program Slicing}},
  year   = {2017}
}

@misc{Roslyn,
  author       = {{Microsoft}},
  howpublished = {\url{https://github.com/dotnet/roslyn}},
  note         = {Accessed: 2018-05-31},
  title        = {Microsoft Rosyln}
}

@article{scarselli2008graph,
  title     = {The graph neural network model},
  author    = {Scarselli, Franco and Gori, Marco and Tsoi, Ah Chung and Hagenbuchner, Markus and Monfardini, Gabriele},
  journal   = {IEEE Transactions on Neural Networks},
  volume    = {20},
  number    = {1},
  pages     = {61--80},
  year      = {2008},
  publisher = {IEEE}
}
@article{Sebastian2018,
  author = {Sebastian, Panichella and Harald, Proksch},
  number = {May},
  title  = {{Redundancy-free Analysis of Multi-revision Software Artifacts Redundancy-free Analysis of Multi-revision Software Artifacts}},
  year   = {2018}
}

@article{shervashidze2011weisfeiler,
  title   = {Weisfeiler-lehman graph kernels},
  author  = {Shervashidze, Nino and Schweitzer, Pascal and Leeuwen, Erik Jan van and Mehlhorn, Kurt and Borgwardt, Karsten M},
  journal = {Journal of Machine Learning Research},
  volume  = {12},
  number  = {Sep},
  pages   = {2539--2561},
  year    = {2011}
}
@book{Shihab2013,
  author    = {Shihab, Emad and Ihara, Akinori and Kamei, Yasutaka and Ibrahim, Walid M. and Ohira, Masao and Adams, Bram and Hassan, Ahmed E. and Matsumoto, Ken Ichi},
  title     = {{Studying re-opened bugs in open source software}},
  year      = {2013},
  isbn      = {1066401292},
  number    = {5},
  volume    = {18},
  abstract  = {Bug fixing accounts for a large amount of the software maintenance resources. Generally, bugs are reported, fixed, verified and closed. However, in some cases bugs have to be re-opened. Re-opened bugs increase maintenance costs, degrade the overall user-perceived quality of the software and lead to unnecessary rework by busy practitioners. In this paper, we study and predict re-opened bugs through a case study on three large open source projects-namely Eclipse, Apache and OpenOffice. We structure our study along four dimensions: (1) the work habits dimension (e.g., the weekday on which the bug was initially closed), (2) the bug report dimension (e.g., the component in which the bug was found) (3) the bug fix dimension (e.g., the amount of time it took to perform the initial fix) and (4) the team dimension (e.g., the experience of the bug fixer). We build decision trees using the aforementioned factors that aim to predict re-opened bugs. We perform top node analysis to determine which factors are the most important indicators of whether or not a bug will be re-opened. Our study shows that the comment text and last status of the bug when it is initially closed are the most important factors related to whether or not a bug will be re-opened. Using a combination of these dimensions, we can build explainable prediction models that can achieve a precision between 52.1-78.6 {\%} and a recall in the range of 70.5-94.1 {\%} when predicting whether a bug will be re-opened. We find that the factors that best indicate which bugs might be re-opened vary based on the project. The comment text is the most important factor for the Eclipse and OpenOffice projects, while the last status is the most important one for Apache. These factors should be closely examined in order to reduce maintenance cost due to re-opened bugs. {\textcopyright} 2012 Springer Science+Business Media, LLC.},
  booktitle = {Empirical Software Engineering},
  doi       = {10.1007/s10664-012-9228-6},
  issn      = {1382-3256},
  keywords  = {Bug reports,Open source software,Re-opened bugs},
  pages     = {1005--1042}
}

@article{siglidis2018grakel,
  author  = {Siglidis, Giannis and Nikolentzos, Giannis and Limnios, Stratis and Giatsidis, Christos and Skianis, Konstantinos and Vazirgiannis, Michalis},
  journal = {arXiv preprint arXiv:1806.02193},
  title   = {{GraKeL: A Graph Kernel Library in Python}},
  year    = {2018}
}

@techreport{slicingsurvey,
  author    = {Tip, Frank},
  title     = {{A Survey of Program Slicing Techniques.}},
  year      = {1994},
  address   = {Amsterdam, The Netherlands, The Netherlands},
  publisher = {CWI (Centre for Mathematics and Computer Science)},
  source    = {http://www.ncstrl.org:8900/ncstrl/servlet/search?formname=detail\&id=oai%3Ancstrlh%3Aercim_cwi%3Aercim.cwi%2F%2FCS-R9438}
}

@article{SMOTE,
  abstract = {An approach to the construction of classifiers from imbalanced datasets is described. A dataset is imbalanced if the classification categories are not approximately equally rep-resented. Often real-world data sets are predominately composed of " normal " examples with only a small percentage of " abnormal " or " interesting " examples. It is also the case that the cost of misclassifying an abnormal (interesting) example as a normal example is often much higher than the cost of the reverse error. Under-sampling of the majority (nor-mal) class has been proposed as a good means of increasing the sensitivity of a classifier to the minority class. This paper shows that a combination of our method of over-sampling the minority (abnormal) class and under-sampling the majority (normal) class can achieve better classifier performance (in ROC space) than only under-sampling the majority class. This paper also shows that a combination of our method of over-sampling the minority class and under-sampling the majority class can achieve better classifier performance (in ROC space) than varying the loss ratios in Ripper or class priors in Naive Bayes. Our method of over-sampling the minority class involves creating synthetic minority class examples. Experiments are performed using C4.5, Ripper and a Naive Bayes classifier. The method is evaluated using the area under the Receiver Operating Characteristic curve (AUC) and the ROC convex hull strategy.},
  author   = {Chawla, Nitesh V and Bowyer, Kevin W and Hall, Lawrence O and Kegelmeyer, W Philip},
  journal  = {Journal of Artificial Intelligence Research},
  pages    = {321--357},
  title    = {{SMOTE: Synthetic Minority Over-sampling Technique}},
  volume   = {16},
  year     = {2002}
}

@article{SoftChangeToBugLoc,
  abstract = {Various information retrieval (IR) based techniques have been proposed recently to locate bugs automatically at the file level. However, their usefulness is often compromised by the coarse granularity of files and the lack of contextual information. To address this, we propose to locate bugs using software changes, which offer finer granularity than files and provide important contextual clues for bug-fixing. We observe that bug inducing changes can facilitate the bug fixing process. For example, it helps triage the bug fixing task to the developers who committed the bug inducing changes or enables developers to fix bugs by reverting these changes. Our study further identifies that change logs and the naturally small granularity of changes can help boost the performance of IR-based bug localization. Motivated by these observations, we propose an IR-based approach Locus to locate bugs from software changes, and evaluate it on six large open source projects. The results show that Locus outperforms existing techniques at the source file level localization significantly. MAP and MRR in particular have been improved, on average, by 20:1{\%} and 20:5{\%}, respectively. Locus is also capable of locating the inducing changes within top 5 for 41:0{\%} of the bugs. The results show that Locus can significantly reduce the number of lines needing to be scanned to locate the bug compared with existing techniques. {\textcopyright} 2016 ACM.},
  author   = {Wen, Ming and Wu, Rongxin and Cheung, Shing-chi},
  doi      = {10.1145/2970276.2970359},
  isbn     = {9781450338455},
  journal  = {31st IEEE/ACM International Conference on Automated Software Engineering (ASE 2016)},
  keywords = {bug localization,information retrieval,soft-,software changes},
  pages    = {262--273},
  title    = {{Locus : Locating Bugs from Software Changes}},
  year     = {2016}
}

@article{SoftwareRepoVisualisation,
  author   = {Greene, Gillian J. and Esterhuizen, Marvin and Fischer, Bernd},
  journal  = {Information and Software Technology},
  title    = {{Visualizing and exploring software version control repositories using interactive tag clouds over formal concept lattices}},
  year     = {2016},
  issn     = {0950-5849},
  abstract = {Context: version control repositories contain a wealth of implicit information that can be used to answer many questions about a project's development process. However, this information is not directly accessible in the repositories and must be extracted and visualized. Objective: the main objective of this work is to develop a flexible and generic interactive visualization engine called ConceptCloud that supports exploratory search in version control repositories. Method: ConceptCloud is a flexible, interactive browser for SVN and Git repositories. Its main novelty is the combination of an intuitive tag cloud visualization with an underlying concept lattice that provides a formal structure for navigation. ConceptCloud supports concurrent navigation in multiple linked but individually customizable tag clouds, which allows for multi-faceted repository browsing, and scriptable construction of unique visualizations. Results: we describe the mathematical foundations and implementation of our approach and use ConceptCloud to quickly gain insight into the team structure and development process of three projects. We perform a user study to determine the usability of ConceptCloud. We show that untrained participants are able to answer historical questions about a software project better using ConceptCloud than using a linear list of commits. Conclusion: ConceptCloud can be used to answer many difficult questions such as What has happened in this project while I was away? and Which developers collaborate?. Tag clouds generated from our approach provide a visualization in which version control data can be aggregated and explored interactively.},
  doi      = {10.1016/j.infsof.2016.12.001}
}

@misc{sorevcorpus,
  author       = {P\^ar\cb{t}achi, Profir-Petru and Dash, Santanu K and Treude, Christoph and Barr, Earl T},
  howpublished = {\url{https://github.com/PPPI/POSIT/blob/master/misc.zip}},
  note         = {[Online; 22.01.2020]},
  title        = {{StackOverflow: Code Formatting Revisions Corpus}},
  year         = {2020}
}

@article{Soto2018,
  abstract = {Code-switching is the fluent alternation between two or more languages in conversation between bilinguals. Large populations of speakers code-switch during communication, but little effort has been made to develop tools for code-switching, including part-of-speech taggers. In this paper, we propose an approach to POS tagging of code-switched English-Spanish data based on recurrent neural networks. We test our model on known monolin-gual benchmarks to demonstrate that our neural POS tagging model is on par with state-of-the-art methods. We next test our code-switched methods on the Miami Bangor corpus of English-Spanish conversation , focusing on two types of experiments: POS tagging alone, for which we achieve 96.34{\%} accuracy, and joint part-of-speech and language ID tagging, which achieves similar POS tagging accuracy (96.39{\%}) and very high language ID accuracy (98.78{\%}). Finally, we show that our proposed models outperform other state-of-the-art code-switched taggers.},
  author   = {Soto, Victor and Hirschberg, Julia},
  pages    = {1--10},
  title    = {{Joint part-of-speech and Language ID Tagging for Code-Switched Data}},
  year     = {2018}
}

@article{Tao2012,
  abstract = {Software evolves with continuous source-code changes. These code changes usually need to be understood by software engineers when performing their daily development and maintenance tasks. However, despite its high importance, such change-understanding practice has not been systematically studied. Such lack of empirical knowledge hinders attempts to evaluate this fundamental practice and improve the corresponding tool support.$\backslash$n$\backslash$nTo address this issue, in this paper, we present a large-scale quantitative and qualitative study at Microsoft. The study investigates the role of understanding code changes during software-development process, explores engineers' information needs for understanding changes and their requirements for the corresponding tool support. The study results reinforce our beliefs that understanding code changes is an indispensable task performed by engineers in software-development process. A number of insufficiencies in the current practice also emerge from the study results. For example, it is difficult to acquire important information needs such as a change's completeness, consistency, and especially the risk imposed by it on other software components. In addition, for understanding a composite change, it is valuable to decompose it into sub-changes that are aligned with individual development issues; however, currently such decomposition lacks tool support.},
  author   = {Tao, Yida and Dang, Yingnong and Xie, Tao and Zhang, Dongmei and Kim, Sunghun},
  doi      = {10.1145/2393596.2393656},
  isbn     = {9781450316149},
  journal  = {Proc. ACM SIGSOFT 20th Int. Symp. Found. Softw. Eng. - FSE '12},
  pages    = {1},
  title    = {{How do software engineers understand code changes?}},
  url      = {http://dl.acm.org/citation.cfm?doid=2393596.2393656},
  year     = {2012}
}

@misc{tensorflow2015-whitepaper,
  author = {Mart\'{\i}nAbadi and AshishAgarwal and PaulBarham and EugeneBrevdo and ZhifengChen and CraigCitro and GregS.Corrado and AndyDavis and JeffreyDean and MatthieuDevin and SanjayGhemawat and IanGoodfellow and AndrewHarp and GeoffreyIrving and MichaelIsard and Yangqing Jia and RafalJozefowicz and LukaszKaiser and ManjunathKudlur and JoshLevenberg and DanMan\'{e} and RajatMonga and SherryMoore and DerekMurray and ChrisOlah and MikeSchuster and JonathonShlens and BenoitSteiner and IlyaSutskever and KunalTalwar and PaulTucker and VincentVanhoucke and VijayVasudevan and FernandaVi\'{e}gas and OriolVinyals and PeteWarden and MartinWattenberg and MartinWicke and YuanYu and XiaoqiangZheng},
  note   = {Software available from tensorflow.org},
  title  = {{TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
  year   = {2015},
  url    = {http://tensorflow.org/}
}

@article{tfidf,
  title     = {Introduction to modern information retrieval},
  author    = {Salton, Gerard and McGill, Michael J},
  year      = {1986},
  publisher = {McGraw-Hill, Inc.}
}

@book{TopicModelSurvey,
  author    = {Chen, Tse-Hsun and Thomas, Stephen W. and Hassan, Ahmed E.},
  publisher = {Springer US},
  title     = {{A survey on the use of topic models when mining software repositories}},
  year      = {2016},
  isbn      = {1066401594028},
  month     = oct,
  number    = {5},
  volume    = {21},
  abstract  = {Researchers in software engineering have attempted to improve software devel- opment by mining and analyzing software repositories. Since the majority of the software engineering data is unstructured, researchers have applied Information Retrieval (IR) tech- niques to help software development. The recent advances of IR, especially statistical topic models, have helped make sense of unstructured data in software repositories even more. However, even though there are hundreds of studies on applying topic models to software repositories, there is no study that shows how the models are used in the software engineer- ing research community, and which software engineering tasks are being supported through topic models. Moreover, since the performance of these topic models is directly related to the model parameters and usage, knowing how researchers use the topic models may also help future studies make optimal use of such models. Thus, we surveyed 167 articles from the software engineering literature that make use of topic models.We find that i) most stud- ies centre around a limited number of software engineering tasks; ii) most studies use only basic topic models; iii) and researchers usually treat topic models as black boxes without fully exploring their underlying assumptions and parameter values. Our paper provides a starting point for new researchers who are interested in using topic models, and may help new researchers and practitioners determine how to best apply topic models to a particular software engineering task.},
  booktitle = {Empirical Software Engineering},
  doi       = {10.1007/s10664-015-9402-8},
  issn      = {1382-3256},
  keywords  = {1 introduction and motivation,LDA,LSI,Survey,Topic modeling,lda,lsi,survey,topic modeling},
  pages     = {1843--1919}
}

@article{TopicTraceability,
  abstract = {Software traceability is a fundamentally important task in software engineering. The need for automated traceability increases as projects become more complex and as the number of artifacts increases. We propose an automated technique that combines traceability with a machine learning technique known as topic modeling. Our approach automatically records traceability links during the software development process and learns a probabilistic topic model over artifacts. The learned model allows for the semantic categorization of artifacts and the topical visualization of the software system. To test our approach, we have implemented several tools: an artifact search tool combining keyword-based search and topic modeling, a recording tool that performs prospective traceability, and a visualization tool that allows one to navigate the software architecture and view semantic topics associated with relevant artifacts and architectural components. We apply our approach to several data sets and discuss how topic modeling enhances software traceability, and vice versa.},
  author   = {Asuncion, Hazeline U and Asuncion, Arthur U and Taylor, Richard N},
  doi      = {10.1145/1806799.1806817},
  isbn     = {978-1-60558-719-6},
  issn     = {0270-5257},
  journal  = {2010 ACM/IEEE 32nd International Conference on Software Engineering},
  keywords = {latent dirichlet allocation,software architecture,software traceability,topic model},
  pages    = {95--104},
  title    = {{Software traceability with topic modeling}},
  volume   = {1},
  year     = {2010}
}

@article{TraceabilityEvolution,
  abstract = {{\textcopyright} 2016 IEEE.Trace links provide critical support for numerous software engineering activities including safety analysis, compliance verification, test-case selection, and impact prediction. However, as the system evolves over time, there is a tendency for the quality of trace links to degrade into a tangle of inaccurate and untrusted links. This is especially true with the links between source-code and upstream artifacts such as requirements - because developers frequently refactor and change code without updating the links. We present TLE (Trace Link Evolver), a solution for automating the evolution of trace links as changes are introduced to source code. We use a set of heuristics, open source tools, and information retrieval methods to detect common change scenarios across different versions of software. Each change scenario is then associated with a set of link evolution heuristics which are used to evolve trace links. We evaluate our approach through a controlled experiment and also through applying it across 27 releases of the Cassandra Database System. Results show that the trace links evolved using our approach are significantly more accurate than those generated using information retrieval alone.},
  author   = {Rahimi, Mona and Goss, William and Cleland-Huang, Jane},
  doi      = {10.1109/ICSME.2016.57},
  isbn     = {9781509038060},
  journal  = {Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016},
  keywords = {Evolution,Maintenance,Traceability},
  pages    = {99--109},
  title    = {{Evolving requirements-to-code trace links across versions of a software system}},
  year     = {2017}
}

@inproceedings{TwitterLDA,
  abstract  = {Twitter as a new form of social media can potentially con- tain much useful information, but content analysis on Twitter has not been well studied. In particular, it is not clear whether as an information source Twitter can be simply regarded as a faster news feed that covers mostly the same information as traditional news media. In This paper we empirically compare the content of Twitter with a traditional news medium, New York Times, using unsupervised topic modeling. We use a Twitter-LDA model to discover topics from a representative sample of the entire Twitter.We then use text mining techniques to compare these Twitter topics with topics from New York Times, taking into considera- tion topic categories and types. We also study the relation between the proportions of opinionated tweets and retweets and topic categories and types. Our comparisons show interesting and useful findings for down- stream IR or DM applications.},
  author    = {Zhao, Wayne Xin and Jiang, Jing and Weng, Jianshu and He, Jing and Lim, Ee-Peng and Yan, Hongfei and Li, Xiaoming},
  booktitle = {Proceedings of the 33rd European conference on Advances in information retrieval (ECIR'11)},
  doi       = {10.1007/978-3-642-20161-5_34},
  isbn      = {978-3-642-20160-8},
  keywords  = {microblogging,topic modeling,twitter},
  pages     = {338--349},
  publisher = {Springer, Berlin, Heidelberg},
  title     = {{Comparing Twitter and Traditional Media using Topic Models}},
  year      = {2011}
}

@article{velickovic2018graph,
  title   = {{Graph Attention Networks}},
  author  = {Veli{\v{c}}kovi{\'{c}}, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Li{\`{o}}, Pietro and Bengio, Yoshua},
  journal = {International Conference on Learning Representations},
  year    = {2018},
  url     = {https://openreview.net/forum?id=rJXMpikCZ},
  note    = {accepted as poster}
} 

@Article{vishwanathan2010graph,
  author  = {Vishwanathan, S Vichy N and Schraudolph, Nicol N and Kondor, Risi and Borgwardt, Karsten M},
  journal = {Journal of Machine Learning Research},
  title   = {Graph kernels},
  year    = {2010},
  number  = {Apr},
  pages   = {1201--1242},
  volume  = {11},
  eprint  = {0807.0093v1},
}

@article{Viterbi1967,
  author   = {A. Viterbi},
  journal  = {IEEE Transactions on Information Theory},
  title    = {Error bounds for convolutional codes and an asymptotically optimum decoding algorithm},
  year     = {1967},
  issn     = {0018-9448},
  month    = apr,
  number   = {2},
  pages    = {260--269},
  volume   = {13},
  abstract = {The probability of error in decoding an optimal convolutional code transmitted over a memoryless channel is bounded from above and below as a function of the constraint length of the code. For all but pathological channels the bounds are asymptotically (exponentially) tight for rates above<tex>R_{0}</tex>, the computational cutoff rate of sequential decoding. As a function of constraint length the performance of optimal convolutional codes is shown to be superior to that of block codes of the same length, the relative improvement increasing with rate. The upper bound is obtained for a specific probabilistic nonsequential decoding algorithm which is shown to be asymptotically optimum for rates above<tex>R_{0}</tex>and whose performance bears certain similarities to that of sequential decoding algorithms.},
  doi      = {10.1109/TIT.1967.1054010},
  keywords = {Convolutional codes}
}

@article{Watanabe2010,
  abstract        = {We propose a new approach to the analysis of Loopy Belief Propagation (LBP) by establishing a formula that connects the Hessian of the Bethe free energy with the edge zeta function. The formula has a number of theoretical implications on LBP. It is applied to give a sufficient condition that the Hessian of the Bethe free energy is positive definite, which shows non-convexity for graphs with multiple cycles. The formula clarifies the relation between the local stability of a fixed point of LBP and local minima of the Bethe free energy. We also propose a new approach to the uniqueness of LBP fixed point, and show various conditions of uniqueness.},
  archiveprefix   = {arXiv},
  arxivid         = {1103.0605},
  author          = {Watanabe, Yusuke and Fukumizu, Kenji},
  eprint          = {1103.0605},
  isbn            = {9781615679119},
  issn            = {0014-2956},
  journal         = {Adv. Neural Inf. Process. Syst.},
  keywords        = {bethe free energy,function,graph zeta,graphical models,ihara-bass formula,loopy belief propagation},
  mendeley-groups = {Untangle},
  number          = {1988},
  pages           = {19},
  pmid            = {10102984},
  title           = {{Loopy Belief Propagation, Bethe Free Energy and Graph Zeta Function}},
  url             = {https://arxiv.org/pdf/1002.3307.pdf http://arxiv.org/abs/1002.3307},
  volume          = {cs.AI},
  year            = {2010}
}

@inproceedings{Weisfeiler1968ReductionOA,
  title  = {Reduction of a graph to a canonical form and an algebra arising during this reduction},
  author = {B. Yu. Weisfeiler and A. A. Leman},
  year   = {1968}
}

@inproceedings{yanardag2015deep,
  title     = {Deep graph kernels},
  author    = {Yanardag, Pinar and Vishwanathan, SVN},
  booktitle = {Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  pages     = {1365--1374},
  year      = {2015}
}

@inproceedings{Yang2016,
  author    = {Yang, Hui and Sun, Xiaobing and Duan, Yucong and Zhao, Han and Li, Bin},
  booktitle = {Lect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics)},
  title     = {{On expanding abbreviated identifiers in the source code}},
  year      = {2016},
  pages     = {588--595},
  volume    = {9937 LNCS},
  abstract  = {Program comprehension is an important and difficult task in software development and evolution, which is costly and time-consuming. Some software abbreviated identifiers in the source code can further increase the difficulty of the program comprehension, especially for the junior developers who have less developing expertise for the software system. Moreover, a number of studies focused on applying information retrieval (IR) techniques to analyze the source code identifiers for various software maintenance tasks. These IR techniques would have difficulty in exploring abbreviations in the program. Hence, this paper proposes a novel approach to expand the abbreviations of software identifiers. The proposed approach searches the expansions of abbreviated identifiers considering the searching resources of the program and the Web. An empirical study has been evaluated and demonstrates that our approach can effectively recommend the expansions, which can not only help developers comprehend the program, but also assist IR techniques in further exploiting the natural language information in the program. {\textcopyright} Springer International Publishing AG 2016.},
  doi       = {10.1007/978-3-319-46257-8_63},
  isbn      = {9783319462561},
  issn      = {1611-3349},
  keywords  = {Abbreviated identifier expansion,Information retrieval (IR) techniques,Program comprehension}
}

@article{Yin2018,
  author          = {Yin, Pengcheng and Deng, Bowen and Chen, Edgar and Vasilescu, Bogdan and Neubig, Graham},
  journal         = {Proc. - Int. Conf. Softw. Eng.},
  title           = {{Learning to mine aligned code and natural language pairs from stack overflow}},
  year            = {2018},
  issn            = {0270-5257},
  pages           = {476--486},
  abstract        = {For tasks like code synthesis from natural language, code retrieval, and code summarization, data-driven models have shown great promise. However, creating these models require parallel data between natural language (NL) and code with fine-grained alignments. Stack Overflow (SO) is a promising source to create such a data set: the questions are diverse and most of them have corresponding answers with high quality code snippets. However, existing heuristic methods (e.g., pairing the title of a post with the code in the accepted answer) are limited both in their coverage and the correctness of the NL-code pairs obtained. In this paper, we propose a novel method to mine high-quality aligned data from SO using two sets of features: hand-crafted features considering the structure of the extracted snippets, and correspondence features obtained by training a probabilistic model to capture the correlation between NL and code using neural networks. These features are fed into a classifier that determines the quality of mined NL-code pairs. Experiments using Python and Java as test beds show that the proposed method greatly expands coverage and accuracy over existing mining methods, even when using only a small number of labeled examples. Further, we find that reasonable results are achieved even when training the classifier on one language and testing on another, showing promise for scaling NL-code mining to a wide variety of programming languages beyond those for which we are able to annotate data.},
  archiveprefix   = {arXiv},
  arxivid         = {1805.08949},
  doi             = {10.1145/3196398.3196408},
  eprint          = {1805.08949},
  isbn            = {9781450357166},
  mendeley-groups = {Code pseudo-PoS tagging}
}

@article{Yu2003,
  abstract = {We propose a principled account on multiclass spectral clustering. Given a discrete clustering formulation, we first solve a relaxed continuous optimization problem by eigen-decomposition. We clarify the role of eigenvectors as a generator of all optimal solutions through orthonormal transforms. We then solve an optimal discretization problem, which seeks a discrete solution closest to the continuous optima. The discretization is efficiently computed in an iterative fashion using singular value decomposition and nonmaximum suppression. The resulting discrete solutions are nearly global-optimal. Our method is robust to random initialization and converges faster than other clustering methods. Experiments on real image segmentation are reported.},
  author   = {Yu and Shi},
  doi      = {10.1109/ICCV.2003.1238361},
  isbn     = {0-7695-1950-4},
  journal  = {Proc. Ninth IEEE Int. Conf. Comput. Vis.},
  number   = {1},
  pages    = {313--319 vol.1},
  title    = {{Multiclass spectral clustering}},
  url      = {http://ieeexplore.ieee.org/document/1238361/},
  year     = {2003}
}

@inproceedings{Zeng2018,
  abstract  = {Code-switching language modeling is challenging due to statistics of each individual language, as well as statistics of cross-lingual language are insufficient. To compensate for the issue of statistical insufficiency, in this paper we propose a word-class n-gram language modeling approach of which only infrequent words are clustered while most frequent words are treated as singleton classes themselves. We first demonstrate the effectiveness of the proposed method on our English-Mandarin code-switching SEAME data in terms of perplexity. Compared with the conventional word n-gram language models, as well as the word-class n-gram language models of which entire vocabulary words are clustered, the proposed word-class n-gram language modeling approach can yield lower perplexity on our SEAME dev data sets. Additionally, we observed further perplexity reduction by interpolating the word n-gram language models with the proposed word-class n-gram language models. We also attempted to build word-class n-gram language models using third-party text data with our proposed method, and similar perplexity performance improvement was obtained on our SEAME dev data sets when they are interpolated with the word n-gram language models. Finally, to examine the contribution of the proposed language modeling approach to code-switching speech recognition, we conducted lattice based n-best rescoring.},
  author    = {Zeng, Zhiping and Xu, Haihua and Chong, Tze Yuang and Chng, Eng Siong and Li, Haizhou},
  booktitle = {Proc. - 9th Asia-Pacific Signal Inf. Process. Assoc. Annu. Summit Conf. APSIPA ASC 2017},
  doi       = {10.1109/APSIPA.2017.8282279},
  isbn      = {9781538615423},
  pages     = {1596--1601},
  title     = {{Improving N-gram language modeling for code-switching speech recognition}},
  volume    = {2018-February},
  year      = {2018}
}

@inproceedings{Zhang2016,
  author          = {Zhang, Feng and Zheng, Quan and Zou, Ying and Hassan, Ahmed E.},
  booktitle       = {Proc. 38th Int. Conf. Softw. Eng. - ICSE '16},
  title           = {{Cross-project defect prediction using a connectivity-based unsupervised classifier}},
  year            = {2016},
  address         = {New York, New York, USA},
  pages           = {309--320},
  publisher       = {ACM Press},
  abstract        = {Defect prediction on projects with limited historical data has attracted great interest from both researchers and prac-titioners. Cross-project defect prediction has been the main area of progress by reusing classifiers from other projects. However, existing approaches require some degree of ho-mogeneity (e.g., a similar distribution of metric values) be-tween the training projects and the target project. Satisfy-ing the homogeneity requirement often requires significant effort (currently a very active area of research). An unsupervised classifier does not require any training data, therefore the heterogeneity challenge is no longer an issue. In this paper, we examine two types of unsupervised classifiers: a) distance-based classifiers (e.g., k-means); and b) connectivity-based classifiers. While distance-based un-supervised classifiers have been previously used in the de-fect prediction literature with disappointing performance, connectivity-based classifiers have never been explored be-fore in our community. We compare the performance of unsupervised classifiers versus supervised classifiers using data from 26 projects from three publicly available datasets (i.e., AEEEM, NASA, and PROMISE). In the cross-project setting, our proposed con-nectivity-based classifier (via spectral clustering) ranks as one of the top classifiers among five widely-used supervised classifiers (i.e., random forest, naive Bayes, logistic regres-sion, decision tree, and logistic model tree) and five unsu-pervised classifiers (i.e., k-means, partition around medoids, fuzzy C-means, neural-gas, and spectral clustering). In the within-project setting (i.e., models are built and applied on the same project), our spectral classifier ranks in the second tier, while only random forest ranks in the first tier. Hence, connectivity-based unsupervised classifiers offer a viable so-lution for cross and within project defect predictions.},
  doi             = {10.1145/2884781.2884839},
  isbn            = {9781450339001},
  issn            = {0270-5257},
  keywords        = {cross-project,defect prediction,graph mining,heterogeneity,spectral clustering,unsupervised},
  mendeley-groups = {Untangle},
  url             = {http://dl.acm.org/citation.cfm?doid=2884781.2884839}
}

@article{zimmermann2005mining,
  title     = {Mining version histories to guide software changes},
  author    = {Zimmermann, Thomas and Zeller, Andreas and Weissgerber, Peter and Diehl, Stephan},
  journal   = {IEEE Transactions on Software Engineering},
  volume    = {31},
  number    = {6},
  pages     = {429--445},
  year      = {2005},
  publisher = {IEEE}
}

@article{DBLP:journals/corr/abs-1904-12787,
  author        = {Yujia Li and
               Chenjie Gu and
               Thomas Dullien and
               Oriol Vinyals and
               Pushmeet Kohli},
  title         = {Graph Matching Networks for Learning the Similarity of Graph Structured
               Objects},
  journal       = {CoRR},
  volume        = {abs/1904.12787},
  year          = {2019},
  url           = {http://arxiv.org/abs/1904.12787},
  archiveprefix = {arXiv},
  eprint        = {1904.12787},
  timestamp     = {Thu, 02 May 2019 15:13:44 +0200},
  biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1904-12787},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}
@article{domges1998adapting,
  title     = {Adapting traceability environments to project-specific needs},
  author    = {D{\"o}mges, Ralf and Pohl, Klaus},
  journal   = {Communications of the ACM},
  volume    = {41},
  number    = {12},
  pages     = {54--62},
  year      = {1998},
  publisher = {ACM New York, NY, USA}
}
@inproceedings{hindle2012naturalness,
  title        = {On the naturalness of software},
  author       = {Hindle, Abram and Barr, Earl T and Su, Zhendong and Gabel, Mark and Devanbu, Premkumar},
  booktitle    = {2012 34th International Conference on Software Engineering (ICSE)},
  pages        = {837--847},
  year         = {2012},
  organization = {IEEE}
}
@inproceedings{lakew2017multilingual,
  title     = {Multilingual Neural Machine Translation for Low Resource Languages.},
  author    = {Lakew, Surafel Melaku and Di Gangi, Mattia Antonino and Federico, Marcello},
  booktitle = {CLiC-it},
  year      = {2017}
}

@inproceedings{mader2009motivation,
  title        = {Motivation matters in the traceability trenches},
  author       = {Mader, Patrick and Gotel, Orlena and Philippow, Ilka},
  booktitle    = {2009 17th IEEE International Requirements Engineering Conference},
  pages        = {143--148},
  year         = {2009},
  organization = {IEEE}
}

@article{Prechelt2014,
  author    = {Lutz Prechelt and Alexander Pepper},
  title     = {Bflinks: Reliable Bugfix Links via Bidirectional References and Tuned Heuristics},
  publisher = {International Scholarly Research Notices},
  vol       = {2014},
  year      = {2014},
  doi       = {https://doi.org/10.1155/2014/701357}
}


@misc{SciPy,
  author = {Eric Jones and Travis Oliphant and Pearu Peterson and others},
  title  = {{SciPy}: Open source scientific tools for {Python}},
  year   = {2001--},
  url    = {http://www.scipy.org/},
  note   = {[Online; accessed 17.08.2018]}
} 

@book{smola1998learning,
  title     = {Learning with kernels},
  author    = {Smola, Alex J and Sch{\"o}lkopf, Bernhard},
  volume    = {4},
  year      = {1998},
  publisher = {Citeseer}
}

@Misc{sodump,
  author       = {{Stack Exchange}},
  howpublished = {\url{https://archive.org/details/stackexchange}},
  note         = {[Online; accessed 05-Sep-2018]},
  title        = {{Stack Exchange Data Dump}},
  year         = {2018},
}
@inproceedings{vaswani2017attention,
  title     = {Attention is all you need},
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle = {Advances in neural information processing systems},
  pages     = {5998--6008},
  year      = {2017}
}
@article{wang2019language,
  title   = {Language models with transformers},
  author  = {Wang, Chenguang and Li, Mu and Smola, Alexander J},
  journal = {arXiv preprint arXiv:1904.09408},
  year    = {2019}
}

@Misc{wikicorpus,
  author       = {{Wikipedia}},
  howpublished = {\url{https://en.wikipedia.org/wiki/Wikipedia:Database_download}},
  note         = {[Online; accessed 05-Sep-2018]},
  title        = {{Wikipedia:Database download}},
  year         = {2018},
}

@InProceedings{Abebe2010,
  author       = {Abebe, Surafel Lemma and Tonella, Paolo},
  booktitle    = {Program Comprehension (ICPC), 2010 IEEE 18th International Conference on},
  title        = {Natural language parsing of program element names for concept extraction},
  year         = {2010},
  organization = {IEEE},
  pages        = {156--159},
  doi          = {10.1109/icpc.2010.29},
}

@InProceedings{Baltes2018,
  author       = {Baltes, Sebastian and Dumani, Lorik and Treude, Christoph and Diehl, Stephan},
  booktitle    = {Proceedings of the 15th International Conference on Mining Software Repositories},
  title        = {{Sotorrent: Reconstructing and analyzing the evolution of {Stack} {Overflow} posts}},
  year         = {2018},
  organization = {ACM},
  pages        = {319--330},
  doi          = {10.1145/3196398.3196430},
}

@InProceedings{Banko:2004:PST:1220355.1220435,
  author    = {Banko, Michele and Moore, Robert C.},
  booktitle = {Proceedings of the 20th International Conference on Computational Linguistics},
  title     = {{Part of Speech Tagging in Context}},
  year      = {2004},
  address   = {Stroudsburg, PA, USA},
  publisher = {Association for Computational Linguistics},
  series    = {COLING '04},
  acmid     = {1220435},
  articleno = {556},
  doi       = {10.3115/1220355.1220435},
  location  = {Geneva, Switzerland},
  url       = {https://doi.org/10.3115/1220355.1220435},
}

@Article{Borg2014,
  author   = {Borg, Markus and Runeson, Per and Ard{\"o}, Anders},
  journal  = {Empirical Software Engineering},
  title    = {Recovering from a decade: a systematic mapping of information retrieval approaches to software traceability},
  year     = {2014},
  issn     = {1573-7616},
  month    = dec,
  number   = {6},
  pages    = {1565--1616},
  volume   = {19},
  abstract = {Engineers in large-scale software development have to manage large amounts of information, spread across many artifacts. Several researchers have proposed expressing retrieval of trace links among artifacts, i.e. trace recovery, as an Information Retrieval (IR) problem. The objective of this study is to produce a map of work on IR-based trace recovery, with a particular focus on previous evaluations and strength of evidence. We conducted a systematic mapping of IR-based trace recovery. Of the 79 publications classified, a majority applied algebraic IR models. While a set of studies on students indicate that IR-based trace recovery tools support certain work tasks, most previous studies do not go beyond reporting precision and recall of candidate trace links from evaluations using datasets containing less than 500 artifacts. Our review identified a need of industrial case studies. Furthermore, we conclude that the overall quality of reporting should be improved regarding both context and tool details, measures reported, and use of IR terminology. Finally, based on our empirical findings, we present suggestions on how to advance research on IR-based  trace recovery.},
  day      = {01},
  doi      = {10.1007/s10664-013-9255-y},
  url      = {https://doi.org/10.1007/s10664-013-9255-y},
}

@Article{Chen2017,
  author        = {Chen, Zhengdao and Li, Xiang and Bruna, Joan},
  title         = {{Supervised Community Detection with Hierarchical Graph Neural Networks}},
  year          = {2017},
  abstract      = {We study methods for supervised community detection on graphs. This estimation problem is typically formulated in terms of the spectrum of certain operators, as well as with posterior inference under certain probabilistic graphical models. Focusing on random graph families such as the Stochastic Block Model, recent research has unified both approaches, and identified both statistical and computational signal-to-noise detection thresholds. We identify the resulting class of algorithms with a family of Graph Neural Networks and show that they can reach those detection thresholds in a purely data-driven manner, without access to the underlying generative models and with no parameter assumptions. For that purpose, we propose to augment GNNs with the non-Backtracking operator, defined on the line graph of edge adjacencies. We also perform the first analysis of optimization landscape on a simplified GNN family, revealing an interesting transition from rugged to simple as the graph size increases. Finally, the resulting model is also tested on real datasets, performing significantly better than rigid parametric models.},
  archiveprefix = {arXiv},
  arxivid       = {1705.08415},
  eprint        = {1705.08415},
  eprinttype    = {arxiv},
  url           = {http://arxiv.org/abs/1705.08415},
}

@InProceedings{Constant:2011:MPT:2021121.2021134,
  author    = {Constant, Matthieu and Sigogne, Anthony},
  booktitle = {Proceedings of the Workshop on Multiword Expressions: From Parsing and Generation to the Real World},
  title     = {{MWU-aware part-of-speech Tagging with a CRF Model and Lexical Resources}},
  year      = {2011},
  address   = {Stroudsburg, PA, USA},
  pages     = {49--56},
  publisher = {Association for Computational Linguistics},
  series    = {MWE '11},
  acmid     = {2021134},
  isbn      = {978-1-932432-97-8},
  location  = {Portland, Oregon},
  numpages  = {8},
  url       = {http://dl.acm.org/citation.cfm?id=2021121.2021134},
}

@InProceedings{constant:hal-00790624,
  author      = {Constant, Mathieu and Tellier, Isabelle},
  booktitle   = {{8th International Conference on Language Resources and Evaluation (LREC'12)}},
  title       = {{Evaluating the Impact of External Lexical Resources into a CRF-based Multiword Segmenter and part-of-speech Tagger}},
  year        = {2012},
  address     = {Turkey},
  month       = may,
  pages       = {646--650},
  file        = {constant-tellier-lrec2012.pdf:https\://hal-upec-upem.archives-ouvertes.fr/hal-00790624/file/constant-tellier-lrec2012.pdf:PDF},
  hal_id      = {hal-00790624},
  hal_version = {v1},
  keywords    = {Multiword Expressions ; Part Of Speech tagging ; Statistical and machine learning methods ; Conditional Random Fields ; Morphosyntactic lexicons ; lexical segmentation},
  pdf         = {https://hal-upec-upem.archives-ouvertes.fr/hal-00790624/file/constant-tellier-lrec2012.pdf},
  url         = {https://hal-upec-upem.archives-ouvertes.fr/hal-00790624},
}

@InProceedings{dietrich2019man,
  author       = {Dietrich, Jens and Luczak-Roesch, Markus and Dalefield, Elroy},
  booktitle    = {Proceedings of the 16th International Conference on Mining Software Repositories},
  title        = {Man vs machine: a study into language identification of stack overflow code snippets},
  year         = {2019},
  organization = {IEEE Press},
  pages        = {205--209},
  doi          = {10.1109/msr.2019.00041},
}

@Article{Falessi2017,
  author   = {Falessi, Davide and Di Penta, Massimiliano and Canfora, Gerardo and Cantone, Giovanni},
  journal  = {Empirical Software Engineering},
  title    = {Estimating the number of remaining links in traceability recovery},
  year     = {2017},
  issn     = {1573-7616},
  month    = jun,
  number   = {3},
  pages    = {996--1027},
  volume   = {22},
  abstract = {Although very important in software engineering, establishing traceability links between software artifacts is extremely tedious, error-prone, and it requires significant effort. Even when approaches for automated traceability recovery exist, these provide the requirements analyst with a, usually very long, ranked list of candidate links that needs to be manually inspected. In this paper we introduce an approach called Estimation of the Number of Remaining Links (ENRL) which aims at estimating, via Machine Learning (ML) classifiers, the number of remaining positive links in a ranked list of candidate traceability links produced by a Natural Language Processing techniques-based recovery approach. We have evaluated the accuracy of the ENRL approach by considering several ML classifiers and NLP techniques on three datasets from industry and academia, and concerning traceability links among different kinds of software artifacts including requirements, use cases, design documents, source code, and test cases. Results from our study indicate that: (i) specific estimation models are able to provide accurate estimates of the number of remaining positive links; (ii) the estimation accuracy depends on the choice of the NLP technique, and (iii) univariate estimation models outperform multivariate ones.},
  day      = {01},
  doi      = {10.1007/s10664-016-9460-6},
  url      = {https://doi.org/10.1007/s10664-016-9460-6},
}

@Article{Gonen2018,
  author        = {Gonen, Hila and Goldberg, Yoav},
  title         = {{Language Modeling for Code-Switching: Evaluation, Integration of Monolingual Data, and Discriminative Training}},
  year          = {2018},
  abstract      = {We focus on the problem of language modeling for code-switched language, in the context of automatic speech recognition (ASR). Language modeling for code-switched language is challenging for (at least) three reasons: (1) lack of available large-scale code-switched data for training; (2) lack of a replicable evaluation setup that is ASR directed yet isolates language modeling performance from the other intricacies of the ASR system; and (3) the reliance on generative modeling. We tackle these three issues: we propose an ASR-motivated evaluation setup which is decoupled from an ASR system and the choice of vocabulary, and provide an evaluation dataset for English-Spanish code-switching. This setup lends itself to a discriminative training approach, which we demonstrate to work better than generative language modeling. Finally, we present an effective training protocol that integrates small amounts of code-switched data with large amounts of monolingual data, for both the generative and discriminative cases.},
  archiveprefix = {arXiv},
  arxivid       = {1810.11895},
  doi           = {10.18653/v1/d19-1427},
  eprint        = {1810.11895},
  eprinttype    = {arxiv},
  url           = {http://arxiv.org/abs/1810.11895},
}

@InProceedings{Haiduc2010a,
  author    = {Haiduc, Sonia and Aponte, Jairo and Marcus, Andrian},
  booktitle = {Proceedings of the 32Nd ACM/IEEE International Conference on Software Engineering - Volume 2},
  title     = {{Supporting Program Comprehension with Source Code Summarization}},
  year      = {2010},
  address   = {New York, NY, USA},
  pages     = {223--226},
  publisher = {ACM},
  series    = {ICSE '10},
  acmid     = {1810335},
  doi       = {10.1145/1810295.1810335},
  isbn      = {978-1-60558-719-6},
  keywords  = {program comprehension, summary, text summarization},
  location  = {Cape Town, South Africa},
  numpages  = {4},
  url       = {http://doi.acm.org/10.1145/1810295.1810335},
}

@InProceedings{Haiduc2010b,
  author    = {S. Haiduc and J. Aponte and L. Moreno and A. Marcus},
  booktitle = {2010 17th Working Conference on Reverse Engineering},
  title     = {{On the Use of Automated Text Summarization Techniques for Summarizing Source Code}},
  year      = {2010},
  month     = oct,
  pages     = {35--44},
  abstract  = {During maintenance developers cannot read the entire code of large systems. They need a way to get a quick understanding of source code entities (such as, classes, methods, packages, etc.), so they can efficiently identify and then focus on the ones related to their task at hand. Sometimes reading just a method header or a class name does not tell enough about its purpose and meaning, while reading the entire implementation takes too long. We study a solution which mitigates the two approaches, i.e., short and accurate textual descriptions that illustrate the software entities without having to read the details of the implementation. We create such descriptions using techniques from automatic text summarization. The paper presents a study that investigates the suitability of various such techniques for generating source code summaries. The results indicate that a combination of text summarization techniques is most appropriate for source code summarization and that developers generally agree with the summaries produced.},
  doi       = {10.1109/WCRE.2010.13},
  issn      = {2375-5369},
  keywords  = {software maintenance;automated text summarization technique;textual description;software entity;source code summarization;Lead;Large scale integration;Semantics;Software systems;Computer science;Correlation;text summarization;program comprehension},
}

@InProceedings{Hellendoorn2018,
  author          = {Hellendoorn, Vincent J and Bird, Christian and Barr, Earl T and Allamanis, Miltiadis},
  booktitle       = {Proc. 2018 26th ACM Jt. Meet. Eur. Softw. Eng. Conf. Symp. Found. Softw. Eng. - ESEC/FSE 2018},
  title           = {{Deep learning type inference}},
  year            = {2018},
  pages           = {152--162},
  publisher       = {ACM},
  volume          = {18},
  abstract        = {Dynamically typed languages such as JavaScript and Python are increasingly popular, yet static typing has not been totally eclipsed: Python now supports type annotations and languages like TypeScript offer a middle-ground for JavaScript: a strict superset of JavaScript, to which it transpiles, coupled with a type system that permits partially typed programs. However, static typing has a cost: adding annotations, reading the added syntax, and wrestling with the type system to fix type errors. Type inference can ease the transition to more statically typed code and unlock the benefits of richer compile-time information, but is limited in languages like JavaScript as it cannot soundly handle duck-typing or runtime evaluation via eval. We propose DeepTyper, a deep learning model that understands which types naturally occur in certain contexts and relations and can provide type suggestions, which can often be verified by the type checker, even if it could not infer the type initially. DeepTyper, leverages an automatically aligned corpus of tokens and types to accurately predict thousands of variable and function type annotations. Furthermore, we demonstrate that context is key in accurately assigning these types and introduce a technique to reduce overfitting on local cues while highlighting the need for further improvements. Finally, we show that our model can interact with a compiler to provide more than 4,000 additional type annotations with over 95{\%} precision that could not be inferred without the aid of DeepTyper. CCS CONCEPTS  Software and its engineering  Software notations and tools; Automated static analysis;  Theory of computation  Type structures; KEYWORDS},
  doi             = {10.1145/3236024.3236051},
  mendeley-groups = {Code pseudo-PoS tagging},
  url             = {https://doi.org/10.1145/3236024.3236051 http://dl.acm.org/citation.cfm?doid=3236024.3236051},
}

@Article{Huang2015,
  author        = {Huang, Zhiheng and Xu, Wei and Yu, Kai},
  title         = {{Bidirectional LSTM-CRF Models for Sequence Tagging}},
  year          = {2015},
  issn          = {0270-6474},
  abstract      = {In this paper, we propose a variety of Long Short-Term Memory (LSTM) based models for sequence tagging. These models include LSTM networks, bidirectional LSTM (BI-LSTM) networks, LSTM with a Conditional Random Field (CRF) layer (LSTM-CRF) and bidirectional LSTM with a CRF layer (BI-LSTM-CRF). Our work is the first to apply a bidirectional LSTM CRF (denoted as BI-LSTM-CRF) model to NLP benchmark sequence tagging data sets. We show that the BI-LSTM-CRF model can efficiently use both past and future input features thanks to a bidirectional LSTM component. It can also use sentence level tag information thanks to a CRF layer. The BI-LSTM-CRF model can produce state of the art (or close to) accuracy on POS, chunking and NER data sets. In addition, it is robust and has less dependence on word embedding as compared to previous observations.},
  archiveprefix = {arXiv},
  arxivid       = {1508.01991},
  doi           = {10.1061/(ASCE)CO.1943-7862.0000274.},
  eprint        = {1508.01991},
  eprinttype    = {arxiv},
  isbn          = {9781510827585},
  pmid          = {3944616},
  url           = {http://arxiv.org/abs/1508.01991},
}

@InProceedings{Jamatia2015,
  author    = {Jamatia, Anupam and Gamb{\"a}ck, Bj{\"o}rn and Das, Amitava},
  booktitle = {Proceedings of the International Conference Recent Advances in Natural Language Processing},
  title     = {part-of-speech tagging for code-mixed english-hindi twitter and facebook chat messages},
  year      = {2015},
  pages     = {239--248},
}

@Article{Kersten2005,
  author   = {Kersten, Mik and Murphy, Gail C},
  journal  = {Proceedings of the 4th international conference on Aspect-oriented software development - AOSD '05},
  title    = {{Mylar}},
  year     = {2005},
  issn     = {0740-7459},
  pages    = {159--168},
  abstract = {Even when working on a well-modularized software system, programmers tend to spend more time navigating the code than working with it. This phenomenon arises because it is impossible to modularize the code for all tasks that occur over the lifetime of a system. We describe the use of a degree-of-interest (DOI) model to capture the task context of program elements scattered across a code base. The Mylar tool that we built encodes the DOI of program elements by monitoring the programmer's activity, and displays the encoded DOI model in views of Java and AspectJ programs. We also present the results of a preliminary diary study in which professional programmers used Mylar for their daily work on enterprise-scale Java systems.},
  doi      = {10.1145/1052898.1052912},
  isbn     = {1595930434},
  keywords = {a cohesive,for instance,in the code,may need to examine,programmers must,repeatedly visit multiple places,system,the code corresponding to,they,to a large software,to make a change},
}

@Article{Kingma14,
  author    = {Kingma, Diederik P. and Ba, Jimmy},
  journal   = {CoRR},
  title     = {{Adam: A Method for Stochastic Optimization.}},
  year      = {2014},
  volume    = {abs/1412.6980},
  added-at  = {2018-08-13T00:00:00.000+0200},
  ee        = {http://arxiv.org/abs/1412.6980},
  keywords  = {dblp},
  timestamp = {2018-08-14T14:24:27.000+0200},
  url       = {http://dblp.uni-trier.de/db/journals/corr/corr1412.html\#KingmaB14},
}

@Article{Knyazev2007,
  author          = {Knyazev, A V and Argentati, M E and Lashuk, I. and Ovtchinnikov, E E},
  journal         = {SIAM Journal on Scientific Computing},
  title           = {{Block Locally Optimal Preconditioned Eigenvalue Xolvers (BLOPEX) in Hypre and PETSc}},
  year            = {2007},
  issn            = {1064-8275},
  number          = {5},
  pages           = {2224--2239},
  volume          = {29},
  abstract        = {We describe our software package Block Locally Optimal Preconditioned Eigenvalue Xolvers (BLOPEX) publicly released recently. BLOPEX is available as a stand-alone serial library, as an external package to PETSc (``Portable, Extensible Toolkit for Scientific Computation'', a general purpose suite of tools for the scalable solution of partial differential equations and related problems developed by Argonne National Laboratory), and is also built into {\{}$\backslash$it hypre{\}} (``High Performance Preconditioners'', scalable linear solvers package developed by Lawrence Livermore National Laboratory). The present BLOPEX release includes only one solver--the Locally Optimal Block Preconditioned Conjugate Gradient (LOBPCG) method for symmetric eigenvalue problems. {\{}$\backslash$it hypre{\}} provides users with advanced high-quality parallel preconditioners for linear systems, in particular, with domain decomposition and multigrid preconditioners. With BLOPEX, the same preconditioners can now be efficiently used for symmetric eigenvalue problems. PETSc facilitates the integration of independently developed application modules with strict attention to component interoperability, and makes BLOPEX extremely easy to compile and use with preconditioners that are available via PETSc. We present the LOBPCG algorithm in BLOPEX for {\{}$\backslash$it hypre{\}} and PETSc. We demonstrate numerically the scalability of BLOPEX by testing it on a number of distributed and shared memory parallel systems, including a Beowulf system, SUN Fire 880, an AMD dual-core Opteron workstation, and IBM BlueGene/L supercomputer, using PETSc domain decomposition and {\{}$\backslash$it hypre{\}} multigrid preconditioning. We test BLOPEX on a model problem, the standard 7-point finite-difference approximation of the 3-D Laplacian, with the problem size in the range {\$}10{\^{}}5-10{\^{}}8{\$}.},
  archiveprefix   = {arXiv},
  arxivid         = {0705.2626},
  doi             = {10.1137/060661624},
  eprint          = {0705.2626},
  isbn            = {1064827500},
  keywords        = {BLOPEX,Beowulf.,BlueGene,Conjugate gradient,LOBPCG,Laplacian,PETSc,domain decomposition,eigenvalue,eigenvector,hypre,iterative method,multigrid,parallel computing,preconditioner,preconditioning},
  mendeley-groups = {Untangle},
  url             = {http://math.cudenver.edu/ http://epubs.siam.org/doi/10.1137/060661624},
}

@Article{Krzakala2013,
  author    = {Krzakala, Florent and Moore, Cristopher and Mossel, Elchanan and Neeman, Joe and Sly, Allan and Zdeborov{\'a}, Lenka and Zhang, Pan},
  journal   = {Proceedings of the National Academy of Sciences},
  title     = {Spectral redemption in clustering sparse networks},
  year      = {2013},
  issn      = {0027-8424},
  number    = {52},
  pages     = {20935--20940},
  volume    = {110},
  abstract  = {Spectral algorithms are widely applied to data clustering problems, including finding communities or partitions in graphs and networks. We propose a way of encoding sparse data using a {\textquotedblleft}nonbacktracking{\textquotedblright} matrix, and show that the corresponding spectral algorithm performs optimally for some popular generative models, including the stochastic block model. This is in contrast with classical spectral algorithms, based on the adjacency matrix, random walk matrix, and graph Laplacian, which perform poorly in the sparse case, failing significantly above a recently discovered phase transition for the detectability of communities. Further support for the method is provided by experiments on real networks as well as by theoretical arguments and analogies from probability theory, statistical physics, and the theory of random matrices.Spectral algorithms are classic approaches to clustering and community detection in networks. However, for sparse networks the standard versions of these algorithms are suboptimal, in some cases completely failing to detect communities even when other algorithms such as belief propagation can do so. Here, we present a class of spectral algorithms based on a nonbacktracking walk on the directed edges of the graph. The spectrum of this operator is much better-behaved than that of the adjacency matrix or other commonly used matrices, maintaining a strong separation between the bulk eigenvalues and the eigenvalues relevant to community structure even in the sparse case. We show that our algorithm is optimal for graphs generated by the stochastic block model, detecting communities all of the way down to the theoretical limit. We also show the spectrum of the nonbacktracking operator for some real-world networks, illustrating its advantages over traditional spectral clustering.},
  doi       = {10.1073/pnas.1312486110},
  eprint    = {http://www.pnas.org/content/110/52/20935.full.pdf},
  publisher = {National Academy of Sciences},
  url       = {http://www.pnas.org/content/110/52/20935},
}

@Article{Kuhn1955,
  author   = {Kuhn, H. W.},
  journal  = {Naval Research Logistics Quarterly},
  title    = {{The Hungarian method for the assignment problem}},
  number   = {1--2},
  pages    = {83--97},
  volume   = {2},
  abstract = {Abstract Assuming that numerical scores are available for the performance of each of n persons on each of n jobs, the assignment problem is the quest for an assignment of persons to jobs so that the sum of the n scores so obtained is as large as possible. It is shown that ideas latent in the work of two Hungarian mathematicians may be exploited to yield a new method of solving this problem.},
  doi      = {10.1002/nav.3800020109},
  eprint   = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/nav.3800020109},
  url      = {https://onlinelibrary.wiley.com/doi/abs/10.1002/nav.3800020109},
}

@InProceedings{LDATime2,
  author    = {Hindle, Abram and Godfrey, Michael W. and Holt, Richard C.},
  booktitle = {2009 IEEE International Conference on Software Maintenance},
  title     = {{What's hot and what's not: Windowed developer topic analysis}},
  year      = {2009},
  month     = sep,
  pages     = {339--348},
  abstract  = {As development on a software project progresses, developers shift their focus between different topics and tasks many times. Managers and newcomer developers often seek ways of understanding what tasks have recently been worked on and how much effort has gone into each; for example, a manager might wonder what unexpected tasks occupied their team's attention during a period when they were supposed to have been implementing new features. Tools such as Latent Dirichlet Allocation (LDA) and Latent Semantic Indexing (LSI) can be used to extract a set of independent topics from a corpus of commit-log comments. Previous work in the area has created a single set of topics by analyzing comments from the entire lifetime of the project. In this paper, we propose windowing the topic analysis to give a more nuanced view of the system's evolution. By using a defined time-window of, for example, one month, we can track which topics come and go over time, and which ones recur. We propose visualizations of this model that allows us to explore the evolving stream of topics of development occurring over time. We demonstrate that windowed topic analysis offers advantages over topic analysis applied to a project's lifetime because many topics are quite local.},
  doi       = {10.1109/ICSM.2009.5306310},
  isbn      = {9781424448289},
  issn      = {1063-6773},
  journal   = {IEEE International Conference on Software Maintenance, ICSM},
  keywords  = {program visualisation;programming language semantics;software engineering;word processing;commit-log comments;latent dirichlet allocation tool;latent semantic indexing tool;software developers;software project development;windowed topic analysis;Control systems;Database systems;History;Indexing;Large scale integration;Linear discriminant analysis;Pattern analysis;Performance analysis;Software development management;Visualization},
}

@InProceedings{Mills:2017:ATL:3106237.3121280,
  author    = {Mills, Chris},
  booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
  title     = {{Automating Traceability Link Recovery Through Classification}},
  year      = {2017},
  address   = {New York, NY, USA},
  pages     = {1068--1070},
  publisher = {ACM},
  series    = {ESEC/FSE 2017},
  acmid     = {3121280},
  doi       = {10.1145/3106237.3121280},
  isbn      = {978-1-4503-5105-8},
  keywords  = {Classification, Machine Learning, Traceability},
  location  = {Paderborn, Germany},
  numpages  = {3},
  url       = {http://doi.acm.org/10.1145/3106237.3121280},
}

@InProceedings{ponzanelli2014improving,
  author       = {Ponzanelli, Luca and Mocci, Andrea and Bacchelli, Alberto and Lanza, Michele and Fullerton, David},
  booktitle    = {2014 IEEE International Conference on Software Maintenance and Evolution},
  title        = {Improving low quality stack overflow post detection},
  year         = {2014},
  organization = {IEEE},
  pages        = {541--544},
}

@Article{relink,
  author    = {Wu, Rongxin and Zhang, Hongyu and Kim, Sunghun and Cheung, Shing-Chi},
  journal   = {Proceedings of the 19th ACM SIGSOFT Symposium and the 13th European Conference on Foundations of Software Engineering},
  title     = {{ReLink: Recovering Links Between Bugs and Changes}},
  year      = {2011},
  pages     = {15--25},
  acmid     = {2025120},
  address   = {New York, NY, USA},
  booktitle = {Proceedings of the 19th ACM SIGSOFT Symposium and the 13th European Conference on Foundations of Software Engineering},
  doi       = {10.1145/2025113.2025120},
  isbn      = {978-1-4503-0443-6},
  keywords  = {bugs, changes, data quality, mining software repository, missing links},
  location  = {Szeged, Hungary},
  numpages  = {11},
  publisher = {ACM},
  series    = {ESEC/FSE '11},
  url       = {http://doi.acm.org/10.1145/2025113.2025120},
}

@Article{Saade2014,
  author        = {Saade, Alaa and Krzakala, Florent and Zdeborov{\'{a}}, Lenka},
  title         = {{Spectral Clustering of Graphs with the Bethe Hessian}},
  year          = {2014},
  issn          = {1049-5258},
  pages         = {1--9},
  abstract      = {Spectral clustering is a standard approach to label nodes on a graph by studying the (largest or lowest) eigenvalues of a symmetric real matrix such as e.g. the adjacency or the Laplacian. Recently, it has been argued that using instead a more complicated, non-symmetric and higher dimensional operator, related to the non-backtracking walk on the graph, leads to improved performance in detecting clusters, and even to optimal performance for the stochastic block model. Here, we propose to use instead a simpler object, a symmetric real matrix known as the Bethe Hessian operator, or deformed Laplacian. We show that this approach combines the performances of the non-backtracking operator, thus detecting clusters all the way down to the theoretical limit in the stochastic block model, with the computational, theoretical and memory advantages of real symmetric matrices.},
  archiveprefix = {arXiv},
  arxivid       = {1406.1880},
  eprint        = {1406.1880},
  eprinttype    = {arxiv},
  url           = {http://arxiv.org/abs/1406.1880},
}

@InProceedings{Solorio2008,
  author       = {Solorio, Thamar and Liu, Yang},
  booktitle    = {Proceedings of the Conference on Empirical Methods in Natural Language Processing},
  title        = {{part-of-speech tagging for English-Spanish code-switched text}},
  year         = {2008},
  organization = {Association for Computational Linguistics},
  pages        = {1051--1060},
}

@Article{StableRoommateProblem,
  author     = {Irving, Robert W. and Manlove, David F.},
  journal    = {J. Algorithms},
  title      = {{The Stable Roommates Problem with Ties}},
  year       = {2002},
  issn       = {0196-6774},
  month      = apr,
  number     = {1},
  pages      = {85--105},
  volume     = {43},
  acmid      = {589924},
  address    = {Duluth, MN, USA},
  doi        = {10.1006/jagm.2002.1219},
  issue_date = {April 2002},
  keywords   = {NP-completeness, approximation algorithm, indifference, linear-time algorithm, stable matching problem, super-stability, weak stability},
  numpages   = {21},
  publisher  = {Academic Press, Inc.},
  url        = {http://dx.doi.org/10.1006/jagm.2002.1219},
}

@Article{Stahl2017,
  author          = {St{\aa}hl, Daniel and Hall{\'{e}}n, Kristofer and Bosch, Jan},
  journal         = {Empir. Softw. Eng.},
  title           = {{Achieving traceability in large scale continuous integration and delivery deployment, usage and validation of the eiffel framework}},
  year            = {2017},
  issn            = {1573-7616},
  number          = {3},
  pages           = {967--995},
  volume          = {22},
  abstract        = {The importance of traceability in software development has long been recognized, not only for reasons of legality and certification, but also to enable the development itself. At the same time, organizations are known to struggle to live up to traceability requirements, and there is an identified lack of studies on traceability practices in the industry, not least in the area of tooling and infrastructure. This paper presents, investigates and discusses Eiffel, an industry developed solution designed to provide real time traceability in continuous integration and delivery. The traceability needs of industry professionals are also investigated through interviews, providing context to that solution. It is then validated through further interviews, a comparison with previous traceability methods and a review of literature. It is found to address the identified traceability needs and found in some cases to reduce traceability data acquisition times from days to minutes, while at the same time alternatives offering comparable functionality are lacking. In this work, traceability is shown not only to be an important concern to engineers, but also regarded as a prerequisite to successful large scale continuous integration and delivery. At the same time, promising developments in technical infrastructure are documented and clear differences in traceability mindset between separate industry projects is revealed.},
  doi             = {10.1007/s10664-016-9457-1},
  keywords        = {Continuous delivery,Continuous integration,Traceability,Very-large-scale software systems},
  mendeley-groups = {Aide-m{\'{e}}moire},
}

@InProceedings{Tian2015,
  author       = {Tian, Yuan and Lo, David},
  booktitle    = {2015 IEEE 22nd International Conference on Software Analysis, Evolution and Reengineering (SANER)},
  title        = {A comparative study on the effectiveness of part-of-speech tagging techniques on bug reports},
  year         = {2015},
  organization = {IEEE},
  pages        = {570--574},
}

@InProceedings{Treude:2015:TTN:2819009.2819128,
  author    = {Treude, Christoph and Sicard, Mathieu and Klocke, Marc and Robillard, Martin},
  booktitle = {Proceedings of the 37th International Conference on Software Engineering - Volume 2},
  title     = {{TaskNav: Task-based Navigation of Software Documentation}},
  year      = {2015},
  address   = {Piscataway, NJ, USA},
  pages     = {649--652},
  publisher = {IEEE Press},
  series    = {ICSE '15},
  acmid     = {2819128},
  location  = {Florence, Italy},
  numpages  = {4},
  url       = {http://dl.acm.org/citation.cfm?id=2819009.2819128},
}

@InProceedings{Treude2015portuguese,
  author       = {Treude, Christoph and Prolo, Carlos A and Figueira Filho, Fernando},
  booktitle    = {Proceedings of the 29th Brazilian Symposium on Software Engineering},
  title        = {Challenges in analyzing software documentation in {Portuguese}},
  year         = {2015},
  organization = {IEEE},
  pages        = {179--184},
}

@Article{Trivedi2018,
  author          = {Trivedi, Rakshit and Sisman, Bunyamin and Dong, Xin Luna and Faloutsos, Christos and Ma, Jun and Zha, Hongyuan},
  journal         = {Proc. 56th Annu. Meet. Assoc. Comput. Linguist. (Volume 1 Long Pap.},
  title           = {{LinkNBed: Multi-Graph Representation Learning with Entity Linkage}},
  year            = {2018},
  pages           = {252--262},
  archiveprefix   = {arXiv},
  arxivid         = {1807.08447},
  doi             = {arXiv:1807.08447v1},
  eprint          = {1807.08447},
  mendeley-groups = {Untangle},
  url             = {http://aclweb.org/anthology/P18-1024},
}

@InProceedings{Vyas2014,
  author    = {Vyas, Yogarshi and Gella, Spandana and Sharma, Jatin and Bali, Kalika and Choudhury, Monojit},
  booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  title     = {Pos tagging of english-hindi code-mixed social media content},
  year      = {2014},
  pages     = {974--979},
}

@Article{Wallach2009,
  author   = {Wallach, Hanna M and Mimno, David and Mccallum, Andrew},
  journal  = {Advances in Neural Information Processing Systems 22},
  title    = {{Rethinking LDA : Why Priors Matter}},
  year     = {2009},
  issn     = {0269-9370},
  number   = {2},
  pages    = {1973--1981},
  volume   = {22},
  abstract = {Implementations of topic models typically use symmetric Dirichlet priors with fixed concentration parameters, with the implicit assumption that such smoothing parameters have little practical effect. In this paper, we explore several classes of structured priors for topic models. We find that an asymmetric Dirichlet prior over the documenttopic distributions has substantial advantages over a symmet- ric prior, while an asymmetric prior over the topicword distributions provides no real benefit. Approximation of this prior structure through simple, efficient hy- perparameter optimization steps is sufficient to achieve these performance gains. The prior structure we advocate substantially increases the robustness of topic models to variations in the number of topics and to the highly skewed word fre- quency distributions common in natural language. Since this prior structure can be implemented using efficient algorithms that add negligible cost beyond standard inference techniques, we recommend it as a new standard for topic modeling.},
  doi      = {10.1007/s10708-008-9161-9},
  isbn     = {9781615679119},
  pmid     = {2508715},
}

@Article{weka,
  author     = {Hall, Mark and Frank, Eibe and Holmes, Geoffrey and Pfahringer, Bernhard and Reutemann, Peter and Witten, Ian H.},
  journal    = {SIGKDD Explor. Newsl.},
  title      = {{The WEKA Data Mining Software: An Update}},
  year       = {2009},
  issn       = {1931-0145},
  month      = nov,
  number     = {1},
  pages      = {10--18},
  volume     = {11},
  acmid      = {1656278},
  address    = {New York, NY, USA},
  doi        = {10.1145/1656274.1656278},
  issue_date = {June 2009},
  numpages   = {9},
  publisher  = {ACM},
  url        = {http://doi.acm.org/10.1145/1656274.1656278},
}

@InProceedings{winata-charmodel,
  author    = {Winata, Genta Indra and Wu, Chien-Sheng and Madotto, Andrea and Fung, Pascale},
  booktitle = {Proceedings of the Third Workshop on Computational Approaches to Linguistic Code-Switching},
  title     = {{Bilingual Character Representation for Efficiently Addressing Out-of-Vocabulary Words in Code-Switching Named Entity Recognition}},
  year      = {2018},
  address   = {Melbourne, Australia},
  month     = jul,
  pages     = {110--114},
  publisher = {Association for Computational Linguistics},
  doi       = {10.18653/v1/W18-3214},
  url       = {https://www.aclweb.org/anthology/W18-3214},
}

@InProceedings{Yasunaga&al.18.naacl,
  author    = {Michihiro Yasunaga and Jungo Kasai and Dragomir R. Radev},
  booktitle = {Proceedings of NAACL},
  title     = {{Robust Multilingual part-of-speech Tagging via Adversarial Training}},
  year      = {2018},
  publisher = {Association for Computational Linguistics},
}

@Comment{jabref-meta: databaseType:bibtex;}
