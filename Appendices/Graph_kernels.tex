\chapter{Graph Kernels}
\label{appendix:graph_kernels}

A kernel is a symmetric function $k: X \times X \rightarrow \mathbb{R}$ on
non-empty sets $X$. A kernel is positive semi-definite if:
\begin{align*}
	&\sum_{i=1}^{n} \sum_{j=1}^{n} c_i c_j k(x_i, x_j) \geq 0, \\
	&\forall n \in \mathbb{N}, x_1, \ldots, x_n \in X, c_1, \ldots, c_n \in \mathbb{R}. 
\end{align*}
The function $k$ can take the arguments in either order and, for any
parametrization by real constants, has non-negative weighted sum over all
inputs. A graph kernel is a positive semi-definite kernel on a set of graphs.

Now, let us consider a collection of graphs $\mathcal{G}$, where each graph $G
\in \mathcal{G}$ has vertexes from an abstract vertex space $\mathcal{V}$.
Further, let $\mathcal{L}_v$ be the space of node labels, and let $lv:
\mathcal{V} \rightarrow \mathcal{L}_v$ the graph labelling function. Similarly,
for edges let $\mathcal{L}_e$ be the space of edge labels, with a similarly
defined $le$. We will now consider a kernel function to be a function $k:
\mathcal{G} \rightarrow \mathbb{R}^{|\mathcal{G}|} \times
\mathbb{R}^{|\mathcal{G}|}$, that is one that returns the matrix of pair wise
similarities of the graphs in the collection $\mathcal{G}$. In particular:
\begin{equation}
\label{eq:kernel_general}
    k(\mathcal{G})_{ij} = \langle\psi(G_i), \psi(G_j)\rangle,
\end{equation}
%
where $\psi$ maps a graph to a vector space and $\langle\cdot,\cdot\rangle$
represents the inner product. This is often referred to as a kernel trick.

\section{Vertex and Edge Histogram Kernels}
\label{appendix:graph_kernels:histogram}
Let us now consider a few simple Graph Kernels: Vertex/Edge Histogram Kernels.
For the Vertex Histogram Kernel, let $n=|\mathcal{L}_v|$, and let $\psi$ return
length $n$ vectors $\mathbf{f}$, such that the $i$-th represents the count of
the $i$-th vertex label in the graph, \ie: 
\begin{equation}
    \psi(G) = \mathbf{f},
\end{equation}
\begin{equation}
    \mathbf{f}_i=|\{v \mid v \in V, lv(v) = \mathcal{L}_{vi}\}|.
\end{equation}
%
The matrix generated by our kernel function is then populated by the inner
product of these vertex label count vectors.

We similarly define such a kernel for Edge Histograms by substituting
$\mathcal{L}_e$ for $\mathcal{L}_v$ above, and $le$ for $lv$. Let the count
vectors generated by edge histograms be $\mathbf{g}$.

We can define a third kernel by considering the kernel induced by considering:
\begin{equation}
    k(\mathcal{G})_{ij} = \langle[\mathbf{f},\mathbf{g}]_i, [\mathbf{f},\mathbf{g}]_j\rangle,
\end{equation}
%
where $[\cdot,\cdot]$ represents vector concatenation. That is, we consider the
inner product of both vertex and edge histograms when computing the inner
product. This defines the Vertex-Edge-Histogram kernel.

For each of the three presented kernels, Vertex Histogram, Edge Histogram and
Vertex-Edge Histogram, we can obtain the Gaussian RBF version of the kernel as
follows. Let $\mathbf{h} = \mathbf{f}$, $\mathbf{h} = \mathbf{g}$, or
$\mathbf{h} = \mathbf{[\mathbf{f}, \mathbf{g}]}$ as appropriate for the
respective kernel. The Gaussian version of the kernels then becomes:
\begin{equation}
    k(\mathcal{G})_{ij} = \text{exp}\left(-\frac{\norm{\mathbf{h}_i - \mathbf{h}_j}}{2\sigma^2}\right),
\end{equation}
%
where $\sigma^2$ is the variance of our Gaussian RBF kernel.


\section{Random Walk Kernel}
\label{appendix:graph_kernels:random_walk}

Let us now consider Random Walk based kernels. For the $\kappa$-step Random Walk
kernel, we consider as input a vector of weights $(\lambda_0, \lambda_1, \dots,
\lambda_\kappa)$ where each $\lambda_i$ is a real positive value and define the
kernel as:
\begin{equation}
    k(\mathcal{G})_{ij} = \sum\limits_{i',j'=1}^{|V_{\times}|}\left[\sum\limits_{t=0}^{\kappa}\lambda_tA_{\times}^{t}\right]_{i'j'},
\end{equation}
%
where $A_{\times}$ represents the adjacency matrix of the direct tensor product
$G_{\times} = (V_{\times}, E_{\times}, \text{lv}_{\times}, \text{le}_{\times})$
of $G_i = (V_i, E_i, \text{lv}_i, \text{le}_i)$ and $G_j = (V_j, E_j,
\text{lv}_j, \text{le}_j)$ \st:
\[
V_{\times} = \{ (v_i, v_j) \in V_i \times V_j \mid \text{lv}_i(v_i) = \text{lv}_j(v_j) \},
\]
\[
E_{\times} = \{ ((u_i, u_j), (v_i, v_j)) \in V_{\times} \times V_{\times} \mid \text{le}_i((u_i, v_i)) = \text{le}_j((u_j, v_j)) \},
\]
%
and labels are propagated \ie:
\[
\text{lv}_{\times}((v_i, v_j)) = \text{lv}_i(v_i) = \text{lv}_j(v_j),
\]
\[
\text{le}_{\times}(((u_i, u_j), (v_i, v_j))) = \text{le}_i((u_i, v_i)) = \text{le}_j((u_j, v_j)).
\]

For the Graphlet kernel, for each graph $G_i$ consider the vector $\mathbf{l_i}
= (l_i^1, l_i^2, \dots, l_i^s)$ such that $l_i^k$ records the number of
embeddings of Graphlet $L_k$ into $G_i$. Each $L_k, 	\forall k \in [1..s]$ is a
graph with $n$ nodes. We say that $L = (V_L, E_L)$ can be embedded into $G$ if
there exists an injective $\alpha: V_L \rightarrow V$ \st $(v, w) \in E_L \iff
(\alpha(v), \alpha(w)) \in E$. Similar to previous kernel formulations, we
define $\psi(G_i) = \mathbf{l_i}$ and thus our kernel:
\begin{equation}
    k(\mathcal{G})_{ij} = \langle\mathbf{l_i}, \mathbf{l_j}\rangle
\end{equation}
%
Due to high computational costs, implementations of this kernel ignore Vertex
and Edge labels. For more details see Shervashidze
\etal~\cite{shervashidze2009}.

\section{Weisfeiler-Lehman Kernel}
\label{appendix:graph_kernels:wl}

For the Weisfeiler Lehman sub-tree kernel, consider the following. The labelling
function $lv$ associated with a graph $G=(V,E)$, represents the labelling
function $lv_0$, the labelling function of 0 steps of Weisfeiler Lehman. To
perform a step, we replace the set of labels of a node $v$, which starts as the
unit set contain $lv_0(v)$, with the set containing the current label and the
sorted set of labels of the neighbours of $v$. The multi-set label is then
compressed to a short label. We thus obtain $lv_{i+1}$ from $lv_i$. This process
is repeated for $h$ iterations. In particular, nodes obtain identical new labels
only if they have identical multi-set labels. Due to obtaining a sequence of
labelling functions, we also generate a sequence of length $h$ of graphs $(G_0,
G_1, \ldots, G_h)$. We can now define our WL kernels as:
\begin{equation}
    k_{WL}(\mathcal{G}) = k(\mathcal{G}_0) + k(\mathcal{G}_1) + \ldots + k(\mathcal{G}_h) 
\end{equation}
%
In \Cref{chapter:flexeme}, we combine the WL kernel formulation with the Subtree
kernel. This kernel is defined as follows. Let $\Sigma_i \subseteq \Sigma$ be
the alphabet of all letters that occur in the labelling of nodes in graphs up
the the $i$-th iteration of WL. We assume all $\Sigma_i$ are pair-wise disjoint.
Further and without loss of generality, let $\Sigma_i = {\sigma_{i0},
\sigma_{i|\Sigma_i|}}$ be ordered sets. Let $c_i: {G, G'} \times \Sigma_i
\rightarrow \mathbb{N}$ be a count function, \st $c(G, \sigma_{ij})$ returns the
number of occurrence of $\sigma_{ij}$ in $G$ as a node label. We will now build
our vector as:
\begin{equation}
\label{eq:wl_vector}
    \psi(G) = (c_0(G, \sigma_{00}), \ldots, c_0(G, \sigma_{0|\Sigma_0|}), \ldots, c_h(G, \sigma_{h|\Sigma_h|})
\end{equation}
%
We obtain the similarity matrix, by replacing $\psi$ from \Cref{eq:wl_vector} in
\Cref{eq:kernel_general}.

