\chapter{Graph Kernels}
\label{appendix:graph_kernels}

In this Appendix, I will first define a kernel and a graph kernel. I will then
show some of the earlier examples of kernels: from Vertex Label Histogram
Kernels through to Random-Walk and Graphlet Kernels. I will then detail the
general Weisfeiler-Lehman graph kernel framework and show how to instantiate it
as a Subtree Graph Kernel. Kernels in general, and graph kernels being no
exception, are used to allow machine learning approaches that make use of
similarity or (pseudo-)metrics to learn, such as clustering. We use the
Weisfeiler-Lehman Subtree Graph Kernel in \Cref{chapter:flexeme} to allow
re-clustering \deltaPDGN graphs into atomic patches.

A kernel is a symmetric function $k: X \times X \rightarrow \mathbb{R}$ on
non-empty sets $X$. A kernel is positive semi-definite if:
\begin{align*}
	&\sum_{i=1}^{n} \sum_{j=1}^{n} c_i c_j k(x_i, x_j) \geq 0, \\
	&\forall n \in \mathbb{N}, x_1, \ldots, x_n \in X, c_1, \ldots, c_n \in \mathbb{R}. 
\end{align*}
The function $k$, for any
parametrization by real constants, has non-negative weighted sum over all
inputs. A \emph{graph kernel} is a positive semi-definite kernel on a set of graphs.

Now, let us consider a collection of graphs $\mathcal{G}$, where each graph $G
\in \mathcal{G}$ has vertexes from an arbitrary vertex set $\mathcal{V}$.
Further, let $\mathcal{L}_v$ be the set of node labels, and let $lv:
\mathcal{V} \rightarrow \mathcal{L}_v$ be the graph labelling function.
Similarly, for edges let $\mathcal{L}_e$ be the set of edge labels, with a
similarly defined $le$. We will now consider a kernel function to be a function
$k: \mathcal{G} \rightarrow \mathbb{R}^{|\mathcal{G}|} \times
\mathbb{R}^{|\mathcal{G}|}$. In particular:
\begin{equation}
\label{eq:kernel_general}
    k(\mathcal{G})_{ij} = \langle\psi(G_i), \psi(G_j)\rangle,
\end{equation}
%
where $\psi$ maps a graph to a vector space and $\langle\cdot,\cdot\rangle$
represents the inner product. By doing so, we avoid needing to compute an
explicit feature space, instead we can operate on the images of points onto a
vector space. This is often referred to as a kernel
trick~\cite{theodoridis2008}.

\section{Vertex and Edge Label Histogram Kernels}
\label{appendix:graph_kernels:histogram}

Let us now consider a few simple Graph Kernels: Vertex/Edge Label Histogram
Kernels. For the Vertex Label Histogram Kernel, let $n=|\mathcal{L}_v|$, and let
$\psi$ return length $n$ vectors $\mathbf{f}$, such that the $i$-th value
represents the count of the $i$-th vertex label in the graph, \ie: 
\begin{equation}
    \psi(G) = \mathbf{f},
\end{equation}
\begin{equation}
    \mathbf{f}_i=|\{v \mid v \in V, lv(v) = \mathcal{L}_{vi}\}|.
\end{equation}
%
The matrix generated by our kernel function is then populated by the inner
product of these vertex label count vectors.
We similarly define such a kernel for Edge Histograms by substituting
$\mathcal{L}_e$ for $\mathcal{L}_v$ above, and $le$ for $lv$. Let the count
vectors generated by edge histograms be $\mathbf{g}$.

We can define a third kernel simply by considering:
\begin{equation}
    k(\mathcal{G})_{ij} = \langle[\mathbf{f},\mathbf{g}]_i, [\mathbf{f},\mathbf{g}]_j\rangle,
\end{equation}
%
where $[\mathbf{f},\mathbf{g}]$ represents vector concatenation. That is, we
consider the inner product of both vertex and edge histograms when computing the
inner product. This defines the Vertex-Edge-Label-Histogram kernel.

For each of the three presented kernels, Vertex Label Histogram, Edge Label
Histogram and Vertex-Edge Label Histogram, we can obtain the Gaussian Radial
basis function version of the kernel as follows. Let $\mathbf{h} = \mathbf{f}$,
$\mathbf{h} = \mathbf{g}$, or $\mathbf{h} = \mathbf{[\mathbf{f}, \mathbf{g}]}$
as appropriate for the respective kernel. The Gaussian version of the kernels
then becomes:
\begin{equation}
    k(\mathcal{G})_{ij} = \text{exp}\left(-\frac{\norm{\mathbf{h}_i - \mathbf{h}_j}}{2\sigma^2}\right),
\end{equation}
%
where $\sigma^2$ is the variance of our Gaussian RBF kernel.


\section{Random Walk Kernel}
\label{appendix:graph_kernels:random_walk}

Let us now consider Random Walk based kernels. For the $\kappa$-step Random Walk
kernel, we consider as input a vector of weights $(\lambda_0, \lambda_1, \dots,
\lambda_\kappa)$ where each $\lambda_i \in \mathbb{R}^+$ and define the
kernel as:
\begin{equation}
    k(\mathcal{G})_{ij} = \sum\limits_{i',j'=1}^{|V_{\times}|}\left[\sum\limits_{t=0}^{\kappa}\lambda_tA_{\times}^{t}\right]_{i'j'},
\end{equation}
%
where $A_{\times}$ represents the adjacency matrix of the direct tensor product
$G_{\times} = (V_{\times}, E_{\times}, \text{lv}_{\times}, \text{le}_{\times})$
of $G_i = (V_i, E_i, \text{lv}_i, \text{le}_i)$ and $G_j = (V_j, E_j,
\text{lv}_j, \text{le}_j)$ \st:
\[
V_{\times} = \{ (v_i, v_j) \in V_i \times V_j \mid \text{lv}_i(v_i) = \text{lv}_j(v_j) \},
\]
\[
E_{\times} = \{ ((u_i, u_j), (v_i, v_j)) \in V_{\times} \times V_{\times} \mid \text{le}_i((u_i, v_i)) = \text{le}_j((u_j, v_j)) \},
\]
%
and labels are propagated \ie:
\[
\text{lv}_{\times}((v_i, v_j)) = \text{lv}_i(v_i),
\]
\[
\text{le}_{\times}(((u_i, u_j), (v_i, v_j))) = \text{le}_i((u_i, v_i)).
\]

For the Graphlet kernel, for each graph $G_i$ consider the vector $\mathbf{l_i}
= (l_i^1, l_i^2, \dots, l_i^s)$ such that $l_i^k$ records the number of
embeddings of Graphlet $L_k$ into $G_i$. Each $L_k, 	\forall k \in [1..s]$ is a
graph with $n$ nodes. We say that $L = (V_L, E_L)$ can be embedded into $G$ if
there exists an injective $\alpha: V_L \rightarrow V$ \st $(v, w) \in E_L \iff
(\alpha(v), \alpha(w)) \in E$. Similar to previous kernel formulations, we
define $\psi(G_i) = \mathbf{l_i}$ and thus our kernel:
\begin{equation}
    k(\mathcal{G})_{ij} = \langle\mathbf{l_i}, \mathbf{l_j}\rangle
\end{equation}
%
Due to high computational costs, implementations of this kernel ignore Vertex
and Edge labels. For more details, see Shervashidze
\etal~\cite{shervashidze2009}.

\section{Weisfeiler-Lehman Kernel}
\label{appendix:graph_kernels:wl}

For the Weisfeiler-Lehman (WL) kernel, consider the following. The labelling
function $lv$ associated with a graph $G=(V,E)$, represents the labelling
function $lv_0$, the labelling function of 0 steps of Weisfeiler-Lehman. To
perform a step, we replace the set of labels of a node $v$, which is initialised
to the unit set containing $lv_0(v)$, with the set containing the current label
and the sorted set of labels of the neighbours of $v$. The multi-set label is
then compressed to a short label. We thus obtain $lv_{i+1}$ from $lv_i$: 
\begin{equation}
    lv_{i+1}(v) = \text{compress}({lv_i(v)} \cup {lv_i(v') \mid v' \in N(v)})
\end{equation}
%
This process is repeated for $h$ iterations. In particular, the compress
function must ensure that nodes obtain identical new labels if and only if they
have identical multi-set labels. Due to obtaining a sequence of labelling
functions, we also generate a sequence of length $h$ of graphs $(G_0, G_1,
\ldots, G_h)$ which differ only in their associated labelling functions. We now
the WL kernel itself, which is in effect a meta kernel, as:
\begin{equation}
    k_{WL}(\mathcal{G}) = k(\mathcal{G}_0) + k(\mathcal{G}_1) + \ldots + k(\mathcal{G}_h) 
\end{equation}
%
where $k$ is an inner kernel. $k_{WL}$ will be a graph kernel as linear
combinations of kernels preserve the properties of being semi-definite positive
and symmetric.

In \Cref{chapter:flexeme}, we combine the WL kernel formulation above with the
Subtree kernel as our inner $k$. This instantiation of WL is defined as follows.
Let $\Sigma_i \subseteq \Sigma$ be the alphabet of all letters that occur in the
labelling of nodes in graphs up the the $i$-th iteration of WL. We assume all
$\Sigma_i$ are pair-wise disjoint. Further, and without loss of generality, let
$\Sigma_i = {\sigma_{i0}, \sigma_{i|\Sigma_i|}}$ be ordered sets. Let $c_i:
\mathcal{G} \times \Sigma_i \rightarrow \mathbb{N}$ be a count function, \st
$c(G, \sigma_{ij})$ returns the number of occurrence of $\sigma_{ij}$ in $G$ as
a node label. We will now build our vector as:
\begin{equation}
\label{eq:wl_vector}
    \psi(G) = (c_0(G, \sigma_{00}), \ldots, c_0(G, \sigma_{0|\Sigma_0|}), \ldots, c_h(G, \sigma_{h|\Sigma_h|}))
\end{equation}
%
By defining the projection function $\psi$, we side-step the process of
computing $k$ directly for each graph. Finally, we obtain the similarity matrix,
by replacing $\psi$ from \Cref{eq:wl_vector} in \Cref{eq:kernel_general}, which
enables Flexeme to then perform agglomerative clustering.

