\chapter{Graph Kernels}
\label{appendix:graph_kernels}

In this Appendix, I will first define a kernel and a graph kernel. I will then
show some of the earlier examples of kernels: from vertex/edge label histogram
kernels through to random-walk and graphlet Kernels. I will then detail the
general Weisfeiler-Lehman graph kernel framework and show how to instantiate it
with a subtree graph kernel. Kernels in general, and graph kernels being no
exception, are used to allow machine learning approaches,such as clustering,
that make use of similarity or (pseudo-)metrics to learn. We use the
Weisfeiler-Lehman subtree graph kernel in \Cref{chapter:flexeme} to allow
re-clustering \deltaPDGN graphs into atomic patches.

A kernel is a symmetric function $k: X \times X \rightarrow \mathbb{R}$ on
non-empty sets $X$. A kernel is positive semi-definite if:
\begin{align*}
	&\sum_{i=1}^{n} \sum_{j=1}^{n} c_i c_j k(x_i, x_j) \geq 0, \\
	&\forall n \in \mathbb{N}, x_1, \ldots, x_n \in X, c_1, \ldots, c_n \in \mathbb{R}. 
\end{align*}
The function $k$, for any
parametrization by real constants, has a non-negative weighted sum over all
inputs. A \emph{graph kernel} is a positive semi-definite kernel on a set of
graphs. As we use kernels as a measure of similarity, we are interested in those
kernels that have an equivalent inner product definition. As inner products are
positive semi-definite, we are only interested in positive semi-definite
kernels.

Now, let us consider a collection of graphs $\mathcal{G}$, where each graph $G
\in \mathcal{G}$ has vertexes from an arbitrary vertex set $\mathcal{V}$.
Further, let $\mathcal{L}_v$ be the set of node labels, and let $lv:
\mathcal{V} \rightarrow \mathcal{L}_v$ be the graph labelling function.
Similarly, for edges, let $\mathcal{L}_e$ be the set of edge labels, with a
similarly defined $le$. We will now consider a kernel to be a function
$k: \mathcal{G} \rightarrow \mathbb{R}^{|\mathcal{G}|} \times
\mathbb{R}^{|\mathcal{G}|}$. In particular:
\begin{equation}
\label{eq:kernel_general}
    k(\mathcal{G})_{ij} = \langle\psi(G_i), \psi(G_j)\rangle,
\end{equation}
%
where $\psi$ embeds a graph into a vector space and $\langle\cdot,\cdot\rangle$
represents the inner product. By doing so, we avoid needing to compute an
explicit feature space, which depending on our set $X$ can be costly. Instead we
can operate on the images of our data points, here graphs, onto a vector space.
This is often referred to as a kernel trick~\cite{theodoridis2008}. This trick
may not be possible for kernels that are not positive semi-definite.

\section{Vertex and Edge Label Histogram Kernels}
\label{appendix:graph_kernels:histogram}

To build your intuition on how to construct a graph kernel, we start with two
simple, but useful examples, that aggregate a graph's labels to build a
histogram summary of a graph: vertex/edge label histogram kernels. For the
vertex label histogram kernel, let $n=|\mathcal{L}_v|$, and let $\psi$ return
length $n$ vectors $\mathbf{f}$, such that the $i$-th value represents the count
of the $i$-th vertex label in the graph, \ie: 
\begin{equation}
    \psi(G) = \mathbf{f},
\end{equation}
\begin{equation}
    \mathbf{f}_i=|\{v \mid v \in V, lv(v) = \mathcal{L}_{vi}\}|.
\end{equation}
%
The matrix generated by our kernel function is then populated by the inner
product of these vertex label count vectors.
We similarly define such a kernel for edge label histograms by substituting
$\mathcal{L}_e$ for $\mathcal{L}_v$ above, and $le$ for $lv$. Let the count
vectors generated by edge histograms be $\mathbf{g}$.

We can define a third kernel simply by considering:
\begin{equation}
    k(\mathcal{G})_{ij} = \langle[\mathbf{f},\mathbf{g}]_i, [\mathbf{f},\mathbf{g}]_j\rangle,
\end{equation}
%
where $[\mathbf{f},\mathbf{g}]$ represents vector concatenation. That is, we
consider the inner product of both vertex and edge label histograms when
computing the inner product. This defines the vertex-edge label histogram
kernel.

For each of the three presented kernels, vertex label histogram, edge label
histogram, and vertex-edge label histogram, we can obtain the Gaussian radial
basis function (RBF) version of the kernel as follows. Let $\mathbf{h} =
\mathbf{f}$, $\mathbf{h} = \mathbf{g}$, or $\mathbf{h} = \mathbf{[\mathbf{f},
\mathbf{g}]}$ as appropriate for the respective kernel. The Gaussian version of
the kernels then becomes:
\begin{equation}
    k(\mathcal{G})_{ij} = \text{exp}\left(-\frac{\norm{\mathbf{h}_i - \mathbf{h}_j}}{2\sigma^2}\right),
\end{equation}
%
where $\sigma^2$ is the variance of our Gaussian RBF kernel.

If we were to apply these kernels to our commit untangling task in
\Cref{chapter:flexeme}, we would make use of only the immediate neighbourhoods
of changed nodes. We would lose considerable topological information: that of
higher-order neighbours.

\section{Random-Walk Kernel}
\label{appendix:graph_kernels:random_walk}

Let us now consider a more complicated kernel that allows for the better
preservation of topological information: random-walk based kernels. For the
$\kappa$-step random walk kernel, we consider as input a vector of weights
$(\lambda_0, \lambda_1, \dots, \lambda_\kappa)$ where each $\lambda_i \in
\mathbb{R}^+$ and define the kernel as:
\begin{equation}
    k(\mathcal{G})_{ij} = \sum\limits_{i',j'=1}^{|V_{\times}|}\left[\sum\limits_{t=0}^{\kappa}\lambda_tA_{\times}^{t}\right]_{i'j'},
\end{equation}
%
where $A_{\times}$ represents the adjacency matrix of the direct tensor product
$G_{\times} = (V_{\times}, E_{\times}, \text{lv}_{\times}, \text{le}_{\times})$
of $G_i = (V_i, E_i, \text{lv}_i, \text{le}_i)$ and $G_j = (V_j, E_j,
\text{lv}_j, \text{le}_j)$ \st:
\[
V_{\times} = \{ (v_i, v_j) \in V_i \times V_j \mid \text{lv}_i(v_i) = \text{lv}_j(v_j) \},
\]
\[
E_{\times} = \{ ((u_i, u_j), (v_i, v_j)) \in V_{\times} \times V_{\times} \mid \text{le}_i((u_i, v_i)) = \text{le}_j((u_j, v_j)) \},
\]
%
and labels are propagated \ie:
\[
\text{lv}_{\times}((v_i, v_j)) = \text{lv}_i(v_i),
\]
\[
\text{le}_{\times}(((u_i, u_j), (v_i, v_j))) = \text{le}_i((u_i, v_i)).
\]

Random-walk based kernels, at an intuitive level, find sub-graphs that are
shared between the graphs we wish to compute a similarity for. Instead of
obtaining these sub-graphs through random walking, we can consider a collection
of sub-graph patterns: graphlets. For the graphlet kernel, for each graph $G_i$
consider the vector $\mathbf{l_i} = (l_i^1, l_i^2, \dots, l_i^s)$ such that
$l_i^k$ records the number of embeddings of Graphlet $L_k$ into $G_i$. Each
$L_k, \forall k \in [1..s]$ is a graph with $n$ nodes. We say that $L = (V_L,
E_L)$ can be embedded into $G$ if there exists an injective $\alpha: V_L
\rightarrow V$ \st $(v, w) \in E_L \iff (\alpha(v), \alpha(w)) \in E$. Similar
to previous kernel formulations, we define $\psi(G_i) = \mathbf{l_i}$ and thus
our kernel:
\begin{equation}
    k(\mathcal{G})_{ij} = \langle\mathbf{l_i}, \mathbf{l_j}\rangle
\end{equation}
%
Due to high computational costs, implementations of this kernel ignore Vertex
and Edge labels. For more details, see Shervashidze
\etal~\cite{shervashidze2009}. If applied to Flexeme, this would discard code
snippet and other valuable information that can help solve the untangling
problem. Indeed, purely by dataflow and controlflow branching, it is difficult
to discern code in a PDG by task.

\section{Weisfeiler-Lehman Kernel}
\label{appendix:graph_kernels:wl}

Let us now first consider the Weisfeiler-Lehman kernel
framework~\cite{shervashidze2011weisfeiler}, which arose from a graph
isomorphism test~\cite{Weisfeiler1968ReductionOA}. The kernel equivalent to the
isomorphism test arose in the Biomedical community as a method of computing the
similarity between graph structures such as proteins as a proxy for determining
the properties of new protein structures~\cite{Sugiyama2018Bioinfo}. When
combined with a subtree kernel, this allows us to consider graphs that have
similar views from each node: when starting a tree from a node up to a certain
depth, two graphs are more similar the more trees are held in common when
considering both topology and labels. In Flexeme's setting, this means that the
two graphs are similar if they have similar statements following each other: a
proxy for similar executions.

For the Weisfeiler-Lehman (WL) kernel, consider the following: the labelling
function $lv$ associated with a graph $G=(V,E)$ becomes the labelling function
$lv_0$, the labelling function of 0 steps of Weisfeiler-Lehman (the base case).
To perform a step, we replace the set of labels of a node $v$ with the set
containing the current label and the sorted set of labels of the neighbours of
$v$. The multi-set label is then compressed to a short label. We obtain
$lv_{i+1}$ from $lv_i$ as follows: 
\begin{equation}
    lv_{i+1}(v) = \text{compress}(\{lv_i(v)\} \cup \{lv_i(v') \mid v' \in N(v)\})
\end{equation}
%
This process is repeated for $h$ iterations. In particular, the compress
function must ensure that nodes obtain identical new labels if and only if they
have identical multi-set labels. Due to obtaining a sequence of labelling
functions, we also generate a sequence of length $h$ of graphs $(G_0, G_1,
\ldots, G_h)$ which differ only in their associated labelling functions. We now
define the WL kernel itself, which is in effect a meta kernel, as:
\begin{equation}
    k_{WL}(\mathcal{G}) = k(\mathcal{G}_0) + k(\mathcal{G}_1) + \ldots + k(\mathcal{G}_h) 
\end{equation}
%
where $k$ is an inner kernel. $k_{WL}$ will be a graph kernel as linear
combinations of kernels preserve the properties of being semi-definite positive
and symmetric.

In \Cref{chapter:flexeme}, we combine the WL kernel formulation above with the
subtree kernel as our inner $k$. This instantiation of WL is defined as follows.
Let $\Sigma_i \subseteq \Sigma$ be the alphabet of all letters that occur in the
labelling of nodes in graphs up the the $i$-th iteration of WL. We assume all
$\Sigma_i$ are pair-wise disjoint. Further, and without loss of generality, let
$\Sigma_i = \{\sigma_{i0}, \sigma_{i|\Sigma_i|}\}$ be ordered sets. Let $c_i:
\mathcal{G} \times \Sigma_i \rightarrow \mathbb{N}$ be a count function, \st
$c(G, \sigma_{ij})$ returns the number of occurrence of $\sigma_{ij}$ in $G$ as
a node label. We will now build our vector as:
\begin{equation}
\label{eq:wl_vector}
    \psi(G) = (c_0(G, \sigma_{00}), \ldots, c_0(G, \sigma_{0|\Sigma_0|}), \ldots, c_h(G, \sigma_{h|\Sigma_h|}))
\end{equation}
%
By defining the projection function $\psi$, we side-step the process of
computing $k$ directly for each graph. The inner product of the vector space
onto which $\psi$ projects represents the number of common rooted subtrees
shared by the respective projected graphs. Finally, let $\psi(G_i) = s_i$, then we obtain the similarity
matrix as:
\begin{equation}
    k(\mathcal{G})_{ij} = \langle\mathbf{s_i}, \mathbf{s_j}\rangle
\end{equation}
%
This enables Flexeme to then perform agglomerative
clustering; the intuition is that common rooted subtrees represent a good proxy
for similar execution paths.
